@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@  										  @@
@@  THIS FILE DESCRIBES THE STEPS TAKEN IN TAKING GAMMIE'S HAM3D CODE AND	  @@
@@  MAKING IT A RELATIVISTIC MHD CODE WITH MPI SUPPORT CALLED HARM3D.		  @@
@@  Please see "README" for a description of the algorithm and how	          @@
@@  to use it.									  @@
@@  										  @@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@





################################
Sat Feb 18 21:21:54 CST 2006
-------------------------------

-- this directory is my development directory in which I
   am altering starting from ham3d

-----------------------------------------------------

PRELIMINARY HAM3D CHANGES:
-- my goal is to change some of the variable names before
	making the split to ham3d_mpi


    N2->N3
    N1->N2
    N0->N1
	-- searching for others:
		prompt> cat *.c *.h| awk '/0+.*1+.*2+/' | less

	-- doing the same for L[0-2], U[0-2], B[0-2], dx[0-2], X[0-2], p[0-2], ek[0-2],
                              , v[0-2], b[0-2], [0-2]-face, j[0-2], w[0-2], c[0-2],
				eb[0-2] (but don't do since b[0-2] will handle it)


-- we can do this all at once, working on the wild side here...:

foreach n ( N L U B dx X p ek v b j w c )
foreach? sed -i s/${n}2/${n}3/ *.c *.h
foreach? sed -i s/${n}1/${n}2/ *.c *.h
foreach? sed -i s/${n}0/${n}1/ *.c *.h
foreach? end

prompt> sed -i s/2-face/3-face/ *.c *.h
prompt> sed -i s/1-face/2-face/ *.c *.h
prompt> sed -i s/0-face/1-face/ *.c *.h


-- it seems to have worked, I'll keep an eye out for any stray ones...


#############################
Wed Sep 13 12:33:27 EDT 2006
-------------------------------

-- actually it didn't work since I failed to use the 'g' option with
   the search/replace, so the pattern was substituted only once per
   matching line (and there were lines with multiple matches).

-- since I don't remember making any other changes to the code,
    I'm going to start with the original version and then redo the
    sed commands:
    -- the new sed commands are:

foreach n ( N L U B dx X p ek v b j w c )
foreach? sed -i s/${n}2/${n}3/g *.c *.h
foreach? sed -i s/${n}1/${n}2/g *.c *.h
foreach? sed -i s/${n}0/${n}1/g *.c *.h
foreach? end

prompt> sed -i s/2-face/3-face/g *.c *.h
prompt> sed -i s/1-face/2-face/g *.c *.h
prompt> sed -i s/0-face/1-face/g *.c *.h


-- now that looks better, let's see if it compiles (before it didn't):
   -- compiled with a warning about "time()" being undefined in ranc.c
      -- changed the header references inside to include "time.h",
      -- compiles fine now

-- changing "makefile" to look more like that in harm-release
   -- seems to work

-- since none of the Num. Rec. stuff in nrutil.[c,h] is used, i'm
   removing the files
   -- compiles fine, so i guess I really didn't miss anything

-- committing these changes to CVS, eliminating nrutil.* from
   repository...


-- changed "makefile" to generate defs.h from decs.h ,
	  so decs.h will be where the mnenomics are defined
	  and where the global variables are added (using "extern ...")


#############################
Thu Sep 14 17:55:13 EDT 2006
-------------------------------

-- commenting and changing  decs.h
   -- adding grid index mnemonics based on N0-N3, e.g. N[0-3][S,E] for
      the start and ending indices for those directions,


-- changing L1,L2,L3 -> GridLength[0-3]:
   (made sure that nothing else matched on L[1-3] )

prompt> sed -i s/L1/GridLength\[1\]/g *.c *.h
prompt> sed -i s/L2/GridLength\[2\]/g *.c *.h
prompt> sed -i s/L3/GridLength\[3\]/g *.c *.h

    -- changed tf->GridLength[0]


-- need to change is->N1S , js->N2S, etc.:

-- changed is->N1s:
   -- checked for instances of "is" that was not the variable "is":
prompt> grep is *.c *.h | grep -v "[^[:alnum:]_]is[^[:alnum:]_]" >is.tmp

   -- made the replacement:
prompt> sed -i -e 's/is/N1S/g' *.c *.h

   -- then replaced those instances of "is" that were within words (from is.tmp)


   -- repeated this for js,ks,ie,je,ke
      (needed things like: )
prompt> sed -i -e 's/N2S_ad/js_ad/g' *.c *.h



-- compiles after these changes


-- change dt,dx1,dx2,dx3    ->  dX[0-3]

prompt> sed -i -e 's/dt/dX\[0\]/g' *.c *.h
prompt> sed -i -e 's/dX\[0\]sqijmin/dtsqijmin/g' *.c *.h
prompt> sed -i -e 's/dX\[0\]sqij/dtsqij/g' *.c *.h
prompt> sed -i -e 's/dX\[0\]t/dtt/g' *.c *.h
prompt> sed -i -e 's/dx1/dX\[1\]/g' *.c *.h
prompt> sed -i -e 's/dx2/dX\[2\]/g' *.c *.h
prompt> sed -i -e 's/dx3/dX\[3\]/g' *.c *.h



#############################
Fri Sep 15 10:51:47 EDT 2006
-------------------------------

-- changing dX[] -> dx[]  since it doesn't make sense to use uppercase
   since we typically write the contravariant line element as lower
   case;

-- since there isn't a need for a relativistic shearing box boundary
   condition, then I'm going to delete it; those needing it for the
   ham3d-mpi version will have to supply their own.
   -- then, i'm removing bounds_plin.c from the CVS



-- added "RECON_TYPE_CHOICE" macro and its options
-- added "BC_TYPE_CHOICE" macro and its options

-- changed bounds.c to add several different types of global
   boundary conditions (those of different coordinate systems)


#############################
Fri Sep 29 10:49:16 EDT 2006
-------------------------------

-- went through step_ch.c
   -- for now nothing to be done I believe
   -- may need to change wave_minmax() in phys.c
   -- will eventually need to change how flux is calculated
      to add different methods
   -- may need to eliminate timestep() calls to wave_minmax()
      since wave_minmax() calc. is already done in fluxcalc()
       ( cmax/cmin were found in the fluxcalc routine in harm2d0

 -- added the following files from harm2d:
	    coord.c  fixup.c  image_interp.c  lu.c
	    metric.c restart.c u2p_defs.h u2p_util.c u2p_util.h utoprim_1dfix1.c
	    utoprim_1dvsq2fix1.c utoprim_2d.c


-- updated all of coord.c
-- updated dump.c


-- working on phys.c
   -- added "of_state" and "of_geom" structures;
   -- stole tried and true routines from harm2d;

-- 3d-ized get_geometry()
-- 3d-ized coord()
-- 3d-ized blgset()



-- added "state q" to argument in source() to eliminate extra
   get_state() call


-- right now I'm in the middle of changing timestep() using
   vchar and max_char_speed[NDIM] ;
   -- may need to combine reconstruct() into numerical_flux()
       in order to calculate min(dt) correctly.


-- need to fix "dir" and "FACE" variables
   dir = 0-NDIM
   FACE1-3 = 0-2

   -- "dir" in phys.c means 0-3

   -- anything on the faces, though, has indices 0-2 corresponding to
      the different faces;

-- clarified dim/dir  index usage in step_ch.c;
-- added idir[],jdir[],kdir[] arrays to handle direction predicates in fluxes and limiers


#############################
Sat Nov  4 21:15:48 EST 2006
-------------------------------

-- i've tried a few Komissarov tests and it works fine.  Need to check
	to see if shocks in all directions are correct in order to test
	that there's no code asymmetry.

-- all but Kom.5 (Alfven wave) and Kom.6 (Compound wave) worked using
	HLLE and minmod.  I haven't yet compared the results  to exact solutions,
	so I'm guestimating here.  Need to also use different slope limiters.
	BTW, the HARM paper did not do Kom.5 and Kom.6 tests since they did not
	have access to the exact solution (they probably copped out).


-- I'm now changing the reconstruction method (switching to L/R states rather
	than slopes).  Will be adding PPM and ENO schemes.

-- changed u2p_defs.h so that we don't have to change GAMMA here anymore (it uses
	the value set in init.c )



#############################
Wed Nov  8 11:20:42 EST 2006
-------------------------------


-- added a bunch of initial data types (init.*.c)  from Komissarov (1999)
	and Giacomazzo and Rezzolla (2006).

-- the code seems to be able to handle hydro shocks fine (where the B-field
	is uniform, either perp. or para. to boundary), but cannot handle
	cases with discontinuities in B_perp and/or  v_perp.

	-- maybe a problem with CT method:


-- added "SAFE" factor to "dt" calculation  so that time step does
	not increase dramatically if the wave speeds change suddenly;



-- Komissarov's test #1 (fast shock) is problematic for RECON_MC  (cour=0.4)
	but not for RECON_MINMOD (cour=0.4)
	-- why?  is it just because minmod is extra diffusive??
	-- works well for:
		-- FLUX_LF/MINMOD/cour=0.4/N=400;
		-- FLUX_HLLE/MINMOD/cour=0.4/N=400;
		-- FLUX_LF/MC/cour=0.4/N=400; (works, but some spikiness near shock)

	-- oscillatory/bad  for :
		-- FLUX_HLLE/MC/cour=0.4/N=400;
		--


-- the problems with the Balsara tests was not a problem with the code
	but a problem with the comparing simulation results at the wrong time
	with riemann results times

-- Balsara Test #1 :
	-- FLUX_LF/MINMOD/cour=0.4/N=600
	--

-- RERUNNING all W/ cour=0.8,  LF/MC:
	-- Balsara1-5: all good (failed for Balsara5 at first, needed to fix exact solution)
	-- kom1: didn't work well, many oscillations
	-- kom2: minor oscillations, ok
	-- kom3: ok, larger bump at small x, better By plot than Kom.
	-- kom4: just as good, better for By again
!!	-- kom5: BAD
!!	-- kom6: BAD
	-- kom7: better
	-- kom8: better
	-- kom9: more osc. in By, more osc. in general than Kom.
	-

-- RERUNNING kom5 and kom6 w/ cour=0.5, all others the same
	-- kom5:
	-- kom6:


-- still don't know what's wrong with kom5,kom6 ;  it doesn't get any better
	with resolution or limiter and appears to be doing a reasonable evolution;
	-- email Sergei for help, we'll see


#############################
Mon Nov 13 16:11:19 EST 2006
-------------------------------

-- changing flow so that left/right states are saved and then used
	in the flux calculation so that we can add higher-order interpolation methods;

-- changed the flow, put the reconstruction and flux calculation into their own
	routines that do all the loops (so that it's now looping within the routines
	and not over them);

-- added para() that is Xiaoyue's parabolic interpolation routine;

-- rerunning the tests for LF+MC with the new changes
	-- balsara3,4  fails at the shock probably needs the floor, will rerun
	-- floor fixed balsara4 problem
	-- still problem with balsara3,
		-- essentiall the shock eventually steepens too much


-- wrote rotor initial data from Del Zanna et al. 2002.
	-- works fine with LF+MC
		-- maybe we don't quite see as large densities.
	-- trying with  LF+PPM_MM
		-- works great without failures, looks as good as Del Zanna's
		-- maybe smaller p_mag


-- got a response from Komissarov:
	-- get gave me the code to generate the alfven wave and
		the initial data method to use the wave to setup
		the tests in his 1999 paper;
	-- using these new routines I was able to recreate his results
		and found that our B-field (i.e. Bz) is larger and narrower,
		which is probably an effect of his code's use of resistivity;
	-- all else agree though

-- realized that init.balsara4.c was wrong;  corrected it


-- LF+MC:
	-- good for all
	-- fails at start of  balsara4
	-- fixed by reducing initial dt;

-- PPM+MM shocktube tests
	-- has noticeable oscillation in rarefaction regions  of kom4-7;
	-- fails for balsara3, and produces incorrect results for balsara4
	-- the balsara3 failure seems to be because an ever steepening
		shock becomes underresolved;
	-- performs worse on kom5-6 that LF+MC


#############################
Thu Nov 16 14:52:53 EST 2006
-------------------------------

-- doing tests along other directions to make sure that
	HARM3D is symmetric to transposing x,y,z;

	-- in doing the cylindrical shock problem, found that divb
		was significantly larger at the X1DN and X2DN boundaries;
		for a run in the yz-plane, the problem occurred along the X2DN and
		X3DN boundaries.  It is just in the first physical cell along those
		edges;
		-- maybe there's a problem with the CT along the lower faces...
		-- no, it's because the divb calculation uses the (i-1),(j-1)...
			cells (i.e. centers the divb calculation at the i,j,k CORNER);
			since the ghost cells are not constrained, then the boundary
			values of divb are going to be erroneous;


	-- in light of this, I'm zeroing out the divb calculation along lower faces,
		and divbcen along upper and lower faces since the calculations
		are misleading at this locations;


-- I did  init.cylexp.c test along other symmetry axes (init.cylexp[yz,xz].c) ,
	transposed the resulting data and compared the different runs and found
	at most 1e-12 scale relative errors in rho,uu,bsq.  The errors started
	at roundoff at t=0 and grew over time.  This is most likely all roundoff
	in origin;


#############################
Fri Nov 17 11:26:48 EST 2006
-------------------------------

-- added ./testing  to contain code used to test various components of HARM3d

-- added ./testing/fixup_interp to test new 3d interpolation/averaging routine;

-- adding new fixup interpolation/averaging routine;

-- changing bounds() so that pflag[]'s boundary conditions (ghost cells) are set;



-- ways of failure in chronological order of when they'd arise:
	-- Utoprim() fails to converge to solution;
		-- interp

	-- Utoprim() gives too small rho, u
		-- set to floor if only problem
		-- interp if can, else use floor values
		-- interp if can, else use previous step's values

	-- gamma_calc() fails
		-- rescale velocities so that gamma = gamma of previous step;
		-- interp
		-- ->ucon_calc -> get_state ->
				(source(),bsq_calc(),advance(),numerical_flux())

	-- gamma becomes too large
		-- set to gammamax and rescale velocities
		-- interp

	-- vchar() fails
		-- set wave speeds to 1 or 0 (depending on which one)  (done already);
		-- interp  (over bordering cells)
		-- vchar()->numerical_flux() -> advance()



-- for now do nothing when vchar() fails since it is handled somewhat already
	and it is usually a sign that things are bad;

-- changing fail() to handle the logic of handling failures;

-- added global integer "treat_floor_as_failure" which, when non-zero,
	makes the inversion routines return with the old primivitive variables
	so that they can be used as the ultimate fix (if interpolation does not work);

-- added interpolation fix when gamma_calc() fails ;

-- just realized that the code was never calling fixup(); so all these
	tests so far did not use a floor at ALL;

-- tested balsara1 and it looks good;

-- trying out balsara3 that ultimately fails w/ PPM w/o fixups:
	-- works now, looks ok;

-- trying balsara3 with lf_mc w/ fixups:
	-- with "treat_floor_as_failure=1" was noiser that
		the run before adding the fixups;
	-- the run with "treat_floor_as_failure=0" looks like the old run
	-- how did lf_mc do the run without the floor??  because
		now it needs it;


-- trying LF_PPM with "treat_floor_as_failure=0" :
	-- eventually crashed becuase of failures to converge (utoprim) and
		gamma max errors and failure to finding an interpolation stencil




#############################
Fri Nov 24 23:15:32 EST 2006
-------------------------------

-- moved  fix_flux() to bounds.c where it should be and converted it to form of harm3d;;
   -- need to double check to see if this is correct in 3D
   -- need to confirm that axisymmetry EMF condition is not "taken care of" by
      the boundary conditions on B2 and U2 already;

-- adding good kerr schild and arbitrary X1,X2 code to harm3d;
   -- getting it from my version of the modern harm2d code;
   -- using my MKS coordinates for the connection and gcov/gcon calculation;
   -- adding MINK spherical coordinates from bothros;


-- added macros "COORD_*" and COORD_TYPE_CHOICE that controls what numerical coordinates
   we are to use;


-- !! we are using h_slope for both the MIXED and the DIAGONAL transformations (as opposed
      to hslope/hrc0
      -- changing s_hrc ->  X1_slope
      -- X1_hrc -> X1_0

-- added init.bondi.c  to specify bondi accretion evolution
   -- still need to finish it


-- removing use of X[] and r,th,phi  :
	    X[]        ->   xp[]
	    {r,th,phi} ->   x[RR,TH,PH]


-- still need to finish changing coord.c
   -- need to derive minkowski connection in xp[] coordinates
   -- for now we are just using the finite difference connection for the following
      implemented coordinate systems:
		 -- bl_spherical
		 -- mink_spherical


-- adding risco_calc() and rhorizon_calc() to init.bondi.c
-- added Rin_calc()

-- added Bondi solution calculation from harm2d into init.bondi.c

-- moved coord() to coord.c

-- added transform_rank[1,2][cov,con]() routines


-- had some problems with Rin_calc() but they are now fixed;

-- bondi solution develops theta-dependence early on that is symmetric
	about the equator;
	-- probably from non-zero v2 that develops after the first timestep;
	-- this value of v2 diminishes with resolution, but not quickly;
	-- bondi evolution seems to work fine with no coordinate
		transformation (i.e. w/  COORD_IDENTITY )
	--


-- testing metric routines:
	!! bl_gdet_func() looks wrong
	-- bl_gcov() and bl_gcon() look right
	-- ks_gcov_func() looks right (and i think we tested it against Ruben's code too)
	--
	-- need to run test_geom() with each COORD_TYPE_CHOICE
	-- running test_geom() with COORD_IDENTITY:
		-- dx_dxp test all 0
		-- consym test all 0
		-- concmp test all below 1e-8, most below 1e-10:
			-- largest ones (1e-10 to 1e-9) are components:
				111,000,001,010,100,101,110 only
			-- and all large ones are for   r > 7-8M
		-- ksvsym test all 0
		-- ksnsym test all 0
		-- blvsym test all 0
		-- blnsym test all 0
!!		-- mkconn test most bad since we need to test this with METRIC_MINK_SPHERICAL
		-- b2kgcv test all within 1e-13
		-- b2kgcn test all 1e-13
		-- k2bgcv test fails for 10  and 01 componets,  all others fine within 1e-13 level
			-- the failures are all due to failure to perfectly cancel terms to
				the level of 1e-17 (i.e. the bl_gcov elements are 0 while the
				trasformed one is ~1e-17)
		-- k2bgcn test fails for 10  and 01 componets,  all others fine within 1e-13 level
			-- the failures are all due to failure to perfectly cancel terms to
				the level of 1e-17 (i.e. the bl_gcov elements are 0 while the
				trasformed one is ~1e-17)
		-- b2k2bv test all within 1e-13
		-- b2k2bn test all within 1e-15
		-- kinvgn test all within 1e-15
		-- binvgn test all within 1e-15
		-- minvgn test all within 1e-15
		-- blgdet test all within 1e-15
		-- ksgdet test all within 1e-15
		-- mkgdet test all within 1e-15
		-- dxdxpn test all within 1e-11 (largest being dx_dxp[1][1])
		-- dxdxp2 test all 0
		-- idxdxp test all 0


	-- running test_geom() with COORD_DIAGONAL: (with h_slope=0.0)
		-- dx_dxp test all within 1e-15
		-- consym test all 0
		-- concmp test all within 1e-9
			-- worst components are 332, 323, 233,
		-- ksvsym test all 0
		-- ksnsym test all 0
		-- blvsym test all 0
		-- blnsym test all 0
!!		-- mkconn test (need to set METRIC_MINK_SPHERICAL)
		-- b2kgcv test all within 1e-13
		-- b2kgcn test all within 1e-13
		-- k2bgcv test same as before
		-- k2bgcn test same as before
		-- b2k2bv test all within 1e-14
		-- b2k2bn test all within 1e-15
		-- kinvgn test all within 1e-15
		-- binvgn test all within 1e-15
		-- minvgn test all within 1e-15
		-- blgdet test all within 1e-15
		-- ksgdet test all within 1e-15
		-- mkgdet test all within 1e-15
		-- dxdxpn test all within 1e-10 (largest being dx_dxp[1][1])
		-- dxdxp2 test bad
			-- the 222 component is bad.
				-- this is probably because the finite difference is
					in accurate, since the exact calculation give 0
					as it should ; the numerical derivative way
					gives values ~1e-6;
			-- the 111 component seems to be not very accurate, but is probably
				due to finite differencing's failure at calculating exp();
		-- idxdxp test all within 1e-15

		-- evolution is not static but seems to eventually settle into steady-state
			-- setting Rin=1.9 (like uniform run) does not seem to help;



	-- running test_geom() with COORD_DIAGONAL: (with h_slope=0.35)
		-- dx_dxp test all within 1e-15
		-- consym test all 0
		-- concmp test all within 1e-9
			-- worst components are 332, 323, 233,
		-- dxdxpn test all within 1e-9 (largest being dx_dxp[2][2])
		-- dxdxp2 test
			-- the 222 component is good within 1e-5
				-- trunc. error?
			-- the 111 component is good to within 1e-7
				-- trunc. error?
		-- idxdxp test all within 1e-15

		-- even though these tests pass they don't give a
			static evolution;
			-- solution becomes theta dependent after 1st timestep;
			-- solution never settles into steady-state


	-- running new tests in test_geom() with COORD_DIAGONAL: (with h_slope=0.0)
		-- trgdet test all within 1e-15
		-- invtrn test all within 0 (there were some sig. diff. from 0 but
		   those entailed the inversion only working to the 1e-15  level (i.e.
		   the exact answer was exactly 0 and the element from the
		   "transform and transform back"  was equal to 0);


	-- running test_geom() with COORD_MIXED (del_x=1e-5):
	    (with h_slope=0.35,X1_0=Log(10.),X1_slope=1)
		-- dx_dxp test all within 1e-15
		-- consym test all 0
		-- concmp test all within 1e-6 (most within 1e-9)
		-- dxdxpn test all within 1e-8
		-- dxdxp2 test
			-- 211 and 222 deviate to within 1e-2-1e-3 sometimes
			-- check to see if it converges
		-- idxdxp test all within 1e-15
			(sig. diff's seen only when there are deviations from delta()
			at roundoff level)
		-- trgdet test all within 1e-15
		-- invtrn test all withine 1e-15
			(sig. diff's seen only when there are deviations from delta()
			at the 1e-15 level)


		-- even though these tests pass they don't give a
			static evolution;
			-- solution becomes theta dependent after 1st timestep;
			-- solution never settles into steady-state


	-- running test_geom() with COORD_MIXED (del_x=1e-8):
	    (with h_slope=0.35,X1_0=Log(10.),X1_slope=1)
		-- concmp test all within 1e-3 (most within 1e-9, just a few at 1e-3-1e-4)
		-- dxdxpn test all within 1e-6 (most within 1e-8, just a few at 1e-6)
		-- dxdxp2 test some 211 212 fail completely
		-- idxdxp test all within 1e-15
			(sig. diff's seen only when there are deviations from delta()
			at roundoff level)
		-- trgdet test all within 1e-15
		-- invtrn test all
			(sig. diff's seen only when there are deviations from delta()
			at the 1e-15 level)
		--

	-- running test_geom() with COORD_MIXED (del_x=1e-4):
	    (with h_slope=0.35,X1_0=Log(10.),X1_slope=1)
		-- concmp test all within 1e-4 (most within 1e-9, just a few at 1e-4-1e-5)
		-- dxdxpn test all within 1e-6 (most within 1e-8, just a few at 1e-6)
		-- dxdxp2 test some (212 only here has large reldiff but that is
			due to the analytic calculation being diff. from 0 by roundoff)
		-- idxdxp test all within 1e-15
			(sig. diff's seen only when there are deviations from delta()
			at roundoff level)
		-- trgdet test all within 1e-15
		-- invtrn test all
			(sig. diff's seen only when there are deviations from delta()
			at the 1e-15 level)

		-- so the differences seen in the conn and dxdxp stuff is most likely
			due to finite difference error in that when del_x gets too small
			then the FD way becomes insensitive to differences due to
			loss of precision in some of the calculation;
			-- that's why the larger del_x calculations led to better
				agreement


-- adding init.magbondi.c
	-- starting from init.bondi2.c
	-- should be finished now, need to test

-- adding init.torus.c
	-- starting from init.bondi2.c
	--

-- added routine ucon2pr() to phys.c ;


#############################
Mon Dec 11 10:44:43 EST 2006
-------------------------------

-- doing 3D bondi flow evolution
	-- seems to be stable up to at least 100M
	-- v^3~1e-4 near the axis which grows slowly in time
	-- v^1 and v^2 in the very first cell (i,j,k)=(0,0,0)
		start off being slightly different than the other
		points;  possibly due to a
		the rest (i.e.

-- I proved that u^th, u^ph do not change under parity symmetry (x,y->-x,-y);
  	-- so the "reflexion" boundary conditions should be changed for U2 and B2

-- added gdet to the divb calculation in diag.c


-- added divb_calc() and divb_cen_calc() routines;

-- noticed that in the magbondi test that divb and divb_cen became non-negligible
	after t=0;
	-- does this mean that the CT is not working or that the initial data (or
		the initial data with the evolution violates the constraint?


#############################
Sat Dec 30 21:28:40 EST 2006
-------------------------------

-- finished testing MPI routines. Should be easy to implement
   into rest of harm.

-- added mpi_global_min(), mpi_global_max() that are needed to find the min./max.
	values over all domains;


-- adding mpi routines into harm3d...
   -- changing dump_sdf() to initialize sdf-specific stuff (alloc work array, naming functions)
   -- write_chunk() in harm_mpi.c  will now be responsible for calculating any functions that
      we want to dump via sdfs;
   DONE -- need to finish changing write_chunk();
   DONE -- need to change all "exit()" calls "myexit()" calls;

	-- need to change certain N1/N2/N3 -> totalsize[1-3]
	-- need to change dump()... how?
	DONE-- need to change timestep() to get global min. dt;
	-- need change dump() so that it either gathers all primitives then dumps,
		or dumps one primitive at a time;
		-- maybe the latter since that is what Hawley's code does;

	-- going through all the files to see if I've overlooked MPI-inconsistencies:
		-- step_ch.c clear
		-- main.c clear
		-- init.torus.c clear
		-- coord.c clear
		-- metric.c  clear
		-- restart.c ...


-- adding torus initial data;
	-- orientation of initial data is wrong...
		-- dump_sdf() problem?
		-- tried changing orientation of array in dump_sdf() but now
			data is scrambled
		-- changed it back, doubled the number of points in X1 direction and now
			there are two disks along X1
		-- what's going on ??!?!????

	-- was doing the mapping twice, scrambled the data;
	-- this problem is finally fixed;


-- there were problems with bounds() calls being commented out in step_ch.c

-- a fixup(p) should've been a fixup(ph);

-- seems to be working now, running it with hydro-only torus data at 64^2
	-- hydro-only test failed when waves hit the axis;
	-- noticed that fix_flux() call was commented out, maybe we actually
		need this..., trying WITH the call...
		-- still problems
		-- see gamma begin to diverge at rout above the disk at t=60-62
		-- see v1 begin to diverge at t=64,
	-- new problems problaby the result of not interpolating over instance
		when rho,u < 0  from utoprim()
		-- trying this out we find many instances in which no stencil is
			found.  It seems that we are encountering more failures though...

	-- differences that I see between harm3d and harm2d:
		-- the interpolation fixup stencils are different
		-- the harm2d run was using the Font fix (now running it without)
		-- harm2d is using the inflow_check() routine (will implement into harm3d)
		-- ??

-- added inflow_check() routine to bounds.c that is very much like that in harm2d;

-- only algorithmic differences that I can think of now is the fixup_utoprim() routine;

-- we find that harm3d is more symmetric (w.r.t. the equator) for longer than harm2d,
	though harm3d's polar atmosphere regions become hot near horizon then
	its radial and azimuthal coordinate velocities become considerable

-- adding harm2d's fixup_utoprim() routine to fixup.c as fixup_utoprim_2d():
	-- keeping it phi-symmetric by not doing interpolation in this dimension;

-- differences between harm2d and current harm3d:
	-- harm2d interpolates using either x or + 4-neighbor interp. stencil;
		-- if both stencils include at least one bad cell, then it
			averages rho,u over all 8 neighbors (regardless of
			whether or not they are failures) and sets v^i = 0;

	-- harm3d interpolates using any available stencil as long as at least
		4 of them work;
		-- if there are no viable stencils, then it leaves the cell
			the way it was;


-- compare quantitatively the initial data in harm3d to that of harm2d and they are
	identical to machine precision


-- implemented harm2d type of fixup_utoprim() (named it fixup_utoprim_2d()) and
	now there are so many failures that the disk just disappears;
	-- pflag[] is set/reset differently in harm3d;  harm2d resets pflag[]
		by assigning it the return value of Utoprim(), while harm3d
		explicitly resets it at the end of fixup_utoprim();


-- setting to constant time throughout integration to compare quantitatively to harm2d;
	-- changes have been made where comments "//-const" appear;
	-- need to change back after making comparisons;



-- remaining differences between harm3d and harm2d:
	-- the order in which boundary conditions are set
	-- flipping sign along boundary for u^2, B^2
	-- need to check to see if metric functions are the same
	-- initial data IS the same;
	-- diff. NG (how could this effect anything?)
	--



-- adding gdump() routine to dump.c ;

-- added standard double and int format strings (FMT_[DBL,INT]_OUT)

-- changed order of boundary conditions so that they are set in
	the order FACE1, FACE2, FACE3
	-- this is like how harm2d does it
	-- it also reduces the number of times inflow_check() needs
		to be called and may be more correct since it does
		not have to use metric functions in ghost zones
		(i.e. on the other side of the axis);



-- changed the order the boundaries are set to be similar to harm2d;
	only difference now is the flipping the sign of U2 at the axis
	and anything else that we are forgetting, and I am seeing
	big differences at t~=460 at outer radial boundary at the
	the equator (large sudden density difference);

	-- should verify that metric functions are the same

-- comparing gdump's between harm2d and harm3d:
	-- header info is fine;
	-- realized that connection is calculated in completely different ways....


-- doing harm3d using antisymmetric condition on U2 and harm2d run with exactly
	the same coordinates;
	--- is MUCH more similar to harm2d run and does NOT have the strange gamma
		spike at late times.
	-- sill hase large difference at equator at r=Rout which is probably
		due to their different conn[] calculations;


-- writing new, hydro.  initial data that will test out the anti-sym. vs.
	sym. BC along axis.
	-- init.*.c files called init.axistest1.c and init.axistest2.c
	-- init. data is a Gaussian independent of z that is pressure dominated
		so that is spreads out, matter reflects through axis;
	-- the idea is to compare a (x,y) evolution against an (r,th) version;
	-- first try uniform r,th coordinates, then try DIAGONAL coordinates
		then try MIXED coordinates;

#############################
Fri Jan 19 10:46:38 EST 2007
-------------------------------

-- Julian seems to be fine with the state of the code and wants me to progress;

-- plan:
	-- test out MPI:
		-- first do 2D MHD torus run in unigrid (randfact=0, but with an
			known perturbation so that turbulence is hastened yet it
			evolves in the same way over runs/processors)
			-- need to change init. data
			-- make sure b-fields are calculated correctly
			-- make sure fix_flux() is right;

		-- do parallel run in 2d of same data;

		-- do unigrid 3d run
		-- do parallel 3d run

	-- implement Hawley axis boundary:
		-- add reflecting boundary conditions along axis
		-- add new grid routines to accomodate cut out.



-- adding find_min_dx() that returns with the smallest length scale in non-numerical
	coordinates;


-- added axis cutout coordinates;

-- had to alter diag() for MPI purposes (slaves were not reaching the dump(), etc.
	commands that they needed to in order to transfer data);

-- made gdump() dump files locally per cpu;


#############################
Mon Jan 22 10:12:06 EST 2007
-------------------------------

-- trying out new reflection/excised condition at the axis....
	-- found many failures at j=N2S+1 (2nd cell from the axis),
		notably failures w/ large gamma's
	-- realized that I was implementing the condition wrong
	-- the correct way is to copy all fields the first physical cell
		into the ghost cells; then the sign of U2 in the ghost
		cells is flipped



-- debugging mpi ...
	-- problem with initial data that the magnetic fields are not set
		along equator;
		-- probably due to the fact that the vector potential is
			not set in the ghost zones (domain is mpi-split
			along equator) and so we get funny stuff there
		-- no it was that bounds_mpi() was only setting boundary
			conditions on p[] and not ph[];  changed it so that
			it takes the primitive variable array as an argument;

	-- problem with find_min_dx();  the min_dx found is smaller than unigrid
		version;
		-- replaced use of N1 w/ totalsize[1] in Rin_calc();



-- having problems along boundary
	-- am not running with flux_ct() being used;
	-- need a 2d version of divb_calc();
	-- w/ new divb_calc(), confirm that flux_ct() is working;
	-- now the runs are stable but gammas reach GAMMAMAX as soon as
		matter hits the axis;
	--


-- every 100M  restart_write() or diag() generates many rdumps in succession;
	-- this is only seen in MPI runs, so I think it is an MPI malfunction
		without output through stdout;


-- fixed a bug with fixup();  icurr/jcurr/kcurr were not set before
	a call to fixup() which resulted in incorrect nfail reporting;


-- adding condition that ensures that B^i_L = B^i_R  along face perp. to x^i;
	-- new condition is in numerical_flux();
	-- need to make sure that this is always the right thing to do (does
		the divergence constraint imply this?  the shock conditions?
		even if the shock conditions imply this then are the shock
		conditions implemented correctly by doing this w/ the
		reconstruction procedure in mind? )


-- corrected phi-sym. boundary condition in setup_mpibc_info_sym() when
	 there is only one domain in phi direction;  no longer
	uses MPI to set the ghost zones like it used to.

-- added support in recon.c for recon_type[] grid function that controls
	what reconstruction method to use at a given cell in a given
	direction.



-- trying poloidal boundary conditions that only require U2=0 along
	the boundary, all other quantities are constant through the
	boundary;
	-- trying with and without reconstruction using geometrized
		B-fields;
		-- geometrized reconstruction:
			-- worse div.b along boundary
			-- more utoprim failures
			-- "noisier" in UU, U1-3 near horizon
			-- considerably slower (~30-50% slower)



-- adding set_cutout_boundary3() that sets U2=B2=0 at boundary so that
	there is no poynting flux (b^2 b_t) through the boundary
	and so that matter cannot travel through boundary along
	field line;

-- doing a 3d run where all processors used the same sequence of random
	numbers for the UU perturbation;  hence, the perturbations
	are azimuthally symmetric and any non-axisymmetries will
	not show up for some time;  after t=280, run is still
	VERY symmetric;
	-- this was with LF,minmod,RESCALE_B=1,NOT CUTOUT

	-- now doing 3d run different random number sequences...
		-- with LF,PPM+MC,RESCALE_B=0,NOT CUTOUT


-- trying set_cutout_boundary():
	-- copies everything through boundary, except u^2 is interpolated
		so that u^2 = 0 at boundary;
	-- tried with (RESCALE_B = 0) and (RESCALE_B = 1)
		-- see run1 and run2 below

-- tried a run with no cutout (no cutout bc) and there were
	many gamma max failures along the axis;
	-- otherwise everything else looks pretty good;

	-- trying it out doing fixup_utoprim() over these failures (euler:~/other/magtorus7);
		-- this run also flips sign of B2 emf-flux in polar ghostzones ;
		--

	-- also trying using set_cutout_boundary3() with DX2_CUTOUT = 0 (bh15:harm3d-2)





-- trying set_cutout_boundary3():
	-- copies everything through boundary, except u^2 and B^2 are interpolated
		so that B^2 = u^2 = 0 at boundary;
	-- tried with (RESCALE_B = 0) and (RESCALE_B = 1) ;
		-- RESCALE_B = 0:

		-- RESCALE_B = 1:
============================================================================
============================================================================
TEST RUNS
============================================================================


run1:(magtorus,euler:~/other/magtorus)
	-- DX2_CUTOUT=0.02
	-- using BC_SPHERICAL_CUTOUT
	-- using set_cutout_boundary():
	-- does NOT fix over instances in which gamma>=GAMMAMAX
	-- RESCALE_B = 0
	-- LF+MC
	== more failures than run2 right along edge (most of the time
		it fails to invert there):
	== but fewer failures elsewhere and evolves fine;
	== large gammas along boundary that don't change
		over time;

run2:(magtorus2,euler:~/other/magtorus2)
	-- DX2_CUTOUT=0.02
	-- using BC_SPHERICAL_CUTOUT
	-- using set_cutout_boundary():
	-- does NOT fix over instances in which gamma>=GAMMAMAX
	-- RESCALE_B = 1
	-- LF+MC
	== more failures than run1 outside the boundary, especially
		in polar regions above the horizon;
	== eventually leads to catastrophic failures

run3:(magtorus3,euler:~/other/magtorus3)
	-- DX2_CUTOUT=0.02
	-- using BC_SPHERICAL_CUTOUT
	-- using set_cutout_boundary3():
	-- does NOT fix over instances in which gamma>=GAMMAMAX
	-- RESCALE_B = 0
	-- LF+MC
	==

run4:(magtorus4,euler:~/other/magtorus4)
	-- DX2_CUTOUT=0.02
	-- using BC_SPHERICAL_CUTOUT
	-- using set_cutout_boundary3():
	-- does NOT fix over instances in which gamma>=GAMMAMAX
	-- RESCALE_B = 1
	-- LF+MC
	== less stable near the horizon
	== more failures near the horizon and in the corona region
	== catastrophic failures like in run2


run5:(magtorus5,bh15:/fs1/scn/harm3d-2)
	-- like run3 except DX2_CUTOUT=0, using set_cutout_boundary3()
	== still many failures along the polar boundary
	== run3 had few if any instances of GAMMAMAX, whereas
		this run has many instances of GAMMAMAX
	== more floor hits, but also we have more vacuum in this run
	== more inversion failures, but also more "floor" area;
	== gamma grows larger against polar boundary than in run3,
		but varies more too (the gamma profile was quite
		static along the boundary in run3);

run6:(magtorus6,euler:~/other/magtorus6)
	-- like run5 execept using BC_SPHERICAL_OUTFLOW  (only reflecting U2 and not B2)



run7:(magtorus7,euler:~/other/magtorus7)
	-- like run6, with DX2_CUTOUT=0 but now reflecting U2 AND B2
	-- also, reflecting r-dir B2 fluxes along boundary
	-- also, interpolating over GAMMAMAX instances;
	==


run8: (magtorus_harm2d-release, bh02:~/harm2d_3dcomparison)
	-- original harm2d release version
	-- (rerunning now since before we ran w/o B-fields)
	== seems to also have large values of gamma just along the
		boundary too;
	== we definitely don't see this at higher resolution runs with
		the new MIXED coordinates (that's not necessarily the case...)


run10: (bh13:~/harm3d-run10)
	-- like run7 except we are using harm2d's fixup_utoprim() routine (fixup_utoprim2());



run11: (bh02:~/harm2d_new,magtorus11)
	-- non-release version of harm2d without FONT-fix and without reconstruction of gamma
	-- note using different scheme than harm3d to set Rin (Rin=1.00 here)

run12: (bh12:~/harm2d_new,magtorus12)
	-- non-release version of harm2d WITH FONT-fix and WITH reconstruction of gamma
	-- note using different scheme than harm3d to set Rin (Rin=1.00 here)


run13: (bh13:~/harm3d-run13 )
	-- like run7 but using Julian's new idea for not interp-fixing
	       w/ ghost cells or cells within the horizon (using fixup_utoprim3())
	==  does not seem to make much of a qualitative difference, still large
	    gamma's along the axis
	== still no failures to find good fixup_utoprim() stencils;


run14: (bh13:~/harm3d-run13 )
	-- like run14 but using ppm+mc


run15: (??)
       -- like run7 but with 256x256 points and with PPM+MC reconstruction;


run16: (??)
       -- 3d run;





#############################
Tue Feb  6 17:46:53 EST 2007
-------------------------------


-- decreased the number of "good" lines needed to perform fixup_utoprim() interpolation;
	-- we were demanding that virtually all lines in 2d were needed;


-- adding fixup_utoprim3() that uses Julian's idea of not using ghost cells or
	cells within the horizon as 'good' cells for fixup interpolation;



-- noticed that in a 3d run with only ncpux[3]=2, there were problems with the
   density diminishing along the mid-phi plane.  This was because the MPI comm.
   "tag" variables were not unique between different boundaries for the same subdomain;
   since there were only 2 subdomains in the phi-direction, then the cells had to share
   both boundaries with the same subdomain, but "tag" was only a function of pid and not
   the particular face it was being used for.
   -- Anyway, I fixed this (now there are tag_send and tag_recv variables)
   -- Try out init.blob.c to test out the periodic boundary conditions in the phi-direction;


-- adding init.kd.c  which is the keplerian disk initial data used by Hawley and Krolik
   -- I think I've figured out the notation used in their code, comparing with the
      paper http://astsun.astro.virginia.edu/~jd5v/KDdisks.pdf  (arXiv:astro-ph/0307260)



#############################
Thu Feb 22 10:52:22 EST 2007
-------------------------------

-- adding routines to calculate the current:
	-- added faraday_calc() to phys.c to calculate the faraday tensor;
	-- added set_levi_civita() to phys.c that sets the static array that
		represents the 4d anti-symmetric tensor;
	-- added call to set_levi_civita() to set_arrays() in main.c;

	-- we need all components of fcon at cell center;
	-- we need all 'i' components of fcon[i][k] on the 2 k-faces (incl. k=0 faces);
	-- jcon must be centered at the half-step in order for the time derivative
		to be second-order; this requires that the spatial components of jcon[]
		full steps to be averaged to the half-step;

	-- will be storing faraday[] (gdet*Faraday tensor) at faces of cell's 4-volume;
	-- created test/comparison routine current_calc2() in phys.c  that uses a
		different stencil;
		-- uses p_old[] to save previous time-step's prim. var's;
	        -- added jcon2[] for sdf dumps;
		-- will probably want to comment this routine out and remove
			references to p_old[] and jcon2[] after tests;


	-- tested levi_civita[] calculation;

-- removed get_geometry() calls from the N3 loops in step_ch.c and fixup.c so that
	we don't have to do this excessively;



-- I'm getting
               my l_in     their l_in
      a=0.      4.2789      4.66
      a=0.5     4.2789      4.61
      a=0.9                 4.57
      a=0.998               4.57

	-- realized that the derivation of the initial data equations
		assumed BL metric;
	-- changed init.kd.c so that it used BL metric into the end,
		at which point the velocity was transformed to KS;
	-- now it compares with the numbers in De Villiers et al. Paper 1;
	-- now the data looks reasonable but the disk moves outward radially
		at nearly luminal speed;
		-- found that this was because I failed to transform the 4-velocity
			into numerical coordinates;
	        -- the disk is now fairly stationary ;
	-- now there are problems with the floor within t<2M;
		-- the problems entail a sudden increase in uu at constant
			radii above/below the disk;  the increase is a few orders
			of magnitude above the floor but MANY orders of magnitude
			below the disk;
		-- this is seen with and without interpolating over the floor;
		-- thinking it was the inversion's fault (inaccurate for small velocities)
 			I wrote a new version of the 2d scheme called utoprim_2d_new.c
			that Taylor expands sqrt(1-vsq);
			-- this didn't help at all;
		-- the problem is sensitive to the floor magnitude; when I increased
			it a couple of magnitudes, the problem went away;
			-- maybe with still has to do with the inversion method, but
				does not involve the velocity (maybe there's a
				minimum value that h can be);


-- added FLOOR_TYPE_CHOICE to decs.h and the types "HARM" and "HAWLEY" to
	organize each different type;
	-- changed fixup1zone() to use RHOPOWER AND UUPOWER now;
	-- may want to figure out a better way of setting the floor so that
		it can handle initial rho profiles that are not normalized to unity
		like the Hawley initial data;



-- trying out init.kd.c again;
	-- for a=0 and treat_floor_as_failure=0, it works fine;
	-- for a=0.95 and treat_floor_as_failure=1,  I see ripples develop
		in uu and v1 (not rho) at small radii.
	-- for  a=0.0 & treat_floor_as_failure=1, still see ripples;
	-- for  a=0.0 & treat_floor_as_failure=0, still see ripples;
	-- I must have changed something else... the floor?

	-- for a=0 & treat_floor_as_failure=0 :
		-- using FLOOR_HARM w/ 1e-10/1e-12 RHOMIN/UUMIN:  fine, no ripples
		-- using FLOOR_HARM w/ 1e-12/1e-14 RHOMIN/UUMIN: no ripples
		-- using FLOOR_HARM w/ 1e-12/1e-16 RHOMIN/UUMIN: (lower than FLOOR_HAWLEY) see ripples
		-- note above we haven't seen ripples, it was in uu, but we always see ripples in
			gamma

	-- a=0.95 FLOOR_HARM w/ 1e-12/1e-14 RHOMIN/UUMIN w/ treat_floor_as_failure=1:
	-- using RHOMIN/UUMIN 1e-7/1e-9 showed signs of ripples in gamma;
	-- trying higher-resolution in r:
		-- N1->256 (was 128)
			-- still see them
		-- trying with init.torus.c

		--


-- magnetic field is evolving fine with init.kd.c (preserving div.B=0)


-- adding testing program for the hdf5 routines:
	-- cvs add testing/hdf5
	-- cvs add testing/hdf5/main.c
	-- cvs add testing/hdf5/Makefile
	-- cvs add testing/hdf5/README.txt



-- changed MAKESDFS -> MAKE_SDFS

-- added pre-compiler flag MAKE_IMAGES  that works like MAKE_SDFS but for images;


-- renamed  dump() -> dump_asci();

-- OUT_LOG -> OUT_HISTORY
-- OUT_DUMP -> OUT_ASCII
-- added OUT_HDF5

-- replaced DTd,DTl,DTi,DTs with DT_out[N_OUT_TYPES]

-- replacing dump_cnt, im_cnt, log_cnt    with N_out[];
-- replacing t_dump, t_log, t_image,    with T_out[];

-- removed ener.out file;  history file will take it's place;


-- homogenized way in which the output routines are called, except
	the restart routines;
	-- that is, which diag.c, all the routines are called in the
		same way and are indexed by the mnemonics OUT_*
	-- moved rdump_cnt into restart.c ;

-- adding new file dump_hdf.c that contains all the routines to deal with
       the hdf format;

-- adding dump_history.c to hold the routines used to generate the history file;

-- note that MAKE_HDF5 controls whether dump_history() writes to hdf files or ascii;


-- added macro USE_LOCAL_RECON_TYPE  that controls whether to use dynamically
	changing (and cell-to-cell) control over the reconstruction method;
	-- this is really slow, so we don't always want dynamic control;


-- added many routines and changes to dump_hdf.c and dump_history.c
	-- the history only uses hdf5 format;


#############################
Mon Mar 12 19:35:41 EDT 2007
-------------------------------

-- trying out hdf and history routines;
	-- with  MAKE_HDF5=1, MAKE_HISTORY=1, MAKE_SDF=0
		-- problem with rdump_cnt becoming random (memory overflow??)
		-- generates history data, but that looks wrong (i.e. not aligned in memory?)

	-- ran it with MAKE_HDF5=0, MAKE_HISTORY=0, MAKE_SDF=1  and it runs fine;

	-- trying with MAKE_HDF5=1, MAKE_HISTORY=0, MAKE_SDF=0
		-- figured out that we were calling dump_hdf() wrong from diag();
		-- had to make dump_hdf() -> dump_hdf_gen() , and then make
			create wrapper routine so that diag() calls to dump()
			were of type "void dump(void)";
		-- also fixed an indexing bug in set_hdf5_gfuncs();


	-- history still has problems;
		-- the integrate proper volume is zero some places, which probably
			indicates that the entire history integration calculation is
			messed up;


-- testing out hdf routines in serial:
	-- I used hdf5_to_sdf.c to make sure that the full dumps look right;
	-- I compared analytic gdet to gdet in gdump file;
	-- I compared gdet integration (H_vol) to analytic function;
	-- they appear to work;



-- trying to resolve ripple  features:
	-- using FLOOR_HAWLEY, RHOMIN = 5e-11, UUMIN=6e-16,  PPM+MC:
		-- ripples w/ wavelength = 2*dx1;

	-- using FLOOR_HAWLEY, RHOMIN = 5e-11, UUMIN=6e-16,  PPM+MM:
		-- ripples w/ wavelength = 2*dx1, but then around the
			equatore there are failures that smear out ripples;
	-- using FLOOR_HAWLEY, RHOMIN = 5e-11, UUMIN=6e-16,  RECON_MC:
	-- ripples only appear with PPM;
	-- also found that ripples go away if you rescale the primitive variables
	   by functions of r^p so that the high-order reconstruction is not performed
           on functions that are inverse polynomials;
	   -- what's the best polynomial to rescale by?  if it's too high-order then
	      the rescaled function will be of high-order and the linear/quadratic
	      interpolation method will not reconstruct the function as well;


-- looking at profile I did before, noticed that mhd_calc() was slow, so
   I optimized all that I could;
	-- removed the conditional operation within the DLOOP1 so that
		 it can be vectorized;
	-- removed extraneous assignments;

	-- mhd_calc() calls:
		-- primtoflux()
		-- source()
		--
	-- get_state() calls:
		-- advance() loop
		-- source()
		-- numerical_flux() loop (twice)
		-- bsq_calc()
		-- current_calc()

	-- primtoflux()
		-- advance() loop
		-- numerical_flux() loop (twice per get_state)


	-- I noticed that bsq is calculated each time mhd_calc() is called;
		since bsq is just a function of the state, then it can be calculated
		when the state is calculated;  this saves about 100 multiplications
		and 100 additions per zone cycle and was trivial to implement;



#############################
Thu Mar 29 18:21:35 EST 2007
-------------------------------

-- added to RESCALE_R code so that B-fields are rescaled like UU;
   -- need to test out if this is a good scaling;


-- added pre-compiler logic to make reconstruction process in reconstruct()  recon.c
   fully 2D/1D if there is only 1 real cell in a dimension;
	-- there is about a 20% speedup with this change, but we can still improve
		efficiency if we want by restricting how fluxes are calculated;


-- testing out changes to history file;


#############################
Sun Apr  8 22:32:15 EDT 2007
-------------------------------

-- 3d run is going very slowly; if it keeps this pace the it will take
   about 100 days  to finish;
   -- profiled the code again and the slowest routines are
      para(), vchar(), mhd_calc(), primtoflux();
   -- I do not know how to optimize para() any more (maybe expand
      the loop, but the compiler should do this);



-- adding Julian's cooling function;  here are his notes :

------------------------------ JULIAN ------------------------------------------------------------------------
The discipline of writing covariant equations is very useful---it reminded
me that we need to account for the momentum of the photons as well as their
energy.   To be specific, the way to write the cooling equation is:

T^\mu_{\nu ; \mu} = -f(\epsilon) u_\nu

where \epsilon is the proper internal energy density and f(\epsilon) is the
cooling rate in the fluid rest frame.  When this equation is evaluated in the
fluid frame, u_\nu = (-1,0,0,0) if the radiation is isotropic; in other
words, there is energy loss but not momentum loss.   In any other frame,
there is momentum loss proportional to the velocity of the fluid in that
frame, of course.

We have a good deal of freedom in choosing the toy-model for the cooling rate.
Here's one proposal:

Let {\cal T}(r) = A2 r2 \Omega2 (r), where A = h/r, the desired aspect ratio,
r is the radial coordinate, and \Omega (r) is the orbital frequency in the
equatorial plane at that radius.   Write E = \epsilon/(\rho{\cal T}).
Then take

f(\epsilon) = s \epsilon \Omega [ (E - 1) + |E - 1| ]^q

where s \lesssim 1 and q \gtrsim 1.    The expression (E-1) + |E-1| > 0 if E > 1,
but = 0 if E < 1.   That way, there is cooling only when the temperature exceeds
the target temperature, and the cooling rate goes continuously to zero as E
approaches unity from above.   The cooling rate is of order \Omega, but slower
if s < 1.    If we want very rapid equilibration, we can choose q > 1.

Because \epsilon and \rho are genuine scalars, while everything else may be
regarded as scalar parameters, these expressions are fully covariant.

                       Julian


------------------------------ JULIAN ------------------------------------------------------------------------




-- working on making the hdf5 restart read working;
	-- adding a few parameters to the restart/full dumps;
	-- global variable called "using_restart" that if set to non-zero value
		indicates that we are starting from a restart file;
		-- this is used in initializing output variables, etc.;
	-- when starting from a restart, we also immediately dump a file called
		"rdump_starttest*.h5"  that should be a copy of the restart
		file just read in;
		-- when testing this out (comparing read in restart file to
			"rdump_starttest*.h5" with "h5diff" command) I found no
			differences, so it looks like it works;


#################################
Mon Apr 16 16:16:37 EDT 2007
-------------------------------

-- added quadrapole B-field initial configuration to init.kd.c
-- also moved the new macros for this and "HYDRO_ONLY" to the top of init.kd.c
	so that changing them is easier;


-- added new routine myH5_write_scalar2()  that does not use the H5Sselect_none() function
	and write scalar data once per processor leading to redundant data output;
	-- it is our hope that this will cure the hdf write problem at NCSA;
	-- added similar myH5_read_scalar2();
	-- works on mercury, queued up on sdss;


-- need to rewrite hdf5_to_sdf() now since now the "scalar" variables are
	considered "gridfunctions" by that program;

-- found out that the scaling in uu is 1/r  initially using a flat floor ;
	-- tried out several scaling powers for uu (include 1/r) and I still cannot
		find the right scaling;
	-- all other's scalings seem fine;
	-- I think this has to do due with the 2P/r mismatch cancelation to within truncation
		error that is seen in spherical sym..
		-- if that's the case, then it's hopeless;
	-- it looks like a 128^2 evolution has ripples only near the beginning when
		the floor is used;


-- ok, i cannot use H5Sselect_none() ever, so we need to dump the gdump functions
	over the whole domain even though it is x3-independent;
	-- now dumping gdump data over entire domain;
	-- using myH5_write_gfunc() instead myH5_write_gdump_func();
	-- see  comments  "//-teragrid"  if this is fixed in the future to change back;



-- a note from the HDF group (THG) mentioned that they think at first glance that my
	problem is the result of a bug in hdf1.6.5 and that it has been fixed in
	hdf1.8.0alpha;


#############################
Fri Apr 20 23:34:50 EDT 2007
-------------------------------

-- working on optimizing the code

-- added macro called "MAX_NTOT" to recon.c which is set to the max. value of N1TOT,N2TOT,N3TOT
   which will be used to allocate work array for vectorized recon method;



-- look in
/data1/euler/scn/harm3d/harm3d/profile/04212007/harm3d
	for code to profile against;
	-- uses icc so we don't have to worry about compiler differences;
	-- there is a gprof.out file that has a report of the profile run;


-- comparing the runtime of 100 reconstructions using new and old routines (using PPM+MC):
     new routines:  time =  13.671u 0.168s 0:13.85 99.8%    0+0k 0+0io 0pf+0w
     old routines:  time =  30.362u 0.129s 0:30.55 99.7%    0+0k 0+0io 0pf+0w

-- comparing the runtime of 100 reconstructions using new and old routines (using MC):
	new : 9.349u 0.158s 0:09.52 99.6%     0+0k 0+0io 0pf+0w
	old : 8.890u 0.147s 0:09.05 99.7%     0+0k 0+0io 0pf+0w

-- comparing the runtime of 500 reconstructions using new and old routines (using MC):
	new : 46.762u 0.265s 0:47.05 99.9%    0+0k 0+0io 0pf+0w
	old : 44.221u 0.245s 0:44.55 99.7%    0+0k 0+0io 0pf+0w

-- comparing the runtime of 500 reconstructions using new and old routines (using MM):
	new: 31.998u 0.255s 0:32.30 99.8%    0+0k 0+0io 0pf+0w  (~1.9e6 zone-recons/sec.)
	old: 25.008u 0.198s 0:25.23 99.8%    0+0k 0+0io 0pf+0w

-- comparing the runtime of 500 reconstructions using new and old routines (using PPM_MC):
	new: 67.738u 0.282s 1:08.04 99.9%    0+0k 0+0io 0pf+0w   (~9e5 zone-recons/sec.)
	old: 151.288u 0.504s 2:31.82 99.9%   0+0k 0+0io 0pf+0w   (~4e5 zone-recons/sec.)


-- after a little more optimization, here are the final benchmarks:
	500 recons:
		PPM_MC:
			new: 64.812u 0.451s 1:05.30 99.9%    0+0k 0+0io 0pf+0w
			old: 151.686u 0.429s 2:33.50 99.0%   0+0k 0+0io 0pf+0w

		PPM_MM:
			new: 49.862u 0.251s 0:50.15 99.9%    0+0k 0+0io 0pf+0w
			old: 128.297u 0.183s 2:08.52 99.9%   0+0k 0+0io 0pf+0w
		PPM_VL:
			new: 66.060u 0.280s 1:06.38 99.9%    0+0k 0+0io 0pf+0w
			old: 133.946u 0.361s 2:14.45 99.8%   0+0k 0+0io 0pf+0w
		MC:
			new:43.270u 0.216s 0:43.52 99.9%    0+0k 0+0io 0pf+0w
			old:44.137u 0.226s 0:44.39 99.9%    0+0k 0+0io 0pf+0w

		MINMOD:
			new: 24.925u 0.177s 0:25.13 99.8%    0+0k 0+0io 0pf+0w
			old: 25.075u 0.126s 0:25.23 99.8%    0+0k 0+0io 0pf+0w
		VANLEER:
			new: 31.708u 0.207s 0:31.97 99.7%    0+0k 0+0io 0pf+0w
			old: 32.150u 0.199s 0:32.41 99.7%    0+0k 0+0io 0pf+0w



-- adding new recon.c methods;

-- noticed that ucon_calc() and mhd_calc() and bcon_calc() are slow.
	-- trying new forms out that should be faster;
	-- added grid functions g_alpha[] and g_beta[] (and member
		of_geom.alpha and of_geom.beta[]) so that we
		don't have to re-calculate these quantities in ucon_calc()
		each time;

	-- tried out run w/ passing gcov[][] instead of *geom to gamma_calc()
		and this made little difference ;
	-- right now vchar() is the slowest;
		-- making vchar_fast();

	-- added "eta", "ptot" and "p"  elements to of_state structure to eliminate
		repeated calculations thereof in calls to primtoflux()/mhd_calc()
		that have the same state;


-- benchmark: 20 steps from t=0 KD data, not using -qp, using -O2:
	-- new version: 04212007_6
		58.401u 0.389s 0:58.89 99.8%    0+0k 0+0io 0pf+0w

	-- orig version: 04212007
		76.205u 0.365s 1:16.85 99.6%    0+0k 0+0io 15pf+0w

	-- new version: 04212007_7
		53.525u 0.466s 0:54.03 99.9%    0+0k 0+0io 0pf+0w

	-- new version: 04212007_7 w/ MC instead of PPM
		50.015u 0.355s 0:50.77 99.1%    0+0k 0+0io 0pf+0w

	-- with -ipo -ipo_obj

	-- with -mp -ipo -ipo_obj
		67.000u

	-- with -mp1 -ipo -ipo_obj
		46.180u 0.440s 0:46.66 99.9%    0+0k 0+0io 0pf+0w

	-- with -ipo

	-- with -ipo -ipo_obj

	-O3 -march=pentium4                     50.957u 0.453s 0:51.52 99.7%
	-O3 -march=pentium4 -ipo -ipo_obj -mp1	46.441u 0.425s 0:46.94 99.8%
	-O3 -march=pentium4 -mp 		73.115u 0.509s 1:13.76 99.7%
	-O3 -march=pentium4 -ipo -ipo_obj 	46.242u 0.369s 0:46.78 99.6%
	-O3 -march=pentium4 -ipo		46.111u 0.445s 0:46.56 99.9%



	-- new version: 04212007_7  on running on mercury
	Thu Apr 26 17:52:03 CDT 2007
	Thu Apr 26 17:54:20 CDT 2007
	    = 137 sec.

	-- new version: 04212007_7  on running on mercury w/ -O3
	Thu Apr 26 18:20:02 CDT 2007
	Thu Apr 26 18:22:09 CDT 2007
	    = 127 sec.




-- benchmark: 100 steps from t=0 KD data, not using -qp, using -O2:
	-- new version: 04212007_6
		316.551u 1.449s 5:22.02 98.7%   0+0k 0+0io 0pf+0w

	-- orig version: 04212007
		405.816u 0.605s 6:48.44 99.5%   0+0k 0+0io 0pf+0w

	-- new version: 04212007_7
		260.622u 0.466s 4:21.25 99.9%   0+0k 0+0io 0pf+0w


-- taking the 20 times steps, runtime=46sec.  number, that results in
	~1e5 zone-cycles /sec.    which is actually a factor of 5
	larger than what I thought harm2d could do.


=======

#############################
Mon Apr 23 22:47:28 EDT 2007
-------------------------------

-- adding utoprim2d_fast.c
-- added global variable : gam_m1_o_gam = (gam-1)/gam


-- just tried unrolling a small loop explicitly and got a tremendous
	speedup with either -O3 or -O2 compiler options w/ Intel compiler;


-- unrolling loops explicitly makes for more efficient code per a test I did;

-- am now unrolling some set-limit short loops that are done a lot;
	-- most of them are either NDIM loops or NP loops, so these changes
		 no longer make this code independent of NP and NDIM values;
	-- if you want to change NP, look for code near comments with "//-fast"



-- looking at quadrupolar run that Kris requested;
	-- explosion occurs at 1200, I think from matter falling/reflecting
		from outer boundary along the axis;
	-- there are many fixup failues (FAIL_FIXUP) in the (i,j)=(0,N2) grid point
		for all domains in the outer half of the radial extent;
		-- is this real or a bug?


-- tried to use -prof_gen and -prof_use  icc compiler flags;
	-- did not work, actually was 2 sec. longer in the above 100 step run;



-- see a 6% difference in runtimes between short PPM and MC runs;

-- using the same 64^2 run setup (down to the same ranc() numbers),
	found differences at t=0 of ~1e-8 to 1e-10.
	-- this is expected since there should be difference in
		arithmetic at the roundoff level (1e-16 - 1e-18)
		and we take sqrt()'s of numbers, give us the 1e-8
		figure;
	-- same result in 3d;



-- profiling the MPI decomposition on mercury:

       ncpu1 x ncpu2 x ncpu3   |   N1 x N2 x N3  | ncpus  | # time steps  | # z-cycles | Time (s)| tot. z-cyc/sec. | z-cyc/sec/cpu | SUs (n_cpu*n_hr) | Est. run-time (days)| Est. SUs
----------------------------------------------------------|---------------|------------|---------|-----------------|------------- -|------------------|---------------------|-----------
1)	   3 x     3 x     1   |  64  x 64 x 64  |    9   | 	568       |  1.34e9    |  3120   | 4.295e5         | 4.7722e4      |  7.8	      |	181		    |39096
2)	   4 x     4 x     1   |  48  x 48 x 64  |   16   | 	567	  | 1.3377e9   |  1825   | 7.32986e5	   | 4.58116e4	   |  8.1 	      |	106		    |40704
3)	   4 x     4 x     2   |  48  x 48 x 32  |   32   | 	567	  | 1.3377e9   |  1037   | 1.28997e6	   | 4.031156e4	   |  9.2	      |	60		    |46080
4)	   5 x     5 x     2   |  38  x 38 x 32  |   50   | 	562	  | 1.2984e9   |   754   | 1.722e6	   | 3.444e4	   | 10.47	      |	43.6		    |52350
5)	   6 x     6 x     2   |  32  x 32 x 32  |   72   | 	566	  | 1.33536e9  |   636   | 2.0996e6	   | 2.916e4	   | 12.7	      |	37		    |63936
6)	   7 x     7 x     2   |  27  x 27 x 32  |   98   | 	559	  | 1.277955e9 |   519   | 2.46234e6	   | 2.51259e4	   | 14.1	      |	30		    |70000
7)*	   7 x     7 x     3   |  27  x 27 x 21  |  147   | 		  |	       |         |		   |		   |		      |			    |
8)	   8 x     8 x     3   |  24  x 24 x 21  |  192   | 	557	  |	       |   521   |		   |		   |		      |			    |
9)	  10 x    10 x     3   |  19  x 19 x 21  |  300   |               |            |         |                 |               |                  |                     |


-- each one used about 565 time steps

       ncpu1 x ncpu2 x ncpu3   |   N1 x N2 x N3  | ncpus  | tot. z-cyc/sec. | z-cyc/sec/cpu | SUs (n_cpu*n_hr) | Est. run-time (days)| Est. SUs
----------------------------------------------------------|-----------------|------------- -|------------------|---------------------|-----------
1)	   3 x     3 x     1   |  64  x 64 x 64  |    9   | 4.295e5         | 4.7722e4      |  7.8	       |	181	     |39096
2)	   4 x     4 x     1   |  48  x 48 x 64  |   16   | 7.32986e5	    | 4.58116e4	    |  8.1 	       |	106	     |40704
3)	   4 x     4 x     2   |  48  x 48 x 32  |   32   | 1.28997e6	    | 4.031156e4    |  9.2	       |	60	     |46080
4)	   5 x     5 x     2   |  38  x 38 x 32  |   50   | 1.722e6	    | 3.444e4	    | 10.47	       |	43.6	     |52350
5)	   6 x     6 x     2   |  32  x 32 x 32  |   72   | 2.0996e6	    | 2.916e4	    | 12.7	       |	37	     |63936
6)	   7 x     7 x     2   |  27  x 27 x 32  |   98   | 2.46234e6	    | 2.51259e4	    | 14.1	       |	30	     |70000
7)*	   7 x     7 x     3   |  27  x 27 x 21  |  147   |		    |		    |		       |		     |
8)	   8 x     8 x     3   |  24  x 24 x 21  |  192   |		    |		    |		       |		     |
9)	  10 x    10 x     3   |  19  x 19 x 21  |  300   |                 |               |                  |                     |

*:  (not doing, odd # of cpus)

Note: runs performed full I/O and variable time steps (should be the same, though), to t=2M.
      "Estimated run-time" and "Estimated SUs" are calculated assuming that the full run
	to t_final=10000M will take the same time per step as the t_final=2M run.
-- note we need to use 2 processors per node  since they
	charge for both processors even if you only use one;
	-- thus ncpus must be even;
-- note these runs dumped "gdump" and so took much longer than usual;
-- I repeated the slowest (most decomposed) runs without dumping the gdump file
	and there was little difference in run times





GRMHD 43000 zone-cycles per second per cpu  on datastar
	6x12x4    domains
	32x16x16  cells per domain


	4000M/18hr

	dt =~ 9e-3
		-- this is the value reported by Kris and is also the right order of
			magnitude for the dt's mentioned in Julian's IDL routines;

	zone-cycles/sec. =  N_cells * N_steps / run-time

	N_steps = (zone-cycles/sec.) * run-time / N_cells  = 43000 * (18 * 3600) / (32*16*16)  = 3.4e5 steps
	-->  dt = t_num / N_steps  = 4000M / (3.4e5 steps) = 1e-2



-- looking a harm fiducial 2d run
	(i.e. r(pmax)=12, Rout=40, a=0.9375,RHOMIN=1e-4,UUMIN=1e-6)

	-- I find that there are no instances of gamma reaching the it's
		max. allowed (ceiling) value, unlike the KD simulations;
	-- is the difference just the fact that you're starting from a larger
		radius?


#############################
Thu May  3 15:56:40 EDT 2007
-------------------------------

-- changed Rin_calc() so that now the horizon lands between the
	cell's edge and center so that gcon00 is never exactly 0
	in the domain.



-- in order to use a cutout, we need to make sure that the reconstructed
	values are consistent with have zero flux across the cutout boundary.
	We do this in set_cutout_boundary3() by assuming that minmod reconstruction
	will be used along that boundary and then alter U2 and B2 so that their
	reconstructed quantities at the boundary are 0.
	Since using a different reconstruction method along the NG cells within
	that boundary would be a hassle, I tried using PPM everywhere with
	set_cutout_boundary3() and BC_SPHERICAL_CUTOUT. I did the following runs:

harm3d_cutout1:
        -- using set_cutout_boundary3();
        -- setting x2 fluxes to 0 along x2 boundaries
        -- using RESCALE_B=0
        -- using ppm+mc recon everywhere
        -- using BC_SPHERICAL_CUTOUT
        -- CUTOUT = 0.038
        -- h_slope = 0.13
	-- ran fine, not catastrophic failures
	-- always reasonable values of gamma
	-- u and rho were always well-behaved
	-- v2 and B2 are smooth near the boundary, values rarely spiked in boundary
	-- most inversion and fixup failures occurred along boundary
	-- gammamax never reached;


harm3d_cutout2:  just like cutout1 except:
        -- using BC_SPHERICAL_OUTFLOW
        -- not using set_cutout_boundary3() or any other set_cutout_boundar() routine;
	-- eventually led to catastrophic failure along cutout boundary and outer radial
		boundary;
	-- run was garbage after that;
	-- before failure,

harm3d_cutout3:  just like cutout1 except:
        -- using set_cutout_boundary3();
        -- does not set x2 fluxes to 0 along x2 boundaries
	-- more dramatic behavior in all quantities along the boundary than in
		harm3d_cutout1 but not as severely different than in cutout2.
	-- there was a minor failure that dissipated away.
	-- more inversion and fixup failures along axis than in cutout1
	-- gammamax sometimes reached along the axis



-- so the conclusion is that, if using a non-zero cutout, then one should use:
	1) BC_SPHERICAL_CUTOUT
	2) set_cutout_boundary3()
	3) set x2-fluxes to zero
	4) use any recon method you like


#############################
Mon May  7 11:29:57 EDT 2007
-------------------------------

-- harm3d-kd9  (KD, a=0.9, LF, PPM+MC, cutout, 192x192x64) :
	-- running on mercury;
	-- runs labeled harm3d-kd9-test*-r1
	-- run crashed in between t=600-640M.
	-- very few inversion failures or fixup failures were reported
		till t=640M, but it seemed as though the pressure was
		reaching the floor in a region at mid latitudes
		above/below the equatorial inflow was.  Also, at t=600M, gamma
		suddenly spiked to 11 in a local region near the horizon at mid-lattitudes.
	-- re-running from t=600M dumping at higher frequency;

	-- after restarting, it seems that the failure occurred between t=636-638M;
	-- there were few inversion failures and no fixup failures until t=638M;
	-- looking at the inversion failures at t=638, it seems that most of the
		failures are along the axis/horizon edge;
	-- b^2/p <= 1e4  is large above and the thin stream of material that
		accretes from the disk to the horizon;
	-- it could be that height of this stream is not resolved, though it
		seems from the number of failures at t=638 that the catastrophic
		failure originates near the axis;
	-- found that there are 2 leading jets blobs, or leading jet fronts that
		have large gammas at two equally-spaced (w.r.t boundary) phi-coordinates;
		-- it seems that the blobs are identical in shape;
		-- they are both in the lower hemisphere
		-- they reach gammas > 80 ;
???		-- I am not sure how it can exceed the GAMMAMAX ceiling???
???		-- it seems that fixup() is flagging and renormalizng v^i at the points
			that reach GAMMAMAX, but that the fixup interpolation calculates
			v^i such that GAMMAMAX is still reached????
		-- a way of fixing this type of problem is to interpolate for gamma, and
			rescale interpolated v^i to the interpolated value of gamma;
			(ala Zhang and MacFadyen)
???		-- another way would be to do what McKinney's new way????
		-- the way Julian wants it is just to impose GAMMAMAX limit after the
			interpolating fix;
			-- will try this out now;

		-- tried restarted with a UUMAX floor 100x that of the previous value;
			-- run eventually crashed seemingly from gamma diverging at
				mid-latitude in plunging region;
			-- I guess this points to a better gamma fix too....

		-- tried using HLLE instead of LF flux;
			-- the run crashed even earlier;  the pressure
				did not seem qualitatively different, though;
			--

		-- trying a new way of fixing gamma:
			-- if GAMMAMAX is reached, then flags for interpolation;
			-- if interpolated velocities give gamma>GAMMAMAX (whether
				the failure being interpolated was originally from
				a GAMMAMAX failure or not) then we interpolate the
				local values of gamma for the new point and rescale
				the interpolated velocities to this new value;
			--

		-- another way of fixing failures is to interpolate with a
			weighting scheme that is upwind somehow, i.e. either
			w.r.t. local fluid velocity or characteristics waves;

-------------------------------------------------------------------------------------------

RUNS :


harm3d-kd9/               : first run of a=0.9 KD initial data
harm3d-kd9-hslope=0.35/   : harm3d-kd9 execpt h_slope=0.35 now (cutout=0.038)
                           -- crashed after t=640M ;
	                   -- did not see a pressure divergence like before as of t=640M;
                           -- there is no obvious feature that may indicate where/when
	                      the run dies;
harm3d-kd9-hslope=0.35_r1/   : restart of harm3d-kd9-hslope=0.35 starting from dump before
				failures;
				-- failure seems to occur in the same way as before, there
					are pressure divergences that runs away along the
					cutout, however these divergences happen on the
					opposite cutout boundary
				-- the "chevron" patterns are still apparenent with this grid;


harm3d-kd9-hslope=0.35_r1-new/
		-- restart of harm3d-kd9-hslope=0.35 from t=600M, cour=0.4, hslope=0.35
		-- using new and correct boundary conditions along the cutout boundary;
		-- lasted to t=665M, no obvious problems from uu

harm3d-kd9-hslope=0.35_r1-new-r1/
                -- restart of harm3d-kd9-hslope=0.35_r1-new from t=665M
		-- increasing n_restart to 3000 in hopes of speeding things up;
		-- lasted to t=680-690
		-- a region of high gamma and pressure develops at r~4-6M that then
			blows up;
		-- gamma increases first, then pressure.
		-- it seems that the high-gamma blob moves into a region whose pressure
			is floored often though nonuniformly (i.e. its patchy)
		-- an insignificant number of inversion failures follow in the bad region's
			wake
		-- there does seem to develop many inversion failures right on the cutout
			boundary opposite where things fail, but the fixups always worked
			(i.e. there are no fixup failures before the catastrophe)
		--
		--

harm3d-kd9-hslope=0.35_r1-new-r1-2/
ANALYZE	-- restart of harm3d-kd9-hslope=0.35_r1-new-r1/
		from t=670-680 with DT_out3=0.01
		-- only lasted to t=676.5  given the walltime limit
		-- generated a huge amount of data... about 700 dump files
		-- am not copying to gauss since there are too many files
		-- we do not see any problems at t=676.5 though so I don't
			think we've gotten to the interesting part


harm3d-kd9-hslope=0.35_r1-new-r1-3:
ANALYZE	-- continuation of harm3d-kd9-hslope=0.35_r1-new-r1-2  using DT_out3=0.1 now
		to reduce the amount of disk space used;
	-- finished




harm3d-kd9-r1/            : restart of harm3d-kd9 at step before failure
harm3d-kd9-test-r1/       : harm3d-kd9-r1 as but with a smaller wallclock limit, smaller DT_out3=2
harm3d-kd9-test-r1-2/     : restart of harm3d-kd9-test-r1 before failure  w/ DT_out3=0.01
harm3d-kd9-test-r1-3/     : continuation of  harm3d-kd9-test-r1-3 (r1-2 had not failed yet)  w/ DT_out3=0.1
harm3d-kd9-test2-r1/      : harm3d-kd9-test-r1 but with even smaller wallclock limit
harm3d-kd9-test4-r1/      : harm3d-kd9-test2-r1 but with HLLE instead of LF
harm3d-kd9-test5-r1/      : harm3d-kd9-test2-r1 but with UUMIN=2e-6 instead of 2e-8
harm3d-kd9-test6-r1/      : harm3d-kd9-test2-r1 but with new gamma fixup routine
harm3d-kd9-test6-r1-2/    : continuation of harm3d-kd9-test6-r1
harm3d-kd9-test6-r1-3/    : starting at step before failure with DT_out3 = 0.1
				-- seems as though the large number of failures occur when v1 becomes
					negative in a very localized region between two
					regions of out-wardly moving distributions of
					high-pressure gas;
				-- in this region, rho/u is not strange, both are above the floor;
				-- bsq is perfectly smooth and there's no variation seen anywhere
					in this region ---->  purely hydro problem!!
				-- rerunning "...r1-4"  with  higher frequency dumps
harm3d-kd9-test6-r1-4/    : starting at t=643.00 of "...test6-r1-3" with DT_out3 = 1e-4
harm3d-kd9-test7-r1       : like harm3d-kd9-test6-r1 but with cour=0.4
				-- no difference seen at t=620, restarting
harm3d-kd9-test7-r1-2     : starting from where harm3d-kd9-test7-r1 left off

harm3d-kd9-test8-r1       : like harm3d-kd9-test7-r1 but with UUMIN=2e-7
				-- no big diff. seen by t=620
harm3d-kd9-test8-r1-2     : starting from where harm3d-kd9-test8-r1 left off

harm3d-kd9-test9-r1       : like harm3d-kd9-test6-r1 but with MC limiter
				-- more diffuse, no big diff by t=622
harm3d-kd9-test9-r1-2     : starting from where harm3d-kd9-test9-r1 left off
harm3d-kd9-test9-r1-3     : starting from where harm3d-kd9-test9-r1-2 left off
harm3d-kd9-nocutout       : redo of run without cutout and hslope=0.35 to see if
	                    improvements have really speeded things up and if
                            problems persist without cutout;

/harm3d-kd9-test10-r1     : starting at t=600 using new corrected set_cutout_boundary2()
		-- lasted till t~>620 then catastrophic failure
		-- many instances of vchar discr failures
		-- possibly did not work because it started with a run that did not
			use this boundary condition;


/harm3d-kd9-test11-r1     : starting at t=600 using new corrected set_cutout_boundary3()
		-- ran past t=640, no catastrophic failures so far
		-- no stderr
		-- there were no inversion failures along cutouts, mostly in the
			plunging and corona/wind boundary;
		-- reaching gamma~=50  in the corona outflow
		-- no fixup failures
		-- no gamma_calc or gamma_max failures
		-- floor only activated in the corona-outflow wedge near the horizon
			above the plunging region;


/harm3d-kd9-test12-r1     : starting at t=600 using new corrected set_cutout_boundary4()
		--
		--


/harm3d-kd9-fix-r2-2 :  starting from rdump_0.sdf of harm3d-kd9-test11-r1 run from t=643
  		      -- going for full 24 hour run to see if successful
		      -- catastrophic failure by t=660M




/harm3d-kd9-cutout2:
		-- rerun of kd9 initial data starting at t=0  using set_cutout_boundary2()
		-- possibly will work if we start from the beginning ;
		-- fatal crash encountered  somewhere between t=600-620M, cannot tell
			where the problems started;



harm3d-kd9-cutout2-r1:
		-- restart of /harm3d-kd9-cutout2 from t=600 using DT_out3=1
		-- using cour=0.4
		-- run failed between t=620-640

/harm3d-kd9-cutout2-r2
ANALYZE		-- running from t=620 of harm3d-kd9-cutout2-r1 using smaller DT_out3
		-- failure occurred within the first 1M, still cannot tell where it
			occurs;
		-- restarting with DT_out3=0.01M
		-- failure


harm3d-cutout1     :  -- 2d run using set_cutout_boundary()
			-- fails
harm3d-cutout2     :  -- 2d run using set_cutout_boundary2()
			-- suprising devoid of noise or explosions of any kind.
			-- ran the full 2000M
			-- no extra heating along the boundary

harm3d-cutout3     :  -- 2d run using set_cutout_boundary3()
			-- good up to t~=1200M  at which point there is noise in the
				funnel (large p noise); then at t~=1300 and t=1500
				a few explosions occur:
				one near the horizon/cutout corner and one in the
				corona/wind boundary;

harm3d-cutout4     :  -- 2d run using incorrect version of set_cutout_boundary4()
			-- bad, ignore

harm3d-cutout4-2   :  -- 2d run using correct version of set_cutout_boundary4()


/harm3d-kd9-fix-r2-2-test1:
		-- restarting from initial restart file of /harm3d-kd9-fix-r2-2  but
                      outputtting every 2m to see what happened
		-- initial glance looks like there's a thin sliver of large gamma
		   and pressure that develops just above plunging region
		-- failure between 652 and 654
		-- most of the inversion failures are above the plunging region,
			(i.e. in the funnel/wind), but do not seem to be
			related to the pressure maxima;
		-- there are no fixup failures before the crash;
		-- pressure maxima are ahead of (in r) regions of large gamma;
			-- this suggests that the hot blobs are the result of
				shock-heating;
		-- v1,v2,v3 all follow in intensity gamma;  this suggests
			that gamma is being rescaled;
		-- big differences seen between the upper and lower hemispheres;
			-- it seems as though v^i and B^i are much more turbulent
				in the upper (hotter, higher gamma) hemisphere
				than the lower hemisphere;
			-- B^3 starts off being smooth at t=644 but becomes more
				spatially variable;
			-- B^1 and B^2 are already variable by t=644 in meridonal slices,
				less so in phi;
			-- both B1 and B2 are smooth in lower hemisphere;
			--



/harm3d-kd9-fix-r2-2-test1-2:
		-- restarting at 652 with smaller DT_out3


harm3d-kd9-fix-r2-2-test2
		-- using smaller cour=0.4 and smaller GAMMAMAX=25, starting at t=643
		-- past t=666;
		-- seemingly KH waves above plunging region are still there, but
			things appear stable;
		-- did 23M in 4 hours, ~ 6M/hr

harm3d-kd9-fix-r2-2-test3
		-- using smaller cour=0.4 and smaller GAMMAMAX=25, starting at t=620
		-- used DT_out3 = 5
		-- run only lasted to t=651 before wall clock time expired;
		-- in 5 hours, only went 31M,  6M/hr, -> 10000M run will last 70 days;



harm3d-kd9-fix-cour=0.4:
		-- starting from t=0
		-- using cour=0.4
		-- using n_restart = 3000  (restart dumps may be slowing it down)
		-- using DT_out3 = 20
		-- lasted past wallclock limit
		-- lasted till t=478M
		-- nsteps = 94809
		-- average dt = 0.005
		-- min dt (at late times) = 0.0039
		-- wallclock time = 23.5 hours
		-- runtime rate = 20 M/hour
		-- zone-cycles/sec/cpu = 36722

harm3d-kd9-fix-cour=0.4-r1:
		-- restart of harm3d-kd9-fix-cour=0.4 from t=480M
		-- critical failure at t=660-680M


harm3d-kd9-fix-cour=0.4-r1-2:
		-- restarting harm3d-kd9-fix-cour=0.4-r1 at t=666M w/ DT_out3=1M
		-- lasted only to t=661M for some reason even though it seemingly
			should have run for 3 hours; ??


harm3d-kd9-fix-cour=0.4-r1-3:
ANALYZE		-- restarting harm3d-kd9-fix-cour=0.4-r1-2
		-- done, resolved failure
		-- there is a large increase in gamma and pressure along
			inner radial/cutout edge that develops into an
			outgoing hot flow that eventually makes the code crash;


harm3d-kd0:
		-- just like harm3d-kd9-fix-cour=0.4  but with   a = 0
		-- ran to t=680M in 24 hours

harm3d-kd0-r1 :
		-- restart of run harm3d-kd0
		-- failure, don't know where exactly, between t=820-840
		-- possibly from a peak in gamma that develops within horizon
			along cutout boundary;


harm3d-kd0-r1-2:
ANALYZE		-- starting from t=820 of harm3d-kd0-r1  using DT_out3=1
		-- lasted till ?




harm3d-kd9-fix2-r2 :
		-- starting from t=600 with new set_boundary_cutout3()  routine
		-- using cour = 0.4
		-- lasted to t=650M

harm3d-kd9-fix2-r3 :
		-- restart of harm3d-kd9-fix2-r2
		-- critical failure seen at t=680M


harm3d-kd9-fix2-r3-noipo:
		-- just like harm3d-kd9-fix2-r3  but not using -ipo  compiler flag;
		-- McKinney once reported that -ipo lead to problems of some sort,
			so maybe this is a problem;
		-- doesn't seem to change much;
		-- run stopped before the fatal error, but can still see the
			development of the blob of high pressure near the
			horizon and cutout boundary;


harm3d-kd9-fix2-r3-redo :
		-- rerun of harm3d-kd9-fix2-r3  using small DT_out3=2 to t=700M
		-- fatal error between t=680-682
		-- seems like there were fixup failures at t=680 that might have
			lead to crash or made it worse.
		-- seems like high-pressure blob originated from high-pressure,
			high-gamma feature on horizon that was made at t=664;
			a few density activations are seen then there but nothing
			striking;
			-- no correlations seen in B1-3 with this feature;
			-- v1 seems to follow gamma most closely, so this appears
				to be a growth of outward radial velocity
		-- before the high-pressure blob is born, all but the thin
			plunging matter is magnetically-dominated;
			-- the high-pressure blob is born with a pressure
				greater than bsq
			-- maybe the problem is a truncation error/boundary
				effect
		-- also the blob is born with high-gamma, right on the
			inner radial boundary near the cutout;
			-- maybe this a boundary condition effect;
		-- gammamax failures occur before pressure max.;
		-- gamma increases suddenly from ~3 at the RIN boundary
			to ~20   just as B2 suddenly increases in magnitude
			too;
		-- at this time B2 is a oscillatory wave travelling in phi;
		-- the gammamax failures start to occur at around t=674,
			though small localized areas of "large" gamma occur
			before any structure in B^i is seen;
			-- these variations in gamma are mirrored in v1 and v3
				but mostly in v1;
			-- the variations in v1 are seen at t=660 and grow from
				there;
			-- this seems to be the first hint of the catastrophe;
			-- this feature in v1/gamma also matches where there are
				a lot of inversion failures ;
				-- maybe fixup method is not interpolating  the
					velocities here very well and encouraging
					divergence of velocities;


harm3d-kd9-fix2-r3-cutout4 :
		-- rerun of harm3d-kd9-fix2-r3  but using set_cutout_boundary4()
			instead of set_cutout_boundary3() in bounds.c  so as
			to maybe control the pressure divergence there;
		-- original run used too large a DT_out3 and did not resolve when
			the failure occurred;
		-- seemed to die early on
		-- redoing run with smaller DT_out3;
		-- this run was using the old and incorrect cutout4 boundary condition
			(forgot to divide by gdet before comparing the energies);
		-- redoing run;
		-- seemed to be fine thorugh t=663 when run was terminated due to
			walltime limit;


harm3d-kd9-fix2-r3-cutout4-r1 :
ANALYZE		-- restart of harm3d-kd9-fix2-r3-cutout4  from t=663M
		-- stopped
		-- cursorally, it seems as though there's a fatal pressure
			explosion on the inner radial boundary and cutout
			boundary at about the same time;
		--


harm3d-kd9-limit-gamma:
	-- starting from t=650M of harm3d-kd9-fix2-r3  using USE_GAMMA_CEILING_X1DN_BC=1
	-- ran to t=663M fine;
	-- restarting

harm3d-kd9-limit-gamma-r1:
	-- ran till t=690M

harm3d-kd9-limit-gamma-r2
	-- restarting from where harm3d-kd9-limit-gamma-r1 left off;
	-- fatal error sometime between t=710-730M


harm3d-kd9-limit-gamma-r2-2:
	-- restart from t=710 (dump0039) of harm3d-kd9-limit-gamma-r2
		using smaller DT_out3=1

harm3d-kd9-limit-gamma-r2-r1:
ANALYZE	-- restarting from t=690M (dump0038) of harm3d-kd9-limit-gamma-r2
		to see where this phi-independent pressure extremum comes
		from that seems to appear between t=690-710 that falls
		toward the hole and leads to the eventual fatal explosion;


limit-gamma-r2-2-inv
ANALYZE	-- restarting harm3d-kd9-limit-gamma-r2-2 from t=716M right before
		there is a large spike in pressure along the cutout.
	-- dumps are frequent (DT_out3=0.01) so that we an examine the inversion process;
	-- this suggests that maybe the prim. inversion routine is
		giving a bad solution since it is only interpolating
		0.1-1% of the time. and we cannot see why the fixup interpolation
		would drive the pressure so strongly;
	-- fixup routine never failed;
	-- no gamma_calc() failures;
	-- the front of the growth of the region of large gamma and pressure
		seems to coincide with where the floor is activated mostly;



limit-gamma-r2-2-inv-etot
ANALYZE	-- same run as limit-gamma-r2-2-inv but with DT_out3 = 1  and new M_tot,
	    E_tot, L_tot monitors work;   we want to see how fast these quantities
	    diverge.
	-- looking at the evolution of Mtot, Etot and Ltot, Mtot and Etot
		both start increasing exponentially (Etot faster) just before
		t=717, which is where the large gamma high pressure region
 		must be forming;
	-- Ltot(t) remains linear in time throughout this period
	-- this suggests that if we can identity when Etot begins to go
		nonlinear, then we can tell the code to stop



harm3d-kd9-limit-gamma-r2-2-ratio:
	-- restart from t=710 (dump0039) of harm3d-kd9-limit-gamma-r2
		using DT_out3 = 5
	-- setting  USE_MAX_UU_RHO_RATIO = 1
	-- setting  MAX_UU_RHO_RATIO = 3.
	-- i.e. restricting   u/rho < 3
	-- the idea is that u/rho is only large when  bsq/u >> 1, which is
		where bsq's truncation error makes  u  error ridden and u can
		develop large gradients.  In this regime these large gradients
		are dynamically significant when u/rho is large.  If we restrict
		u/rho then we can limit the effect of the pressure gradients.
	--


harm3d-kd9-limit-gamma2:
	-- limiting gamma in the X1DN ghost zones and the X1DN cells
		that lie within the horizon;
	-- starting from t=650M of harm3d-kd9-fix2-r3  using USE_GAMMA_CEILING_X1DN_BC=1

harm3d-kd9-limit-gamma2-r1:
ANALYZE	-- restarting harm3d-kd9-limit-gamma2 from end
		with larger DT_out3 since it took 4hours to go 30M;
	-- ran fine till t=708M
	-- the gamma cap works in the horizon, but large gammas (~6)
		still develop outside the horizon;
	-- most of the points near the inner radial boundary and the cutouts
		have gamma near the cap;
	-- will have to make sure that max(gamma) outside horizon is affected
		by the value of the gamma cap inside the horizon;
		-- if so then this is arbitrary;
	-- WHAT MAKES THERE BE LARGE VELOCITIES HERE!!!!!!!


harm3d-kd9-limit-gamma2-r2:
ANALYZE -- continuuing from end of  harm3d-kd9-limit-gamma2-r1
	-- fatal problems somewhere in t=718-728M
	--

harm3d-kd9-limit-gamma2-r2-2
ANALYZE	-- rerunning harm3d-kd9-limit-gamma2-r2 from t=718M with DT_out3=1




harm3d-kd9-newlimgam2:
	-- like harm3d-kd9-limit-gamma2   but with new line in advance()
		that always set p_f = p_i   so that the guess that Utoprim()
		uses is always 2nd-order and from the previous step;
	-- starting from t=650M of harm3d-kd9-fix2-r3  using USE_GAMMA_CEILING_X1DN_BC=1


harm3d-kd9-newlimgam2-r1
	-- restarting harm3d-kd9-newlimgam2  from end
		with larger DT_out3 since it took 4hours to go 30M;
	-- ran fine to t=708M
	-- the gamma cap works in the horizon, but large gammas (~7)
		still develop outside the horizon;
	-- most of the points near the inner radial boundary and the cutouts
		have gamma near the cap;
	-- will have to make sure that max(gamma) outside horizon is affected
		by the value of the gamma cap inside the horizon;
		-- if so then this is arbitrary;
	-- WHAT MAKES THERE BE LARGE VELOCITIES HERE!!!!!!!
	-- comparing to harm3d-kd9-limit-gamma2-r1 to see if p_f=p_i  made
		a difference:
		-- limit-gamma2 seems to have smaller gamma's and now pressure
			extrema along the cutout;
		-- maybe not seting p_f=p_i is better?



harm3d-kd9-newlimgam2-r2
ANALYZE -- continuuing harm3d-kd9-newlimgam2-r1 run
	-- fatal problems somewhere between t=708-718
	--

harm3d-kd9-newlimgam2-r2-2:
ANALYZE	-- restart of harm3d-kd9-newlimgam2-r2 from t=708 with DT_out3=1



woodhen:harm3d-kd9-mc:
ANALYZE	-- starting from t=0 using MC limiter;
	-- developed noise like the others along the cutout boundary;
	-- this run did not use ANY gamma limits (beside gamma<50) and did NOT use
		the pressure/gamma fixups along the cutout
	-- DID use the MC limiter
	-- even though one does eventual see the ripples, the ripples are very transient and
		are not present from t=0;  they develop just as the inflow channel is created
		and then the ripples show up and then they vanish like they are ejected
		through the wind or something
	-- the solution is much smoother, but the "whipping" along the cutout occurs;
	-- noise in gamma and u develops along the cutout, but the fixup interpolation
		was used when rho,u < 0  instead of the floor


/data1/euler/scn/harm3d/harm3d/ripples:
	-- contains tests to eliminate the ripples seen above the equatorial
		plunging region;


/data1/euler/scn/harm3d/harm3d/ripples/orig/:
	-- standard LF,PPM_MC, 192^2, rhomin=2e-6,uumin=2e-8, w/ B-field,
	-- old version of inflow_check;
	-- set_cutout_boundary3();
	-- catastrophic failures at t~=1250M
	-- large gammas (>50) are seen along cutout boundary; explosion comes
		from cutout/horizon boundary;

/data1/euler/scn/harm3d/harm3d/ripples/bigfloor:
	-- like orig but uumin=2e-6
	-- orig's gamma grows quickly at the equator which is
		matter accreting fast from the atmosphere there;
	-- bigfloor does not have high-gamma equatorial flow
		immediately after t=0;  it's largest gamma flow
		is located near the poles at t~0 but the gamma's
		there are not large.
	-- run eventually leads to catastrophic failures as the first
		batch of plunging matter hits the horizon, and matter
		is ejected at high velocity along the axis;
		-- i'm not sure why the large velocity matter is not seen
			at this time in the orig run;  why would the larger
			floor accomodate this behavior?

/data1/euler/scn/harm3d/harm3d/ripples/constfloor/:
	-- like orig but floor = constant floor w/  rhomin=uumin=2e-6
	-- very little matter starts accreting right after t=0;
	-- fatal explosion occurs as matter hits the horizon, like bigfloor


/data1/euler/scn/harm3d/harm3d/ripples/mc/:
	-- like orig but limiter=MC
	--

/data1/euler/scn/harm3d/harm3d/ripples/ripples-monitor-inflow/:
	-- like orig but we're outputting before and after values of v^i
		in inflow_check();
	-- the before values are very close to the after values;


/data1/euler/scn/harm3d/harm3d/ripples/no-b/:
	-- like orig but with B^i = 0 ;
	-- disk is stable and there is very little accretion, which is
		triggered by numerical diffusivity and initial deviations
		from the equilibrium solution due to the random noise
		added to uu;

/data1/euler/scn/harm3d/harm3d/ripples/ripples-no-hor-inflow/:
	-- like orig but inflow_check() is never used;



harm3d-fb:
harm3d-fb-2:
ANALYZE	-- fishbone moncrief initial data using h_slope=0.13 and using
		a cutout just like in the KD runs;

	-- ran till t=370m then crashed due to a pressure explosion
	-- pressure explosion seems to stem from a region of high gamma
		just below the equator (not as far as KD crashes) near
		but not right on the horizon;
	-- this region also seems devoid of significant B-field.  The distribution
		of bsq is asymmetric about the equator with one hemisphere
		having much larger bsq than the other.
	-- there do not seem to be any type of failures in this region
		before gamma/uu grow too large;



harm3d-kd9-july:
	-- using code version as of july 1st, 2007
	-- previous version was in harm3d-kd9-limit-gamma-r2-2-ratio:
	-- fixed dphi=Pi/8 symmetry problem (runs were symmetric with this
		periodicity in phi) by changing how ranc() is seeded (now seeded
		with  myid+1  so that each cpu has different random number sequence;
	-- ran past t=560M

harm3d-kd9-july-r1:
	-- restart of harm3d-kd9-july
	-- has already passed t=680M fine;
	-- crash somewhere between t=680-720M


harm3d-kd9-july-r1-2
ANALYZE -- restart of harm3d-kd9-july-r1 from t=680M with DT_out3=1
	-- seems to crash due to an explosion along cutout triggered
		by matter falling back onto hole


harm3d-kd9-july-lowres:
	-- just like harm3d-kd9-july but with 128x128x32 points;
	-- crashed sometime between t=640-680


harm3d-kd9-july-lowres-2
	-- restart of harm3d-kd9-july-lowres from t=640M
	-- lasted to t=658
	-- catastrophic explosion of pressure and gamma starts
		on the horizon near the equator
	-- explosion arises within 1M between t=656-657M
	-- explosion is coincident with a sudden increase in U0 and U1,
		though we don't know if the prim's or the cons's increased
		first
	-- looking at M_tot and E_tot over time and it seems as  though
		neither increase significantly between t=656-657 even though
		their is a large local increase in both.
		-- however there was a bug in how E_tot (M_tot was fine) was calculated
		-- M_tot did not increase right away since rho does not diverge there
			right away
		-- U0 is diverging because gamma is diverging
		-- U1 should diverge is more strongly since both uu and gamma are diverging
			so E_tot should change noticeably at this time with the correction;
	--


harm3d-kd9-july-lowres-3
ANALYZE	-- rerun of harm3d-kd9-july-lowres-3  with dumps after
		every timestep to see if the prim's or the cons's increase first
	-- it seems as though U1 is large
	-- at the peak of U1,  the flow is cold ( u/rho << 1 ) and   rho ~ U1
	-- the peak is seen in rho since U1 and rho are very similar
	-- rho is very smooth while uu is very noisy;
	-- bsq ~ 1e-5-1e-3,  u ~ 1e-7-1e-3,  rho ~ 1e-4-1
	-- bsq << rho always,  bsq/u fluctuates as u is noisy but bsq is not;
	-- so this region is cold, moving fast (hits the gamma=5 cap within horizon),
		and matter dominated (not magnetic field dominated);
	-- i can't find what is causing the increase in rho/U0 in that region;
		-- v^i appear smooth there
		-- will have to look at u^i  to make sure
		-- should plot  gdet*rho*u^\mu  next to each other




harm3d-kd9-july-hybrid:
        -- newest version of harm3d (like harm3d-kd9-july) but using
        	the grid structure of harm3d-kd9-hslope=0.35_r1-new but with
		CUTOUT = 0.045 instead of 0.038.
	-- so h_slope = 0.35, n_within_horizon=5, Rout=120, a=0.9
	-- limiting gamma to 5 along horizon boundary and within horizon
	-- limiting  p/rho < 3
	-- using new ranc() seeding to eliminate pi/8 periodicity bug;
	-- for a 24hour run, ran till t=550M
	-- the last step was about when matter was about to begin to accrete
		onto the hole


harm3d-kd9-july-hybrid-r1
ANALYZE	-- continuing  harm3d-kd9-july-hybrid  from t=550
	-- crashed sometime between t=680-720

harm3d-kd9-july-hybrid-r1-2
ANALYZE	-- restart of harm3d-kd9-july-hybrid-r1 from t=680 with DT_out3=1
	-- finished
	-- explosion does start along the cutout boundary

harm3d-kd9-july-hybrid-r1-3
ANALYZE	-- using harm3d-kd9-july-hybrid-r1-2 going from t=693 w/ DT_out3=0.01
	-- it seems as though the gamma limiter in the horizon is reinforcing
		the development of density/pressure/U0 along the inner
		radial boundary since it is constraining u^r to be smaller
		than what can develop outside the horizon;  this is creating
		u^r(outside) << u^r(inside) < 0   so that it is pushing
		matter up against the boundary faster than it can advect
		it through the boundary;
	-- the density can increase indefinitely as long as this feature in
		u^i remains;  it does so long enough in the simulation that
		the density/pressure increase to the point that it finally
		affects zones outside the horizon and an explosion ensues;



harm3d-kd9-july-hybrid-nolim-r1
ANALYZE -- continuing  harm3d-kd9-july-hybrid  from t=550
		but without limiting gamma to 5 within the horizon ;
	-- crashed sometime between t=720-760M

harm3d-kd9-july-hybrid-nolim-r1-r2
ANALYZE -- restarting harm3d-kd9-july-hybrid-nolim-r1 from t=720 w/ DT_out3=1
	-- problem occurs along cutout boundary away from inner radial boundary
	--

harm3d-kd9-july-hybrid-nolim-r1-r3
ANALYZE	-- restarting harm3d-kd9-july-hybrid-nolim-r1-r2 from t=720-724 w/ DT_out3=0.1
	--



harm3d-kd9-july-hybrid-nolim-r1-2
ANALYZE	-- restart from t=680M of harm3d-kd9-july-hybrid-r1
		but without limiting gamma to 5 within the horizon ;
	-- linear regions of significant over-pressure were seen from the
		very start; both lines extended straight along constant phi, starting
		from the middle, suggesting that this is an MPI problem;
	-- the rdump_start.h5 file did contain defects in rho and v2
	-- rerunning without using unitree
	-- ran till t=725 with no catastrophic explosion yet, though we do see
		uu growing very large and will probably result in an explosion
	-- this growth occurs at the tip of a sheared outflow along the cutout
		boundary;  hot, fast matterial slams into cold slow material
		along the cutout along diagonal r,phi surfaces;
	-- the cells right along this problematic cutout surface are most always
		at the max. allowed by the u/rho ratio threshold; i.e. they are the
		hottest they are allowed to be.
	-- large (negative) U1 feature along the cutout boundary (that eventually
		diverges because of rapidly accumulating pressure/velocity
		from the floor and interpolation) seems to grow dramatically
		between 690 and 700


harm3d-kd9-july-hybrid-nolim-r1-2-2
ANALYZE  -- starting from t=680  of harm3d-kd9-july-hybrid-nolim-r1-2
		which is right before the large divergence in U1 is seen
	-- using DT_out3=0.2  to gauge where this really starts;
	-- it seems that the noise in U1 starts before t=680
	-- this run showed how


harm3d-kd9-july-hybrid-nolim-r1-2-3
ANALYZE	-- code of harm3d-kd9-july-hybrid-nolim-r1-2-2 but starting from t=640
		of harm3d-kd9-july-hybrid-r1
	-- at t=640, U1 is smooth, so we want to see how this noise develops
	-- it seems that the noise originates from a region on the lower
		cutout boundary at the inner radial boundary after
		it becomes necessary to floor uu over a large region there;
		-- the first sign comes at t=642 when we see that uu needs
			to be floored about half the time;
			-- all quantities are smooth at this time
		-- by t=644, we see that uu has been floored all the time
			in a large region which leads to a constantly cold
			region near the inner radial boundary;
??			-- there are no errors but "floor instances" errors there;
??			-- even though the cells one cell away in theta
				half nearly as many floor occurrences, the
				pressure there is not at the floor;
??			-- this seems contradictory: why can't we see uu at the floor
				in the cells just off the cutout boundary ??
			-- at this time, we do see gamma become non-smooth with a
				maxima in the middle of the floored region;


harm3d-kd9-july-hybrid-nolim-r1-2-4
ANALYZE -- running harm3d-kd9-july-hybrid-nolim-r1-2-3 from t=640 to 646
		with DT_out3=0.1  to capture what is really going on;
	-- found that a line of cells immediately adjacent to the inner
		radial boundary and the cutout boundary suddenly drop to
		the floor after declining in magnitude somewhat beforehand.
	-- this floored region grows until a cell at the large-phi-end of
		the region fails to invert and then more inversion follow
		and cascade through the region;
		-- it seems as though the fixup interpolation creates a
			new state that influences its neighbors to fail;
		-- the failures go through the whole floored region and
			leave behind a very inhomogeneous in pressure
			and density area right along the cutout boundary;
		-- after its interpolating for a while, then the cells
			1 cell away from the cutout start heating up more;
		-- we do bsq/uu become larger in this region before the
			floor is activated;  this threshold value is
			about 10^1.7
		-- gamma also seems to increase marginally before the
			floor activation;
	-- note that the region in which the pressure hits the floor
		suddenly is where the density is floored too;  the density
		is always floored there for a couple M before the pressure
		hitting the floor;


harm3d-kd9-july-hybrid-nolim-r1-2-5
ANALYZE	-- starting from t=642.7 (dump=43) of harm3d-kd9-july-hybrid-nolim-r1-2-4
		using DT_out3=1e-3
	-- only integrating for 0.1
	-- this is when uu hits the floor on the cutout boundary along the inner
		radial boundary

harm3d-kd9-july-hybrid-nolim-r1-2-6
ANALYZE	-- starting from t=644.5 (dump=61) of harm3d-kd9-july-hybrid-nolim-r1-2-4
		using DT_out3=1e-3
	-- only integrating for 0.1
	-- this is when (it seems) the fixup interpolation wipes out the uu-floored
		region and creates all the inhomogeneities in rho, uu;


harm3d-kd9-july-hybrid-nolim-smflr:
	-- using code of  harm3d-kd9-july-hybrid-nolim-r1-2-4
		and starting from t=640 of harm3d-kd9-july-hybrid-nolim-r1-2-4
		with  RHOMIN=2e-8 and UUMIN=2e-8
	-- encountered the same problem as harm3d-kd9-july-hybrid-nolim-r1-2-5
	-- uu still hit floor along cutout and X1DN and then the fixup interpolation
		made it noisy;

harm3d-kd9-july-hybrid-nolim-useflr:
ANALYZE	-- using code of  harm3d-kd9-july-hybrid-nolim-r1-2-4
		and starting from t=640 of harm3d-kd9-july-hybrid-nolim-r1-2-4
		with treat_floor_as_failure=0 (using old values for the floor,
		i.e. UUMIN=2e-8 and RHOMIN=2e-6);
	-- seems to be working in that the floored region in uu grows ;
		there is some noise  in uu in the floored region, but hopefully
		it won't be  a problem.



harm3d-kd9-fixp:
	-- run from t=640 of harm3d-kd9-july-hybrid-nolim-r1-2-4
	-- USE_GAMMA_CEILING_X1DN_BC=0 (no gamma limiting whatsoever beside GAMMAMAX)
	-- we use the floor if there is a solution for P but  u<0
	-- if u[*,j=0,*] becomes much different than u[*,j=1,*]  (same with other
		cutout boundary) and if bsq>>u then we copy the j=1 value of u
		to the j=0 cell;
	-- reached t=680 with no apparent problems;

harm3d-kd9-fixp-r1
ANALYZE	-- continuation of harm3d-kd9-fixp from t=680
	-- USE_GAMMA_CEILING_X1DN_BC=0 (no gamma limiting whatsoever beside GAMMAMAX)
	-- lasted to t~=685. , ended because E_tot was diverging
	-- E_tot was diverging because a region of large gamma was developing
		along the cutout boundary, coming from the inner radial boundary;

harm3d-kd9-fixp-r1-2
	-- rerun of harm3d-kd9-fixp-r1 using DT_out3=0.2
	-- USE_GAMMA_CEILING_X1DN_BC=0 (no gamma limiting whatsoever beside GAMMAMAX)


harm3d-kd9-fixp-r1-3
	-- rerun of harm3d-kd9-fixp-r1, not dumping, just w/ CHECK_INVERSION;
	-- USE_GAMMA_CEILING_X1DN_BC=0 (no gamma limiting whatsoever beside GAMMAMAX)

harm3d-kd9-fixp-r1-4
	-- rerun of harm3d-kd9-fixp-r1, using DT_out3=2, going to 720,
		w/ CHECK_INVERSION, w/ USE_GAMMA_CEILING_X1DN_BC=1
	-- this change allowed it to go to t=698 with no superbad explosions;
	-- U1 is a little noisy, as is gamma, but u is much smoother

harm3d-kd9-fixp-r1-4-r1
	-- continuation of harm3d-kd9-fixp-r1-4
	-- finally lead to U1 divergence, lots of garbage near the cutout
		at intermediate r ;


harm3d-kd9-fixp-r1-5
	-- rerun of harm3d-kd9-fixp-r1, using DT_out3=2, going to 720,
		w/ CHECK_INVERSION, w/ USE_GAMMA_CEILING_X1DN_BC=1,
		w/ limit_gamma_in_horizon()
	-- U1 is a little noisy, as is gamma, but u is much smoother
	-- has slightly larger values of gamma than harm3d-kd9-fixp-r1-4 and U1 is
		noiser than harm3d-kd9-fixp-r1-4


harm3d-kd9-fixpg:
	-- rerun of harm3d-kd9-fixp-r1-4 from t=680 but  using
		FIX_CUTOUT_PRESSURE=1   AND   FIX_CUTOUT_GAMMA=1
	-- also dumping CHECK_PRIM_CHANGE information;




harm3d-kd9-fixp-r1-4-redo:
	-- rerun harm3d-kd9-fixp-r1-4  to test out the new changes where i moved
           fixup1zone() call into recover_primitives();
	-- looks fine;


harm3d-kd9-fixp-r1-4-redo:


harm3d-kd9-fixp-r0
	-- harm3d-kd9-fixp run from t=640-680 w/ DT_out3=1 to see how noisy gamma develops


harm3d-kd9-fixp-cour=0.1
	-- run from t=660 data from harm3d-kd9-fixp using cour=0.1
	-- lasted to t=672

harm3d-kd9-fixp-cour=0.1-r1
	-- continuation of harm3d-kd9-fixp-cour=0.1-r1 from t=672
	-- developing large gamma region along the inner radial boundary near the cutout

harm3d-kd9-fixp-cour=0.1-r2
	-- continuation of harm3d-kd9-fixp-cour=0.1-r1 from t=672
	-- lead to an explosion of large gamma and pressure like always;


harm3d-kd9-mc-long
RUNNING	-- restart of run harm3d-kd9-mc that was ran on woodhen that lasted to t=731 before
		showing lots of high gamma noise along the cutout boundary;
	-- rerunning with gamma limiter in the X1DN ghost cells plus I will use the
		floor instead of interpolating with rho,u < 0
	-- will not use any other gamma limiters (besides gamma<50) and will NOT use
		any gamma/pressure cutout fixups;




harm3d-kd9-keepu:
ANALYZE	-- using KEEP_CONSERVED_VARS=1
	-- starting from harm3d-kd9-fixp from t=640
	-- lasted till t=640.17, lots of noise within the horizon;
	-- not sure yet what exactly happened, but U1 suddenly diverges
		near the inner radial boundary and cutout;
	-- possibly U[] needs to calculate from the primitives after
		certain P[] fixes are implemented, like :
		-- inflow_check()
		-- set_cutout_boundary???
		-- maybe we shouldn't use set_cutout_boundary();




harm3d-kd9-nosetcutout
RUNNING	-- running starting from t=0 using a cutout boundary
	    except that none of the set_cutout_boundaryN() routines
	    are used;
	-- this is exploring the idea that it was never the boundary
           conditions but how the floor was handled;
        -- also, resetting B2 in the physical cell adjacent to the
            cutout does not preserve the divb constraint;
	-- starting from harm3d-kd9-fixp from t=680






harm3d-kd9-july-hybrid-nolim-useflr-r1:
	-- continuation of harm3d-kd9-july-hybrid-nolim-useflr
	-- crashed, diverging stuff near the cutout plus some errors that look
		like the inversion process is returning a solution that looks
		weird (v^i spontaneously sky-rockets)

harm3d-kd9-july-hybrid-nolim-useflr-r2:
ANAYZE -- rerun of harm3d-kd9-july-hybrid-nolim-useflr-r1 but w/ DT_out3=1 and
		fixing the
	--??



harm3d-kd9-july-hybrid-outflow
	-- just like harm3d-kd9-july-hybrid-nolim-r1-2  except that we are using
		spherical outflow boundary conditions, which are the same as before
		except that set_cutout_boundary3() is not used and that pure reflecting
		boundary conditions are used;
	-- in other words we are no longer resetting closest physical cell to the
		theta boundary to a value that we ensure that v^2 and B^2 are 0 at the
		boundary;
	-- also setting USE_GAMMA_CEILING_X1DN_BC = 0
	-- I am trying this out because I've noticed that v^2 can be quite discontinuous
		in r,phi right on the boundary.  This occurs because a physical cell
		next to the theta boundary can be reset without its neighbors being reset.

	-- starting from t=680 dump of harm3d-kd9-july-hybrid-nolim-r1-2, before hot spots
		arose on the cutout boundary;
	-- didn't work at all, explosion immediately
	--

harm3d-kd9-july-hybrid-outflow2
	-- like harm3d-kd9-july-hybrid-outflow  except that it does not use
	    reflecting boundary conditions along X2 like SPHERICAL_OUTFLOW usually does;
	-- i.e. it just copies all the primitives into the ghosts but still sets the
		flux to be zero along the X2 boundary;
	-- there was lots of heating along the cutout and it crashed sometime t=690=700M


harm3d-kd9-july-hybrid-outflow3
	-- like harm3d-kd9-july-hybrid-outflow2 but now does not set the flux to zero
		along the boundary;
	-- this should be more consistent that harm3d-kd9-july-hybrid-outflow2  but
		who knows if it'll be more stable
	-- explosive failure sometime t=690-700

harm3d-kd9-july-hybrid-outflow3-r1
	-- restart of harm3d-kd9-july-hybrid-outflow3 from t=690  w/ DT_out3=0.3
	-- did not finish run


harm3d-kd9-july-hybrid-outflow3-r2
	-- continuation of harm3d-kd9-july-hybrid-outflow3-r1



harm3d-kd9-july-hybrid-consinterp
ANALYZE -- like harm3d-kd9-july-hybrid-nolim-r1-2  but using BC_SPHERICAL_CONSINTERP
	-- using USE_GAMMA_CEILING_X1DN_BC=0
	-- failed to find good inversions in the ghost zones a lot;
	-- had problems early on in the first few steps;
	-- need to investigate further the cause of the crash;



harm3d-kd9-july-hybrid-cutout2
	-- like harm3d-kd9-july-hybrid-nolim-r1-2  but using USE_GAMMA_CEILING_X1DN_BC = 0
	-- also using cutout=0.026  which (with h_slope=0.35) gives th_min = 0.045pi
	-- starting from the beginning t=0

harm3d-kd9-july-hybrid-nolim-nogammamax
        -- starting from t=680 dump of harm3d-kd9-july-hybrid-nolim-r1-2, before hot spots
		arose on the cutout boundary;
	-- using GAMMAMAX = 1000
	-- putting no restrictions on gamma anywhere,i.e
	-- using USE_GAMMA_CEILING_X1DN_BC=0

harm3d-kd9-july-hybrid-nofloor
        -- starting from t=680 dump of harm3d-kd9-july-hybrid-nolim-r1-2, before hot spots
		arose on the cutout boundary;
	-- interpolating over instances of the floor
	-- setting to the floor and then flag for interpolation so that if there exists
		no viable interpolation stencil then we still have the floored state;
        -- didn't last past t=690
	-- didn't look good, maybe there are unforeseen problems with the procedure;




harm3d-kd9-stat:
	-- starting from t=723.4 dumping everytime step DT_out3=1e-3 and using the
	   new dump_stat() routine to dump statistical information to figure out
	   where the sudden increase in conserved variables comes from
	-- using t=723.4 data from the run harm3d-kd9-july-hybrid-nolim-r1-r3
        -- turning off all gamma limiting (except for global GAMMAMAX=50 one)
	-- I've changed the coordinate system, so I hope I'm using the right
	   parameters to replicate the old ones ;
	-- only integrating to t=723.5, at which one finds a dramatically
	   increased set of conserved variables
	-- seems as though all instances in which U(P(U_int)) differed from U_int
		significantly was where there were inversion failures;
		-- other than these points, the relative inversion error was
			always below 1e-6, except for a few points (<20)
			in U[4] that rel. differed by <1e-6
		-- the inversion failure points are located most of the time along
			the cutout, but the position along the cutout changes
			from timestep to timestep;
			-- in other words, the inversion failure points shift
				in position from timestep to timestep
	-- comparing rd1=(U[1]-U_pre[1])/U_pre[1]   with rd2=(U_of_P[1]_4-U[1])/U[1]
		so that rd2 is the relative change caused by the various fixups/inversion
		errors and rd1 is a measure of the physical rel. change caused by the fluxes
		and the sources;
		-- find that  rd2 is negligible every except along the cutout boundaries
			and a little bit away from the cutout boundary w/ the explosion;
		-- the ratio rd2/rd1 varies dramatically in space (r,phi) along the cutouts
		-- this suggests that the floor, is what is causing the problem;
	-- now trying to find the maximum source of "fixup/interpolation" error;
	-- looking at the change caused by the 1st bounds() call:
		-- it seems that the change is only significant along the cutout
			boundary, as expected (elsewhere there are variations at
			roundoff error levels);
		-- at the failure cutout boundary, whenever there is no change
			from the bounds() call (i.e. set_cutout_boundary3() has done
			nothing) there is large positive v2;

	-- found an instance where U1 change significantly off the cutout boundary between
	   U_of_P_1 and U_of_P_2 (i.e. after the 1st bound() call);
		    -- realized however that bounds() was not the culprit but that GAMMAMAX
		       is also imposed in this step even though it will later be interpolated
		       over;
		    -- confirmed that ninterp != 0 for this point;
		    -- running this check for all P and times...
		    -- I did confirm that for all points not along the cutout boundary where
		       there were signficant differences between U_of_P_1 and U_of_P_2, that
		       these points were flagged for interpolation implying that they were
		       altered by the GAMMAMAX condition;

	-- looking at integrated absolute differences between consecutive fixes:
		--  U0:  0 > (1e2-1e3) interp=all      >  (1e2) floor   > (1e1) bounds1   > (1e3) bounds2
		--  U1:  0 > (1e1) all>~interp>~floor  > (1e4) bounds1  > (1e2) bounds2
		--  U2:  like U1
		--  U3:  0 > (1e2) all                 > (1e.2) bounds1 > (<1e1) interp   > (1e.5) floor > (1e1) bounds2
		--  U4:  0 > (1e2) all=interp          > (1e1) floor    > (1e2.5) bounds1 > (1e2) bounds2
		--  U5:  0 > (1e12) all                > (1e2) bounds1  > (1e2) interp    > (1e1) bounds2
		--  U6:  0 > (1e4) all=bounds1         > (1e12) interp  > (1e2) bounds2   >> floor
		--  U7:  like U5

		-- so, B1 and B3 are changed insignificantly from the fixes
		-- U0 is affected to the 1% level primarily by the interpolation;
		-- U1,U2 have the largest contribution of the fixes, which accounts for 10% of
			total change in a timestep;  this error is equally due to
			the interpolation and floor;
			-- why do U1 and U2 respond so similarly to the fixes?
		-- U3 is affected to the 1% level primarily by bounds1 and interpolation;
			-- this is because of the cutout re-interpolation at the boundary;
		-- U4 is affected by 1% mostly by interpolation but a little by the floor
		-- U6 is affected at the 0.1% level by bounds1, rest is insignificant;
		-- U5 and U7 are not affected significantly

	-- the change in U1 is large (~O(1)) when the floor or u/rho>3 criteria are imposed;
		-- the ratio criterion happens a lot along the cutout w/ the ultimate explosion;
		--


hyades:fb-cutout
	-- fishbone moncrief init data using KD-HARM2d-hybrid
		grid structure (h_slope=0.35,cutout=0.045) in 2d;
	-- n_within_horizon = 5
	-- lasted with no problems to 1460M


hyades:fb-cutout-r1
	-- continuation of hyades:fb-cutout
	-- lasted with no problems till t=2000M
	-- noticed that the inversion is constantly failing along the
		cutout boundaries at places and that it is also constantly
		failing to fix them up too; however, i guess the code
		can stably evolve this state.
	--



hyades:kd-harmgrid
	-- KD init data with h_slope = 0.35, outflow (no cutout) boundary
		conditions, n_within_horizon = 5,  cour=0.4
	-- ran till t=1740  with no problems
	-- gamma hits the ceiling frequently in localized out-going bursts
		in the funnel region
	--

hyades:kd-harmgrid-r1
ANALZYE	-- continuation of hyades:kd-harmgrid
	-- lasted to t=2880M  with no problems
	-- there were many ivnersion and fixup failures along the cutout
		but not as many as in the fb-cutout run; however, it seems
		as though this run had more failures in the funnel near the
		innner radial boundary;
	-- the regions of high failure along the cutout boundary were transient
		and seemed to follow
	-- lasted till t=4000M w/o problems


hyades:kd-harmgrid-limitgamma
ANALYZE	-- just like hyades:kd-harmgrid-r1  but limiting gamma to 5  within
		the horizon
	-- lasted w/o problems to t=1760

hyades:kd-harmgrid-limitgamma-r1
        -- continuation of hyades:kd-harmgrid-limitgamma
	--  end of run at  t=4000M w/o problems


hyades:kd-harmgrid-smflr
hyades:kd-harmgrid-smflr-r1
	-- just like hyades:kd-harmgrid  but with
 		 using RHOMIN=2e-9,UUMIN=2e-11
	--  end of run at  t=4000M w/o problems

hyades:kd-harmgrid-cutout
ANALYZE	-- h_slope=0.35,cutout=4.5e-2, KD init data

hyades:kd-harmgrid-cutout2
ANALYZE	-- h_slope=0.35,cutout=2.6e-2, KD init data
	-- this value of cutout gives theta_min=0.045pi  for this given h_slope;


hyades:kd-harmgrid-consinterp
ANALYZE	-- like hyades:kd-harmgrid-cutout  but using BC_SPHERICAL_CONSINTERP



hyades:2d-nocutout
ANALYZE	--  2d running starting from t=0 using a cutout boundary
	    except that none of the set_cutout_boundaryN() routines
	    are used;
	-- this is exploring the idea that it was never the boundary
           conditions but how the floor was handled;
        -- also, resetting B2 in the physical cell adjacent to the
            cutout does not preserve the divb constraint;
	-- died early on... why??

hyades:keepu
RUNNING	-- always keep U, never find U(P)
	-- running from t=0
	-- didn't last too long, need to investigate why???

hyades:keepu-nosetcutout
ANALYZE	-- always keep U, never find U(P)
	-- using spherical outflow boundary conditions and no
	   set_cutout_boundaryN()  routines;
	-- running from t=0
	-- didn't last too long, need to investigate why???



harm_fid2d_ppm
	-- old run without much sophistication
	-- using fishbone moncrief init data
	-- n_within_horizon = 5,  h_slope = 0.35, no cutout
	-- ran for 2000 M fine;



sdsc:harm3d-kd9-ee
	-- restart of harm3d-kd9-july-hybrid from t=550 dumping every 20M using
		FIXUP_TREE=2, entropy equation fix, no CUTOUT fixups, only limiting
		gamma in X1DN ghost cells;

	-- going past t=1500M  fine
	-- entropy equation is being used constantly along the cutout boundary and
		some of the funnel near the horizon;
		-- the boundary of this region is smooth and does not vary in time significantly
			other than extending further in r over time (as jet propagates outward);
	-- fixup_interp_prim() is rarely used and only occurs mostly along the cutout near
		the horizon
	-- fixup_inter_v() is used about 10 times more often, and again occurs mostly
		near the cutout boundaries and near the horizon;
	--


sdsc:harm3d-kd9-ee1
	-- restart of harm3d-kd9-july-hybrid from t=550 dumping every 20M using
		FIXUP_TREE=1, entropy equation fix, no CUTOUT fixups, only limiting
		gamma in X1DN ghost cells;




woodhen:harm-kd9-newee
DONE	-- restart of harm3d-kd9-july-hybrid from t=550 dumping every 20M using
		FIXUP_TREE=2, entropy equation fix, no CUTOUT fixups, no Tmax,
		no limiter on gamma anywhere except for global GAMMAMAX  one,
		and NOT using the skip_mpi=1 option with bounds();
	--



woodhen:harm3d-kd9-oct
RUNNING	-- using various history/mpi dump fixes and coordinates closer to
		GRMHD grid (smaller cutout)
	-- full resolution (192x192x64)
	-- a = 0.9

woodhen:harm3d-kd0-oct (now on datastar)
RUNNING	-- just like woodhen:harm-kd9-oct  except with
 		a = 0.
	-- ran till t=500, now submitted to run on datastar

woodhen:harm-kd9-lowres-stopee
	-- starting from t=1500 of harm3d-kd9-newee run  but not using the
		entropy equation.
	-- catastrophic explosion occurs at  t=1520-1540
	-- so we need the entropy equation after the initial transient stage!!!!



woodhen:oct-lowres-smflr       (under harm3d-kd9)
	-- lowres run 96x96x32
	-- like harm3d-kd9-oct except that it uses a floor a factor 1e-2 that of the old run
	-- with the harm3d-kd9-oct, we are not launching a jet;  it seems that it is matter
		dominated, or at least bsq/rho is not very large compared to the grmhd runs;
	-- thinking the floor may be injecting too much mass into the jet, we lowered the floor
	-- this did not help produce a jet

woodhen:oct-lowres-smallerflr  (under harm3d-kd9)
	-- lowres run 96x96x32
	-- like oct-lowres-smflr except it uses a floor 1e-2 that of it
		 (or 1e-4 that of the harm3d-kd9-oct run)
	-- still did not launch a jet


woodhen:oct-lowres-polarflux  (under harm3d-kd9)
ANALYZE	-- like harm3d-kd9-oct except that  we do not set the x2-dir. fluxes to be zero
	-- lowres run 96x96x32
	-- thinking that the polar boundary condition could be interfering with the
		jet, we are trying out an even less agressive method;
	--

woodhen:oct-lowres-nocutout  (under harm3d-kd9)
ANALYZE	-- lowres run 96x96x32
	-- thinking that the cutout boundary condition could be interfering with the
		jet, we eliminated it altogether
	-- still no jet
	-- for runs with non-trivial cutouts, the heating that occurs along the cutout
		is artificial as we can see from the absence of it in this run
	-- we actually see less of a jet with no cutout since the high-gamma
		fluid in the previous runs was due to matter being squeezed
		against the cutout and shooting along it;


hyades:oct-rhocut-2d
	-- noticed that our initial conditions were not like grmhd's exactly in that
		our density cutout (rho_min), which sets the extent of the initial
		field lines in the disk, were different (grmhd: rho_min=0.25*rho_max,
		harm3d had rho_min=0.5*rho_max).
	-- so setting rho_min = 0.25*rho_max, but like harm3d-kd9-oct in all other
		respects;
	-- this does make the initial bsq cofiguration like grmhd's
	-- does NOT seem to change anything regarding the jet;


hyades:oct-2d-mc:
RUNNING -- like harm3d-kd9-oct except in 2d and using 	MC limiter
	--


hyades:oct-2d-hi-ee  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee)
	-- like harm3d-kd9-oct but 2d, and using
	     BETA_MIN = (1.e-3)   (as opposed to 1e-2)
		and using no cutout and using a floor 1e-4 that of harm3d-kd9-oct
		and using rho_cut = 0.25 rho_max
		and freeing the flux constraint (i.e. flux_x2 = 0 ) on the cutout boundar
	-- JETS!!!!
	-- GRMHD's bsq/rho reaches 10^8  in jet, while this jet reaches 10^5
	-- GRMHD's gamma reaches >10 while this jet reaches 2 right along the polar boundary
	-- inflow right along polar boundaries



hyades:oct-2d-hi-ee-beta  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee-beta)
	-- like hyades:oct-2d-hi-ee but using BETA_MIN=1e-2
	-- by t=1000, bsq/rho reaches about 10^7  (an order of magnitude smaller than grmhd)
	-- by t=1000, max(gamma) is about 5 (grmhd's is >10)
	-- slightly more jet than ee
	-- inflow right along polar boundaries

hyades:oct-2d-hi-ee-cutout  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee-cutout)
	-- like hyades:oct-2d-hi-ee but using a cutout of GRMHD
	-- by t=800, blobs of gamma>=10 material appear in funnel and along cutout boundary,
		probably because of the interaction there;
	-- no inflow along polar boundaries, probably because matter is squeezed along it

hyades:oct-2d-hi-ee-flr  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee-flr)
	-- like hyades:oct-2d-hi-ee but using RHOMIN=2e-6,UUMIN=2e-8
	-- outflows but weaker than in oct-2d-hi-ee
	-- inflow right along polar boundaries

hyades:oct-2d-hi-ee-noflux  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee-noflux)
	-- like hyades:oct-2d-hi-ee but using flux_x2=0  on polar boundaries
	-- very similar to hyades:oct-2d-hi-ee
	-- inflow right along polar boundaries


hyades:oct-2d-hi-ee-cutout-flr  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee-cutout-flr)
	-- like hyades:oct-2d-hi-ee-cutout but using RHOMIN=2e-6,UUMIN=2e-8

hyades:oct-2d-hi-ee-cutout-beta  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee-cutout-beta)
	-- like hyades:oct-2d-hi-ee-cutout but using BETA_MIN=1e-2

hyades:oct-2d-hi-ee-cutout-noflux  (in /data1/gauss/grmhd/harm2d/harm3d-kd9-2d-hi-ee-cutout-noflux)
	-- like hyades:oct-2d-hi-ee-cutout but settting flux_x2 = 0 along cutout


hyades:no-cooling
DONE	-- in gauss:/data1/gauss/grmhd/harm3d/harm3d-kd9/test-cooling
	-- run from t=0 to t~=3000  without cooling  to be used as
		fiducial run
	-- used old coordinates with a bigger cutout
	-- leads to an explosion near the horizon when the high density inflow
		stream hits the cutout boundary at around t=2000
		(DON'T CROSS THE STREAMS!! )
	--


hyades:coolrun1
RUNNING	-- in gauss:/data1/gauss/grmhd/harm3d/harm3d-kd9/test-cooling
	-- using old coordinates (bigger cutout) but with history/mpi dumps fixes
	-- using t=1500 dump from hyades:no-cooling as source to cool;
	-- setting h_o_r=0.1 , q_exp = 1, s_norm = 0.1
	-- after ~50 M noticed that most of the cooling is from hot matter
		in the funnel
	--

hyades:no-coolingrun-oct
	-- 2d, KD, a=0.9 run
	-- using new coordinates with a smaller cutout
	-- in /data1/gauss/grmhd/harm3d/harm3d-kd9/test-cool
	-- no cooling, fiducial run for comparison

hyades:coolrun1-oct
	-- like hyades:no-coolingrun-oct but with cooling
	-- s_norm = 0.1, q = 1 , h_o_r = 0.1
	-- using new coordinates with a smaller cutout
	-- in /data1/gauss/grmhd/harm3d/harm3d-kd9/test-cool
	-- too little cooling, disk temperature did reduce but
		was always above the target temperature and H/R
		was always larger than h_o_r


hyades:coolrun4-oct
	-- like hyades:no-coolingrun-oct but with cooling
	-- s_norm = 10, q = 0.5 , h_o_r = 0.1
	-- using new coordinates with a smaller cutout
	-- in /data1/gauss/grmhd/harm3d/harm3d-kd9/test-cool
	-- maybe too much cooling since disk's temperature was
		a 2-3 times cooler on average of the target temperature;
	-- the plunging region is hotter than the rest of the disk, which
		possibly suggests that our cooling function isn't large
		enough there;
	-- also, H/R < h_o_r

hyades:coolrun5-oct
	-- like hyades:no-coolingrun-oct but with cooling
	-- s_norm = 1, q = 0.5 , h_o_r = 0.1
	-- using new coordinates with a smaller cutout
	-- in /data1/gauss/grmhd/harm3d/harm3d-kd9/test-cool
	-- these parameters seem to yield a disk with temperature
		straddling the target and with H/R ~ h_o_r
	-- the plunging region is still hotter than the rest of
		disk

hyades:coolrun5-new
	-- like hyades:coolrun5-oct but with different cooling profile
	-- s_norm = 1, q = 0.5 , h_o_r = 0.1
	-- cooling profile is different in that Omega is now found assuming that
		the plunging orbits have the same energy and angular momentum
		of the equatorial ISCO ;
		-- this yields a larger Omega in the plunging region than what
			was used before and should cool this region more, making
			it more like the rest of the disk;
	-- in /data1/gauss/grmhd/harm3d/harm3d-kd9/test-cool



hyades:coolrun2
RUNNING	-- like hyades:coolrun1 but with
		s_norm = 1

----------------------------------------------------------------------------------

		-- we are seeing that at the lower courant factor that there are
			fewer instances of inversion failures;

		-- even though bsq does vary near the pressure divergence, it is much
			lower than the pressure and probably does not cause the divergence;
			meaning, we do not think that it is due to the loss of accuracy
			problem when b^2/p >> 1   ;


		-- rho gamma  is also smaller that p there ;


			-- not sure



-- added init.ring.c data that starts off with ucon=[1,0,0,0], is phi-independent
	and should just fall into the hole



#############################
Wed May 23 11:48:08 EDT 2007
-------------------------------

-- changed the way code handles all failures in vchar();
	since there are reasonable fallback strategies for these failures,
	I made them non-fatal;  now just a simple message is printed whenever
	any of them occurs.



-- adding option "ALLOW_NEGATIVE_DENSITIES"
	then when set to non-zero value, allows the fluid to have negative
	rest-mass and internal energy densities;
	-- the place where negative densities can be a problem is in vchar();
		-- here we use the same recovery methods as before;
	-- this idea originates from Jon McKinney in that he found that this
		unphysical states are valid numerical states and
		make for a more stable evolution;



-- noticed that set_cutout_boundary*() did not calculate df10 and df21 before using
	them;  fixed it;
	-- also, none of them changed the first real cell even though I meant to make
		them;  I corrected this too.
	--

-- added new set_cutout_boundary4() that diminishes the pressure (rather u)
	so that the total energy never increases when changing U2 and B2 in
	the first real cell;



-- added macro USE_GAMMA_CEILING_X1DN_BC to be used to control whether to
	limit gamma in ghost zones at X1DN boundary;
	-- added gamma cap to inflow_check() in bounds.c  for inner boundary;
	-- does this for all inner boundaries, but inflow_check() is only called
		for the radial boundaries;



#############################
Tue Jun 19 16:59:29 EDT 2007
-------------------------------

-- added line to step_ch() before first advance() call ph[] = p[] so Utoprim()
	has a "good" guess i.e. a 2nd-order value;



-- added routine in  testing/full_dumps/full_dumps.c  that takes a given
	version of HARM3d and some hdfs (e.g. from a parallel run) and
	calculates all auxiliary grid functions used in the half-step
	integration process;
	-- all but the phi domain boundaries are set correctly;




-- eliminating the phi-component source calculation in source();


-- adding directory and code in ./testing/inversion_test  to read in
	primitive variables and conserved variables,
	perform the inversion from conserved variables to primitives
	and compare with read in prim. var's;
	-- this test is in response to the hypothesis that the inversion
		 is causing the sudden increase in E_tot at t=716  of the
		run harm3d-kd9-limit-gamma-r2-2  near the cutout boundary;
	-- the energy divergence is associated with a pressure divergence and
		(p goes from   p<<bsq  to p>>bsq )



-- adding macro OUTPUT_CONSERVED_VARS  that indicates whether we are to
	allocate space for the storage of the conserved variables and to
	them write them to the hdf files;
	-- the conserved variables are saved immediately after updating them ;



-- added macro MAX_UU_RHO_RATIO  that is used to constrain uu so that uu/rho
	is never larger than this number;
	-- use the constraint if USE_MAX_UU_RHO_RATIO != 0


#############################
Thu Jun 28 13:28:42 EDT 2007
-------------------------------

-- added init.linearwave.c  that contains the tests used in the HARM paper
	involving linear wave propagation;



#############################
Thu Jul  5 15:08:21 EDT 2007
-------------------------------
-- removed an errant extra factor of sqrt(-g) in the calculation of the E_tot and L_tot ;



#############################
Mon Jul 16 16:24:17 EDT 2007
-------------------------------

--trying to run harm3d  on Frans' cluster "woodhen":
	-- needed to install sdf and hdf libraries
	-- some problems with "make test" step in the hdf installation
		since the float->double type conversion test fails; not important;
	-- getting the following error when i try to do some sort of sdf or hdf i/o to
		disk using more than one processor:

[0,1,1][btl_openib_component.c:1328:btl_openib_component_progress] from woodhen-002 to: woodhen-001 error polling
HP CQ with status LOCAL QP OPERATION ERROR status number 2 for wr_id 66029176 opcode 0
mpiexec noticed that job rank 0 with PID 13599 on node woodhen-001 exited on signal 15 (Terminated).

	-- googled the error but nothing else, just somebody asking for help about this
		issue but no one replied;

	-- I'm recompiling the libraries to make sure that they're using the new openmpi
		libraries;
	-- still can't get it work with openmpi....
	-- works fine with everything (libraries and code) compiled with mpich
		-- problem is (I believe) is that mpich does use the infiniband interconnect,
			but the code seems to be running >~90% via top so that's fine... :}



#############################
Thu Jul 19 13:17:10 EDT 2007
-------------------------------
-- adding routine dump_stat() to dump_hdf() that dumps a series of statistics to files called
		<RUN_TAG>_STAT_<timeid>.h5
	-- stat's dump frequency controlled by DT_out[OUT_STAT]
	-- changed OUT_*  variables
	-- increased value of N_OUT_TYPES by 1;
		-- note that restarting with old hdf's that do not have OUT_STAT
			written will result in not setting DT_out[OUT_STAT] or T_out[OUT_STAT]
			to the right value ;
	-- dump_stat() also responsible for calculating and tallying statistics;
	-- added N_U_OF_P  to decs.h to represent  the number of instances we
		calculate U(P) in the timestep process for statistical reasons
	-- we want to monitor the change in U along the entire update process;
		-- only save this information in on the last timestep since the intermediate
			step is only really used to calculate the fluxes and are
		-- added global variable "final_substep" that indicates that we are in
			the final substep of the integration process;
		-- calculating the conserved variables from primitives immediately after:
			-- inversion calculation
			-- floor and "MAX_UU_RHO_RATIO" adjustments;
			-- fixup interpolation (incl. FAIL_GAMMA_MAX, FAIL_GAMMA_CALC, FAIL_UTOPRIM)

	-- calculate the contribution from flux along the boundaries for those
		conserved variables that have no sources;

	-- since dump_hdf5.c is so large, I am making a new file called dump_stat.c
	   that will contain the statistical routines;

-- adding new boundary condition specified by macro BC_SPHERICAL_CONSINTERP
	-- does BC_SPHERICAL_OUTFLOW, except the densitized conserved variables are copied
		in the ghost zones along the X2 boundaries; t
	-- we're trying this because it seems as though the densitized conserved variables
		are very smooth near the cutout boundary, but the primitives are not.
	-- routine to do this is   cutout_cons_interp() of bounds.c
	--
	-- getting error message:
cutout_cons_interp(): inversion failure at (3,67,3,0,7)
cutout_cons_interp(): U =                           nan                           nan
                         nan                           nan                           na
n                           nan                           nan
 nan
cutout_cons_interp(): P =      1.397580805962454163e-07      2.371054566419577965e-10
    0.000000000000000000e+00      0.000000000000000000e+00      0.000000000000000000e+0
0                           nan                           nan
 nan



-- adding additional parameters to the DIAGONAL/MIXED coordinate systems;
	-- making it such that the user can specify the bounds on theta, independent of
		the bounds on x2;
	-- x2 now always runs from [0,1]
	-- labeling changes  (or old version of code lines) ala  //-newcoords
	-- making DX2_CUTOUT a global variable called  th_cutout
	-- making global variables  th_beg, th_end, th_length
		which represent, respectively, the min. value of theta, max. value
		of theta and the length of the grid in the theta-direction;

	-- need to:
		DONE -- adding all coordinate parameters to the write/read dump routines;
			-- added the following variables to the read/write dumps:
				cour, gam, a, h_slope, th_cutout, th_beg, th_end, X1_slope,
	                        X1_0, R0, Rin, Rout, r_isco, r_horizon, n_within_horizon,
				NG,NP

		DONE -- make changes to all necessary init.*.c files ;
		DONE -- make sure we get rid of all instances of DX2_CUTOUT;
		DONE -- double check all instances of h_slope;
		DONE -- correct find_min_dt();
		DONE -- redo test run to make sure new coordinates work;
				-- after 10M , absolute difference were no larger
					than roundoff error, which is what you'd
					expect since the initial data is a function of
					of the coordinates and the coordinates changed
					by roundoff amounts;



-- adding new routine check_for_crash() in step_ch.c  that is called from main()
	after each step to see if the simulation has reached a "bad state", or one
	which we should nip in the bud.



#############################
Thu Jul 26 16:59:32 EDT 2007
-------------------------------
	-- adding file dump_stat.c
	-- adding routine free_global_arrays() to main.c that frees all the
		global dynamically allocated arrays;

	-- doing a 128^2 2d run to make sure that dump_stat is working correctly:
		-- sint_d_U[4] (col 118-125) (comparing U_fin[] and U_fin2[] which are
			respectively the conserved variables calculated
			from the final primitives and that taken after each fix)
			for the first few timesteps always evaluates to something near
			roundoff error which is what we were expecting;


	-- confirmed that sint_F[1] (ones along x2 faces) are all zero (except for B1-B3
		components since they are changed to preserve CT conditions
	-- found a bug in the way that U_of_P[1] was being calculated (index problem);


#############################
Wed Aug  1 17:53:25 EDT 2007
-------------------------------

-- fixing a problem with how MAKE_STAT recorded U_of_P[l][3] was calculated;
	-- the problem was that I was doing the calculation in fixup_utoprim()
		within the bulk of the routine, which is only reached if there
		were any pflag[] failures in that domain for that timestep;
	-- so U_of_P[l][3] was not set when there were no pflag[] failures and
		then produced misleading results;
	-- now put the  U_of_P[l][3] calculation after fixup_utoprim() using
		calc_U_of_P() that does it for all cells,domains,times;

-- adding macros CHECK_INVERSION  and INVERSION_THRESH   that control,
	respectively, whether to report inversion method errors in U (i.e. (Unew(P(U))-U)/U)
	and at what threshold or accuracy to report relative errors;



-- adding MAKE_STAT2 macro that controls whether or not to dump source, fluxes,
   wave speeds, etc. ,
	-- adding new file dump_stat2.c that houses these new routines;
	--

############################
Tue Aug  7 16:34:34 EDT 2007
-------------------------------
-- changed fail() in diag.c  such that it counts how many times each each type of
	failure occurred;  if this number exceeds some ceiling, then fail() no longer
	prints error messages;
	-- this is

-- adding entropy equation...
	-- adding macro USE_ENTROPY_EQ that controls whether we use it or not
	-- addding macro EE_AT_LARGE_BSQ that controls whether to use EE at large bsq/p
		-- if true then we use EE whenever bsq/p > EE_BSQ_U_RATIO (another new macro)

	-- adding macro N_EOM to control the number of equations of motion we have;
		-- this is in principle different than NP = number of primitive variables;
	-- adding EOM_LOOP which uses "l" and loops over N_EOM;

!!	-- calculating all the fluxes, U[], etc. for the entropy equation for all
		cells is inefficient;  need to somehow only calculate these
		when they are needed;


#############################
Wed Aug  8 16:04:12 EDT 2007
-------------------------------
-- adding macro KEEP_CONSERVED_VARS  which if set keeps the conserved variables
   throughout the time step so that we are always updating them, not the primitive
   variables;
	-- in this way, we can think of the primitive variables as just auxiliary
		variables that help us calculate the fluxes;
	-- this may minimize noise that develops near the inner radial boundary
		in the conserved variables.
	-- adding global array Uglob[] that stores the conserved variables;
	-- need to

-- also changed the code to use Uglob[] at the corrector step so that we do not have
	to calculate again;

-- added macro FIX_CUTOUT_PRESSURE that, if set, executes a code segment in fixup()
	that runs across the cells adjacent to the cutout boundary and sees if their
	pressure is much different than their neighbor in x2 and if  bsq/uu there;
	-- if so, then we copy the x2-neighbor's pressure to the cutout cell;


#############################
Thu Aug  9 11:03:46 EDT 2007
-------------------------------

-- storing x[] and xp[] into global arrays:  x_gf[] and xp_gf[];
	-- since the coordinates at the cell centers are used most often, we only
		store these;

-- changing name of Uglob to U_gf[];



-- confirmed that Utoprim_2d_fast() and Utoprim_2d() work the same to roundoff error
	by using testu2p.c  in ./testing/utoprim


#############################
Sun Aug 12 22:58:09 CDT 2007
-------------------------------
-- making a few big changes:
   -- adding routine recover_primitives() to step_ch.c that is called
      after the update to U  in advance();
      -- it's responsibilities include performing the inversion
      -- making sure the inverse makes sense;
      -- calling fixup1zone()

   -- added argument "gamma" to inversion routines and fixup1zone() so that
      fixup1zone() does not have to calculate it a second time and so
      that we can check to see if gamma is reasonable coming from the inversion
      routines;

   -- moved the FIX_CUTOUT_PRESSURE stuff from fixup() to set_cutout_boundar3()
      since fixup() will not be used every timestep

   -- add gamma_calc() call in fixup_utoprim_2d() to be consistent with its
      use of fixup1zone()

   --


-- adding CHECK_PRIM_CHANGE macro that controls whether or not we see if there
	are large changes in the primitive variables after the inversion routine;


#############################
Tue Aug 21 14:25:38 EDT 2007
-------------------------------

-- tested out a few inversion routines that use the entropy density instead
	of the total energy equation;
	-- the test location is  ./testing/entropy_eq

	-- the routines timed like the following:
2d_fast : 7.880u 0.294s 0:08.20 99.6%     0+0k 0+0io 1pf+0w
ee      :11.679u 0.286s 0:12.07 99.0%    0+0k 0+0io 1pf+0w
ee2     : 8.869u 0.291s 0:09.19 99.5%     0+0k 0+0io 1pf+0w

	-- ee2 fails sometimes when ee doesn't, but these are instances where
		ee returns primitives O(1) errors;
	-- ee2 is faster so we will use it;



-- adding routine fixup_entropy_eq() to fixup.c that integrates the entropy equation
	and then inverts for the primitive variables with the new entropy;
	-- need to make it conform to the "KEEP_CONSERVED_VARS" method;

	-- adding global array c_glob[]  that holds the min/max wave speeds per
		cell per face that are needed for the fixup_entropy_eq() ;


-- adding time dimension to U_gf[]  since it's needed by  fixup_entropy_eq();



-- changing fixup logic in order to add entropy equation fix;
	-- need to add :
		DONE -- check_floor()
		DONE -- check_gamma()
	        DONE -- check_Tmax()
		DONE -- fixup_floor()
		DONE -- fixup_gamma()
                DONE -- fixup_Tmax()
                DONE -- check_entropy_eq()
	        DONE -- fixup_entropy_eq()
		DONE -- fixup_interp_prim()  (rename of fixup_utoprim)
		DONE -- fixup_interp_v()
	-- need to change:
		DONE -- fixup() to include new routines
		-- recover_primitives() to include new routines
		-- change how arrays are recorded for dump_stat() ;
		DONE -- add additional cases in fail()  for the new failure states;
		DONE -- add support for new elements of nfail[] and nfailtot;
			DONE -- make the nfailtot_*  variables elements in an
				array nfailtot[N_NFAIL];



-- renaming fixup_utoprim() to fixup_interp_prim()
-- renamed NFAIL_FIXUP  to NFAIL_INTERP_PRIM
-- adding NFAIL_UTOPRIM_EE
-- adding NFAIL_TMAX
-- adding NFAIL_INTERP_V
-- renamed FAIL_FIXUP to FAIL_INTERP_PRIM
-- adding FAIL_INTERP_V
-- renaming FAIL_UTOPRIM_NEG  to FAIL_FLOOR
-- adding FAIL_TMAX
-- renaming FAIL_UTOPRIM_CONV to  FAIL_UTOPRIM
-- adding FAIL_UTOPRIM_EE

-- adding FAIL_CUTOUT_PRESSURE
-- adding FAIL_CUTOUT_GAMMA
-- adding FAIL_CUTOUT_BC

-- adding NFAIL_CUTOUT_PRESSURE
-- adding NFAIL_CUTOUT_GAMMA
-- adding NFAIL_CUTOUT_BC

-- adding NFAIL_USE_EE
-- adding FAIL_USE_EE


-- adding PFLAG_NONE, PFLAG_INTERP_PRIM, PFLAG_INTERP_V  to represent the various
   states the pflag[] array can take;



DONE !! add "state" argument to all calls to fail();
!! update dump_stat();
!! make sure that [i,j,k]curr are set before any instance of fail() is called;
DONE !! make sure that pflag is always set to one of two values;
DONE !! use these two values of pflag in the fixup_interp_*() routines;

-- at the bottom of recover_primitives();
	-- need to make sure that the "state" argument in all the fail() calls
		are correctly set;
	--



-- fixed a problem with set_cutout_boundary(), set_cutout_boundary3() and
	set_cutout_boundary4() where in they failed to initialize pflag1,pflag2
	to zero for each interpolation;


-- finished making the fixup tree changes;
	-- found out that FIXUP_TREE = 1  does not work very well;
		-- the entropy equation is used too much and the disk's accretion
			is delayed for a long time;
	-- found a bug in which fixup_entropy_eq() was using gdet in the definition
		of S  but  utoprim_1d_ee2.c was not;
		-- changed all the utoprim_1d_ee*.c routines to use the densitized
			entropy function now;
		-- a run was done with this bug and it seemed to work fine;  now problems
			along the cutout boundary and gamma remained below 20 for all time;

	-- tested fixup_entropy_eq() by comparing solutions from the regular
		inversion and the entropy equation inversion during a simulation run;
		-- found that all the variables except for the pressure were identical
			to the ~1e-5 level but the pressure was usually 10-30%  larger
			in the EE solution;

	--

-- changing step_ch() so that it only calls fixup_interp_*() and the 2nd bounds() call
	if failure_exists==1 globally ;


-- adding mpi_global_imax() that finds the global max of an integer;


#############################
Wed Aug 29 16:13:31 EDT 2007
-------------------------------

-- found out that sometimes Utoprim_1d_ee2() fails that Utoprim_2d_fast() does not fail;
	-- in all the instances tested so far, Utoprim_1d_ee() finds solutions;
	-- so I am adding it as the default entropy eq inversion routine and making Utoprim_1d_ee2()
		the backup routine (in case Utoprim_1d_ee() fails) ;
	-- if Utoprim_2d_fast() succeeds but neither Utoprim_1d_ee() or Utoprim_1d_ee2()
		succeeds, then we use the full solution no matter what (even if we are
		in the bsq>p regime);   it's better to have a solution than no solution
		at all since interpolation doesn't work very well;


-- added routine  int check_boundary_pflag( void )   that returns with non-zero status
	if there are any non-zero pflag[] values along any subdomain's faces;
	-- this is used to determine whether bounds_mpi() and bounds_mpi_pflag() need
		to be called;


-- adding argument to bounds() that controls whether or not the bounds() call
   is the second one (after the fixup_interp_*() calls) so that we can skip the
   MPI calls if we  can;


#############################
Fri Aug 31 14:51:28 EDT 2007
-------------------------------

-- IT SEEMS THAT THE ENTROPY EQUATION FIX IS A GREAT SUCCESS.  RUN GOING PAST T=970
   NOW AND EVERYTHING LOOKS FINE!!!!!!!!!!!!!!!!!!!!!!!!!
	OMFG!!!!!

-- updated fixup_interp_prim() and fixup_interp_v() since they are not exactly how
	they should be per directions in ./info/fixups


-- update dump_stat() and things associated with MAKE_STAT
	-- removing (this information is now in nfail*[] arrays):
		-- mask_pflag
		-- ninterp
		-- nfloor
		-- tot_nfloor
		-- tot_ntinterp

	-- putting U[] calculations in fail():
		-- added routine calc_dU_stat() to find the change in U[] after each
			fixup change to p[]

	-- using U_out[] now as an array that stores the most current values of the
		conserved variables so that we can measure the change in U[] from a
		given local fixup

	-- we are now only writing in dump_stat():
		-- U_pre[] (U before full time step)
		-- U_fin[]  (U after all fixups )
		-- U_avg[]  (U time-averaged since last stat dump)
		-- dU_[0-4]  (change in U from each type of fixup integrated from last STAT dump)

	-- moved nfail[] and nfailtot[] to dump_stat();


-- added dump_ener() that dumps space-integrated conserved variables and the
   fluxes along the physical boundaries every time step;



#############################
Fri Sep  7 15:43:56 EDT 2007
-------------------------------

-- moved calc_dU_stat() to dump_stat.c


-- trying to track down seg fault in "MAKE_STAT" stuff
	-- noticed that I was


-- also trying to fix the weird MPI-boundary artifact seen when using the entropy equation;


-- format of *_stat.out  file:

nstep t dt nfailtot[0:N_NFAIL-1] sint_U[0:7] sint_dU[0:4][0:N_STAT-1]
1     2  3    4-18                19-26        27-37,38-48,49-59,60-70,71-81

where
	sint_U[0:7] = sum of U_out[0:7] * dV
	sint_dU[0:4][0:N_STAT-1]  =  sum of dU_stat[0:4][0:N_STAT-1]

	N_STAT = 11
	N_NFAIL = 15




-- added DUMP_ALL_STAT  macro that controls whether to dumps all the dU_stat[] and U_*[]
	arrays;



#############################
Wed Sep 12 12:12:56 EDT 2007
-------------------------------

-- trying out the new mpi routines (called "*_fast()") on hyades doing 3d runs of
	dimensions 96x96x32 and it seems as though the new routines are slightly (<1%)
	slower than the new routines.
	-- this run only used 9 processors  : 3x3x1

	-- will have to try larger runs and on different clusters


-- fixed a bug with how M_tot was calculated; it was not being summed over processors

-- profiling the code again...

	-- tweaking func_vsq() a little since it is the slowest part of utoprim_2d_fast()
		-- seemed to help a little
	-- vectoring assignments in reconstruct_fast() more...
		-- found that they did not speed anything up, so I'm reverting to the
			old way;

	-- adding gridfunctions gdet_inv[] (1/gdet) and g_ncon[] (4-vel. of FIDO)
		to eliminate calculating these each time;

	-- adding boundary_mpi_pflag,boundary_phys_pflag , which are set if
		 pflag was set within cells involved in mpi or
		physical boundary operations



-- benchmark:  after these tweaks, it seems that a unigrid run of 96x96x32 (starting in the
	middle of a low-resolution accretion disk run)
	on gauss (Xeon 5130  2GHz, 4000 bogompis)  that
	the code is running at 50000 zone-cycles per second


-- benchmark:
	-- on the IBM machine at SDSC  the code  (see gauss in sdsc-bench )
	-- running for 20 minutes each;
	-- using restart file from t=1840
	-- always keeping full domain:  192x192x64


Name          | cells per cpu    |  cpus decomp.  | tot. cpus | run-time (sec.) |  z-cycles/sec/cpu  |  dt (M) elapsed  |  total days / run |  total SUs / run
-----------------------------------------------------------------------------------------------------------------------------------------------
harm3d-bench0   | 32x32x32 cells |  6x6x2   cpus  |   72      |  1291           |   38758            | 3.77  M elapsed  |        39.6       |   68429
harm3d-bench1   | 16x32x64 cells |  12x6x1  cpus  |   72      |  1286           |   42196            | 4.09             |        36.4       |   62899
harm3d-bench2   | 32x16x64 cells |  6x12x1  cpus  |   72      |  1290           |   38432            | 3.74             |        39.9       |   68947

harm3d-bench3   | 16x32x32 cells |  12x6x2  cpus  |  144      |  1295           |   35868            | 7.15             |        21.0       |   72448
harm3d-bench4   | 16x16x64 cells |  12x12x1 cpus  |  144      |  1294           |   34148            | 6.79             |        22.1       |   76230
harm3d-bench7   |  8x32x64 cells |  24x 6x1 cpus  |  144      |  1294           |   34515            | 6.86             |        21.8       |   75419

harm3d-bench8   | 12x32x32 cells |  16x6x2  cpus  |  192      |  1295           |   31825            | 8.53             |        17.6       |   81007
harm3d-bench8-2 | 12x32x32 cells |  16x6x2  cpus  |  192      |             |               |              |               |

harm3d-bench5   | 16x16x32 cells |  12x12x2 cpus  |  288      |  1296           |   26542            | 10.76            |        13.9       |   96247
harm3d-bench6   |  8x16x64 cells |  24x12x1 cpus  |  288      |  1295           |   26543            | 10.76            |        13.9       |   96318

** note that the harm3d-bench8-2 run used the newer "*_fast()" mpi routines;

-- so comparing our 288 cpu run to Hawley,  we find that it takes 7.42 times the SUs to run harm3d, while
   harm3d is 0.62 as fast, and has a timestep 0.27 its size


-- a full run at half resolution (in each direction) would require ~2e6 steps
	at the best decomposition (bench3), would
	require 6x3x1  (18) cpus and would require 254 hours (10.6 days)  and  4569 SUs

	-- at the least efficient rate (bench5) that would require 6x6x1 (36) cpus and would
		require 171 hours (7.1 days)  and 6156 SUs





#############################
Thu Sep 27 10:14:34 EDT 2007
-------------------------------

-- added macro "TRACE_CALLS"  that when set makes each major routine write
	"beg" and "end" messages that are helpful when tracking down bugs
	-- added routine trace_message()  in diag.c


-- added macro RUNTIME_SECONDS that controls how long a run will last in seconds;
	-- if <=0  , the run lasts forever;


#############################
Mon Oct  1 16:15:31 EDT 2007
-------------------------------

-- adding routine "exit_status()" in harm_mpi.c that is used to make sure
	that program exits cleanly;
	-- it used to such that a process just quits without informing the
            others;
        -- now the processes query each other before an MPI procedure (within
	   the routines in harm_mpi.c or dump_hdf.c) to make sure that all
           proceeding without fatal errors;
	-- if one process has encountered a fatal error at that point then
           the rest will start exitting just as the failed process has.

-- this required changes to fail() and sticking calls of exit_status() in
   various routines;

-- adding file called "dump_radflux.c" that will house the routine "dump_radflux()"
   that is responsible for writing the radiative fluxes ;
	-- adding MAKE_RADFLUX   to decs.h
	-- adding OUT_RADFLUX    to decs.h
	-- adding support for dump_radflux() in diag();
	-- editing dump_radflux.c :
		-- starting from dump_stat.c


-- it seems that the entropy equation is needed even after the initial transient
	accretion phase (i.e. after t=1500M)  from a run on woodhen called
	harm3d-kd9-lowres-stopee


#############################
Thu Oct  4 17:31:15 EDT 2007
-------------------------------
-- finished dump_radflux()
	-- added DT_out[OUT_RADFLUX]  assignment to all init*.c files;

-- I believe I removed all possibilities in which an mpi can fail and hang the
	other processes;
	-- this required additional exit_status() calls (mainly in the "mpi_global_*"
		routines)

-- added shell-integrated cooling luminosity estimates to dump_history();

-- added feature in dump_hdf() that allows one to select which of the fundamental
	grid functions to include in the main hdf dumps;
	-- see "write_to_hdf[]"


#############################
Mon Oct  8 15:42:54 EDT 2007
-------------------------------
-- added routines:
	vel_circular_equatorial(), omega_circular_equatorial()
	for the cooling function


-- added stdout output dump for the cooling temperature profile;

-- eliminating OUTPUT_COOLING_FLUX  and replacing it with MAKE_RADFLUX
	-- so dumps and history output for the radiative flux are only
		generated when  MAKE_RADFLUX = 1


#############################
Wed Oct 17 17:06:19 EDT 2007
-------------------------------

-- found a bug in dump_history() that lead to miscalculations for H_T??d
	-- so doubt the validity of these functions
	-- this doesn't matter since these functions are redundant
		(just sum the "b" and "c" functions)


#############################
Thu Oct 25 10:23:17 EDT 2007
-------------------------------
-- found another bug in dump_history() in the way Unbound points were defined:
	should   use
		h u_t < -1      instead of    u_t < -1


-- looking at the GRMHD KDPg data, that their floor is constant with values
#define RHOMIN      (2.69e-13)    /* Floor on rest-mass density */
#define UUMIN       (1.19e-15)    /* Floor on internal energy density */


#############################
Tue Nov 27 15:11:55 EST 2007
-------------------------------

-- found out that the b-field reconnects more when there is a cutout, yet
	we want the largest dt possible;
	-- to solve this problem we are changing coordinates in x2 so that
		the grid dramatically coarsens at the poles

	-- we are calling this coordinate system COORD_DIAGONAL2
	-- we are adding many new routines to coord.c to calculate these coordinates;
	-- the discretization of the new coordinates is set to be a set of piecewise
		continuous linear functions ; they are joined by continuous representations
		of the step function so that the function is differentiable;
	-- added global variables to decs.h for COORD_DIAGONAL2
	-- added routines to coord.c



-- since this is adding more complexity to harm, i am changing harm usage
	so that grid, coordinates, metric are all determined at compile time and
	are not set by restart dumps;
	-- eliminating reads in restart_read_hdf5()
	-- adding "Macro" directory in "Header" directory in all dumps now which
		will contain the values of alll compile time macros

-- tested COORD_DIAGONAL2 routines in ./testing/coord_diagonal2
	-- added ./testing/coord_diagonal2  to cvs repository



-- I benchmarked harm3d with icc and gcc:

gcc (GCC) 3.4.6 20060404 (Red Hat 3.4.6-3)
icc version 8.0


going to t=5M using 32^3 points without output.
gcc : 149.174u 0.211s 2:29.63 99.8%   0+0k 0+0io 18pf+0w
      151.402u 0.120s 2:31.56 99.9%   0+0k 0+0io 0pf+0w

icc : 114.323u 0.196s 1:54.76 99.7%   0+0k 0+0io 12pf+0w
      115.255u 0.172s 1:55.50 99.9%   0+0k 0+0io 0pf+0w


going to t=5M using 192x192x1 points without output.
gcc: 766.856u 0.854s 12:48.52 99.8%  0+0k 0+0io 15pf+0w
icc: 544.010u 0.691s 9:05.08 99.9%   0+0k 0+0io 0pf+0w


	-- icc is still better, but not as much as I had imagined



-- the initial sets of parameters for the new theta gridding didn't work so well;
	-- two sets led to explosions at around t=2000M and they all general
		had errantly large pressures in the funnel region;
	-- noticed that they hit the floor more often than the COORD_DIAGONAL  run
	-- looked at connection, gdet, gcov, nothing strange happening near axes;
		-- they look very much like COORD_DIAGONAL metric functions
		-- compared to finite differenced connection and they compare as well
			as expected;  a little large near the axial boundaries, where
			they differ by 0.3%  (the largest error)


-- noticing that stipple/checkered pattern in pressure,rho,gamma in the funnel seems
	to be tied to when the entropy equation is used;



-- adding calculation of dx_dxp_gf[],dxp_dx_gf[],regularize_gf[],unregularize_gf[]
	arrays to the end of calc_all_geom();
	-- need to add rescaling in recon.c
	-- need to add transformations and regularity to new type of
		boundary conditions
	-- added regularize_prim() in recon.c that is used to regularize primitive
		vectors before reconstruction;
	-- added macro RESCALE_REGULARIZE  to control whether to use this regularization;


-- adding file "init.funnel.c" used to set initial data that looks like the funnel
	from a 2d simulation using KD torus initial data;


-- realized that fixup_interp_prim() and fixup_interp_v() were only being called if there
	were failures in the cells near the boundaries;
	-- fixed it so that it is now called;
	-- added global array "failure_type[]" to indicate which type of fixup interpolation to do;


#############################
Wed Dec 12 11:45:45 EST 2007
-------------------------------

-- added regularization to the reconstruction and BCs but then was getting NaNs on hyades and woodhen;
	-- found that the nans went away when I put print statements about the operations that
		causes the nans;
	-- this suggested to me that there was a problem with how they were being stored in memory
		since the print statements explicitly put the values on the stack
	-- I tried doing the run without the -ipo compiler option and the bug vanished, even without
		the print statements;

--  Benchmarked on two dual quad core intel machines (2.4GHz) :
run # | # cpus  |  grid/cpu | Npts    | nsteps | run-time (s)| ZC/sec | ZC/sec/cpu
-----------------------------------------------------------------------------------------------------
   1  | 2x4     | 16x32x32  |  1.31e5 | 1479   |     533     | 3.64e5 | 4.54e4
   2  | 1x4     | 32x32x32  |  1.31e5 | 1479   |     915     | 2.12e5 | 5.29e4
   4  | 1x8     | 16x32x32  |  1.31e5 | 1479   |     647     | 2.99e5 | 3.74e4
   5  | 2x8     |  8x32x32  |  1.31e5 | 1804   |     447     | 5.29e5 | 3.30e4
   6  | 1x4     | 16x32x32  |  6.55e4 | 1500   |     462     | 2.13e5 | 5.32e4
   7  | 1x8     | 16x32x32  |  1.31e5 | 1479   |     673     | 2.88e5 | 3.60e4
   8* | 2x8     | 16x32x32  |  2.62e5 |  126   |      53     | 6.23e5 | 3.89e4


	-- seems like spreading out the job over more nodes is the best strategy;




-- eliminated system() call in diag() to make the directories since there is a bug in
	Infiniband drivers and MPI with system()/fork() calls;


#############################
Fri Jan 18 18:02:07 EST 2008
-------------------------------

-- added to decs.h:
#define USE_COOLING_FUNCTION (1)   /* whether to use the cooling function in the source terms */


-- changing usage of cooling function so that it only uses the cooling function in
  	the source terms if the flow is bound;
	-- i have noticed that there are stability problems with cooling the funnel,
	   so I hope this cures them;



#############################
Wed Feb 27 11:08:56 EST 2008
-------------------------------

-- added the following programs in ./bin :

   mks_to_bl.c    : transforms primtive variables from MKS coordinates to BL coordinates
                     (this is to be used by the B-field rendering program)
   radfix.c       : Before Feb. 2008, harm3d had a bug in how the radiative flux was dumped;
                    thie program fixes those rad. flux dumps that overlap in time with the
	            full dumps (the other cannot be salvaged).

   radflux2.c     : This calculates the cooling function, ucon[] and the dissipation profiles
                     from the rad. flux dumps.

   stressdiss.c   : This calculates the stress profiles from the full dumps.




#############################
Mon Mar 24 12:27:41 EDT 2008
-------------------------------

-- added init.monopole.c data that is supposed to be the initial data used in
	Komissarov MNRAS 350 (2004)
	-- I have not implemented his floor conditions, so it pretty much explodes
		after a while.



-- working on a new flux_ct() routine called flux_ct_para() which will use para()
	to calculate emf's ;
	-- using 2nd-order averaging may be the reason why we see 2h waves in B2 and Bphi
		eventually;
	-- seems to work in 2d, divb is a little smaller with flux_ct_para() than
		with the old routine,
	--  trying it out on the 3d cooling run that crashed....
		-- seems to fix the oscillations seen in all functions, but also showingn new boundary
			effects.
		-- noticed that the EMFs were being calculated inconsistently between neighboring
			subdomains, since I was not using corner-centered interpolation stencils
			for the boundary EMFS
		-- fixed this and trying again

-- working on recon.c
	-- in ./testing/recon_timing  found that using run-time bounds in the recon
		loop doesn't affect runtime significantly;
	-- tweaked the recon methods to be faster
	-- removed linear_recon_vect2_[1-3]  since they were exactly the same as
		linear_recon_vect_[1-3] and were not being used;
	-- added linear_recon_vect() and para_vect() that use run-time bounds
	-- changing calls to linear_recon_vect_[1-3] to linear_recon_vect()
	-- changing calls to para_vect_[1-3] to para_vect()


-- moved macro MAX_NTOT decs.h  to represent the biggest of N1TOT,N2TOT,N3TOT
	(used in flux_ct_para_fast());

-- moved definition of RECON_USE_PPM to decs.h since it is now used in step_ch.c
	to select flux_ct scheme;

-- timing a 80x80x32 run of init.kd.c a=0.9 init data:
	flux_ct_para_fast():
	198.095u 1.037s 3:23.61 97.7%   0+0k 0+0io 0pf+0w

	flux_ct_para():
	209.106u 1.158s 3:35.03 97.7%   0+0k 0+0io 0pf+0w


	flux_ct():
	188.241u 1.207s 3:15.96 96.6%   0+0k 0+0io 0pf+0w


-- timing a 16x32x32 run of init.kd.c a=0.9 init data:
	flux_ct_para_fast():
	17.772u 0.134s 0:18.70 95.7%    0+0k 0+0io 0pf+0w

	flux_ct_para():
	18.498u 0.135s 0:19.44 95.7%    0+0k 0+0io 0pf+0w

	flux_ct():
	17.209u 0.119s 0:18.12 95.5%    0+0k 0+0io 0pf+0w



-- in order to use parabolic interpolation in the emf interpolation
	uniformly over the whole domain (and not drop to 2nd-order interp.
	on MPI boundaries) we need to perform para() centered
	at i-1, requiring N1S-3,N1S-2,N1S-1 fluxes which means that
	these means that we need

-- added macro USE_FLUX_CT_PARA  in decs.h  to signal when to use parabolic
	interpolation for the EMFs

-- changed the loop bounds in numerical_flux() in step_ch.c  to use USE_FLUX_CT_PARA
	-- needed to extend the flux calculation to the full extent orthogonal to the
		direction of the flux (e.g. if we are calculation F[0], then we go over
		N2TOT and N3TOT cells in the x2,x3 directions respectively so that
		those flux terms can be used in the EMF reconstruction;
	-- this is slight over kill since only the mag. field fluxes are needed
		there;  I don't see this being a significant slow down, however;

-- changed bounds used in reconstruction_fast() so that it is consistent with the
	USE_FLUX_CT_PARA condition;



-- changed use of USE_MPI_IO in dump_hdf.c so that we always read in one rdump_start.h5
	file (i.e. use MPI-IO) when doing a MPI run;
	-- This is because I want to test out NOT using MPI-IO in a MPI run to see
		if it's causing sig. slowdown;
	-- Also, we don't have a facility to read in multiple rdump_start.p0*.h5 files ;

-- added macro USE_MPI_IO_HIST to have an independent control of MPI-IO in making history files;




-- noticed that dx0 changed all of a sudden after using new flux_ct_para_fast2() method
	-- I think this is because we are reconstructing within the ghost zones and
		so the max_char_speed[dim] calculation is being done within the
		cutout, thus reducing dx0;
	--

-- adding new init.*.c file  called  init.vertfieldkd.c   that is initial data that Kris is
   interested in for studying disks with vertical field threading the disk initially;
    Field is initialized the standard vert. field way, but the disk is pushed out to larger radius;


#############################
Fri Apr 18 11:14:51 EDT 2008
-------------------------------
-- eliminated use of "mkdir()"  command, so we'll be dumping to run directory for
	all files;  this is because I suspect making a new directory and writing to it
        on some FS's  is unstable (e.g. this seems to be the case with pvfs2 on hyades,
	and may be the problem with pvfs2/infiniband on woodhen)

-- added programs  "hdf5_merge" and "hdf5_decomp" in ./bin ;
    -- hdf5_decomp  takes a full-domain hdf5 snapshot file, creates a new subgrid domain
         decomposition (e.g. the ones we use to parallelize harm), and distributes
	 the data from the full grid to these subgrids into new files.

    -- hdf5_merge  performs the inverse of hdf5_decomp;

    -- we're finding that using collective IO on abe/t3 (that use Infiniband with lustre FS)
	is quite slow;  distribute IO seems to perform better for our application;


-- trying to optimize MPI stuff:
	-- made exit_status() return immediately;
	-- commented out use of MPI_Barrier everywhere
        -- created mpi_global_imax2()
	-- vectorized calls mpi_global_imax(&boundary_mpi_pflag); mpi_global_imax(&boundary_phys_pflag);
	   to   mpi_global_imax2(&boundary_mpi_pflag) ,&boundary_phys_pflag);
        -- eliminated extraneous mpi_global_sum(M_tot) call in step_ch.c (done in dump_ener() already)



#############################
Sun Apr 27 19:41:41 EDT 2008
-------------------------------

-- after optimizations and not using MPI-IO we're getting on ABE:
	36243  zone-cycles/sec/cpu    for a 144-cpu run

    where as before it was  21330 zone-cycles/sec/cpu    for a 144-cpu run


-- running with 16x32x32 domains , harm3d uses   about 162MB per domain



-- let's say that you finish a standard run to totalsize[0] (e.g. 10000M)
       and you want continue past this time to 2*totalsize[0] (e.g. 20000M)
	but you want to use a different history file;
	 -- you need to do the following :
            1) use the last (or maybe the 2nd to last) full dump as the restart file
            2) Reset dx0  to its finite value in the restart file (dx[0] is set to at the last timestep) 
	    3) change N_hist_dump and N_out0  in the restart dump files  to zero
            4) add  "using_restart = 0;"  at the beginning of dump_history() in dump_history.c
            5) comment out the assignment of N_hist_dump_tot in setup_history2() in dump_history.c
            6) change the filename of the history file to "*.history.2.h5"  in setup_history2() in dump_history.c 
	    7) in init.c,  increase the value of GridLength[0]


         -- after the first run using this new history file dump_history.c file, you will need to 
            1) delete the "using_restart = 0;"  line in dump_history()  of dump_history.c 






-- added
inverse_transform_rank1con()
inverse_transform_rank1cov()
inverse_transform_rank2con()
inverse_transform_rank2cov()
that transform various ranked tensors from xp (numerical coordinates)
to x (physical coordinates)


#############################
Mon Jul  7 10:36:11 EDT 2008
-------------------------------

--  SU usage estimate on ABE:
	-- for the typical  192x192x64 run using 144 cpus we go about
           1232M  or 345961 timesteps in  48 hours walltime  with  6888 SUs

	-->  avg.  timestep dt = 0.0036
	-->  26 M/hr
        -->  2 steps / sec.
	--> 4.7e6 zone-cycles/sec.
        --> 32768 zone-cycles/sec./cpu
	--> 0.179 M/SU


#############################
Tue Jul 29 14:39:46 EDT 2008
-------------------------------

-- fixed bug when using USE_FLUX_CT_PARA=1 with any N1,N2,N3 = 1 .  The bug was in
	recon.c in that it did not set the p_L,p_R to all the necessary ghost zones.

-- working on making harm3d able to use 3d metrics....
	-- adding macro  METRIC_DIM  which is set to the number of spatial dimensions
		on which the metric is dependent.
		-- this will be used to determine within what loops we need to call get_geometry()


-- added ./testing/geom_glob  to compare timing between using global arrays of geometry functions
  	or global array of geometry structures
	-- the latter is about 60 times faster
	-- this is needed to speed up geometry retrieval per 3d cell;
	-- right now we get by the slowdown by only retrieving it for every



	todo:
		-- make global array of of_geom structures;
		-- make conn[] 3d-able
		--


#############################
Mon Oct 13 15:17:56 EDT 2008
-------------------------------

-- going to be starting a large number of runs, so I want to make sure that I'm
   decomposing the grid in an efficient way.  I did a simple profile of harm3d
   for a characteristic thin disk run.  Here are the results:



run # |  name in unitree archive
----------------------------------
   1  |  harm3d-kd0-thin-r14-bench1_out.tar
   2  |  harm3d-kd0-thin-r14-bench2_out.tar
   3  |  harm3d-kd0-thin-r14-bench3_out.tar
   4  |  harm3d-kd0-thin-r14-bench4_out.tar


-- each run started from a checkpoint at t ~= 1300M  for a run that enters
    a fully turbulent state at around t=4000-5000M (so this isn't an ideal
    profile, but it's at least not from initial data).

-- all runs use 192x192x64 (Nr x Nth x Nph)  global grids, a=0.0,  H/R=0.05


run # | # cpus  |  grid/cpu | Npts     | nsteps | run-time (s)| ZC/sec | ZC/sec/cpu | SUs
-----------------------------------------------------------------------------------------------------
   1  | 18x8    | 16x32x32  |  2.36e6  | 3309   |    1756     | 4.45e6 | 3.1e4      |  73.24
   2  | 32x8    | 12x24x32  |  2.36e6  | 4846   |    1678     | 6.82e6 | 2.7e4      | 127.5
   3  | 36x8    | 16x16x32  |  2.36e6  | 5109   |    1668     | 7.22e6 | 2.5e4      | 145.2
   4  | 64x8    | 12x12x32  |  2.36e6  | 7994   |    1641     | 1.15e7 | 2.2e4      | 256


dt = 4.5e-4

-- conclusions:
	 1) zone-cycles/sec. =   1.79260e+06  +   19014.6 x Ncpu


#############################
Wed May  6 14:51:54 EDT 2009
-------------------------------

-- adding a new coordinate system which we will call

     COORD_TYPE_CHOICE = COORD_DIAGONAL3

    It is a diagonal coordinate system that follows
    the one used by Shafee et al. 2008.  Theta is now a
    polynomial of the form :

    Theta(x) = Pi/2 * ( d*(2*x-1) + (1-d-t)*(2*x-1)^n + 1 )

    where Shafee et al 2008 used a slightly different form that
    neglected the possibility of a cutout region and used the following
    values for the parameters:

    d = 0.15
    t = 0
    n = 7


    Note that

    Min(dTheta) = Pi * d

    so "d" controls the smallest polar cell extent.

    As one increases "n", more points are placed near the equator and the largest
    dTheta becomes larger.

    The parameter "t" is proportional to the cutout size:

      t = Theta_cutout / (Pi/2)  =  2*Theta_cutout / Pi

    In the code we label

      t = diag3_cutout
      d = h_slope
      n = diag3_exponent

	diag3_factor is a temporary variable ; 

    The reasons for using a polynomial is that it is simple, not transcendental,
    is more adjustable than a sin/cos system.  Apparently the cusp that this
    coordinate system has at the axis is not a problem, maybe because there
    already is a coordinate singularity there.


--- while adding the above coordinates, I pondered whether I should pow() or not to 
    take the exponents and profiled pow() versus using a macro like MY_POW().  
    To my surprise I found that---when using the intel compilers---pow(x,n) (where
    n is an integer) is always faster than x*x*x...  for   2 <= n  <= 11 (and maybe 
    even higher as I only tested these values of n.  So, use pow() whenever you need to!!




#############################
Tue May 19 14:56:24 EDT 2009
-------------------------------

-- profiled harm3d on Ranger (TACC) and found that the slowest part was the parabolic
	interpolation in either the reconstruction step or the flux_ct step;
	-- I've optimized this as much as I possible can, so I'm moving on

-- performed a scaling experiment on Ranger (TACC) using the  192x192x64 run given in 
/data1/gauss/harm3d/archive/2d/cooling/a=0.0/h_o_r=0.05/192x192_08222008
  (h_slope=0.45,  H/R = 0.05,  Rout=120, full theta, quarter phi (0,pi/2))

    -- Kept the total number of cells constant:

RUNID | Ntot1xNtot2xNtot3 |  N1xN2xN3    | Nc1xNc2xNc3  | # cells   |  Nt     | # cpus  |   zc/sec    |  zc/sec/cpu |  # M / hr   |  SUs / M  
-----------------------------------------------------------------------------------------------------------------------------------------------
    0 |   192 x 192 x 64  | 48 x 96 x 32 |  4 x  2 x  2 |  2359296  |  1547   |    16   |  1.01e+06   |  6.33e+04   |  8.69e+00   |  1.84e+00   
    1 |   192 x 192 x 64  | 48 x 48 x 64 |  4 x  4 x  1 |  2359296  |  1601   |    16   |  1.05e+06   |  6.55e+04   |  9.00e+00   |  1.78e+00   
    2 |   192 x 192 x 64  | 48 x 48 x 32 |  4 x  4 x  2 |  2359296  |  2925   |    32   |  1.92e+06   |  5.99e+04   |  1.66e+01   |  1.92e+00   
    3 |   192 x 192 x 64  | 32 x 32 x 16 |  6 x  6 x  4 |  2359296  |  9937   |   144   |  6.51e+06   |  4.52e+04   |  5.76e+01   |  2.50e+00   
    4 |   192 x 192 x 64  | 24 x 48 x 64 |  8 x  4 x  1 |  2359296  |  2910   |    32   |  1.91e+06   |  5.96e+04   |  1.65e+01   |  1.93e+00   
    5 |   192 x 192 x 64  | 24 x 48 x 32 |  8 x  4 x  2 |  2359296  |  5353   |    64   |  3.51e+06   |  5.48e+04   |  3.07e+01   |  2.08e+00   
    6 |   192 x 192 x 64  | 48 x 24 x 64 |  4 x  8 x  1 |  2359296  |  2918   |    32   |  1.91e+06   |  5.98e+04   |  1.66e+01   |  1.93e+00   
    7 |   192 x 192 x 64  | 48 x 24 x 32 |  4 x  8 x  2 |  2359296  |  5324   |    64   |  3.49e+06   |  5.45e+04   |  3.05e+01   |  2.10e+00   
    8 |   192 x 192 x 64  | 24 x 24 x 64 |  8 x  8 x  1 |  2359296  |  5351   |    64   |  3.51e+06   |  5.48e+04   |  3.07e+01   |  2.09e+00   
    9 |   192 x 192 x 64  | 24 x 24 x 32 |  8 x  8 x  2 |  2359296  |  9668   |   128   |  6.34e+06   |  4.95e+04   |  5.60e+01   |  2.29e+00   
   10 |   192 x 192 x 64  | 24 x 24 x 16 |  8 x  8 x  4 |  2359296  | 16498   |   256   |  1.08e+07   |  4.22e+04   |  9.56e+01   |  2.68e+00   
   11 |   192 x 192 x 64  | 16 x 32 x 32 | 12 x  6 x  2 |  2359296  | 10505   |   144   |  6.88e+06   |  4.78e+04   |  6.09e+01   |  2.36e+00   
   12 |   192 x 192 x 64  | 32 x 16 x 32 |  6 x 12 x  2 |  2359296  | 10332   |   144   |  6.77e+06   |  4.70e+04   |  5.99e+01   |  2.41e+00   
   13 |   192 x 192 x 64  | 16 x 24 x 64 | 12 x  8 x  1 |  2359296  |  7393   |    96   |  4.84e+06   |  5.05e+04   |  4.26e+01   |  2.25e+00   
   14 |   192 x 192 x 64  | 16 x 24 x 32 | 12 x  8 x  2 |  2359296  | 13512   |   192   |  8.86e+06   |  4.61e+04   |  7.82e+01   |  2.45e+00   
   15 |   192 x 192 x 64  | 24 x 16 x 64 |  8 x 12 x  1 |  2359296  |  7456   |    96   |  4.89e+06   |  5.09e+04   |  4.29e+01   |  2.24e+00   
   16 |   192 x 192 x 64  | 24 x 16 x 32 |  8 x 12 x  2 |  2359296  | 13243   |   192   |  8.68e+06   |  4.52e+04   |  7.67e+01   |  2.50e+00   
   17 |   192 x 192 x 64  | 16 x 16 x 64 | 12 x 12 x  1 |  2359296  | 10310   |   144   |  6.75e+06   |  4.69e+04   |  5.97e+01   |  2.41e+00   
   18 |   192 x 192 x 64  | 16 x 16 x 32 | 12 x 12 x  2 |  2359296  | 17850   |   288   |  1.17e+07   |  4.06e+04   |  1.03e+02   |  2.78e+00   
   19 |   192 x 192 x 64  | 12 x 16 x 64 | 16 x 12 x  1 |  2359296  | 14143   |   192   |  9.27e+06   |  4.83e+04   |  8.19e+01   |  2.34e+00   
   20 |   192 x 192 x 64  | 12 x 16 x 32 | 16 x 12 x  2 |  2359296  | 25347   |   384   |  1.66e+07   |  4.33e+04   |  1.47e+02   |  2.61e+00   
   21 |   192 x 192 x 64  | 16 x 12 x 64 | 12 x 16 x  1 |  2359296  | 12553   |   192   |  8.23e+06   |  4.28e+04   |  7.27e+01   |  2.64e+00   
   22 |   192 x 192 x 64  | 16 x 12 x 32 | 12 x 16 x  2 |  2359296  | 21249   |   384   |  1.39e+07   |  3.63e+04   |  1.23e+02   |  3.12e+00   
   23 |   192 x 192 x 64  | 12 x 12 x 64 | 16 x 16 x  1 |  2359296  | 16894   |   256   |  1.11e+07   |  4.32e+04   |  9.79e+01   |  2.62e+00   
   24 |   192 x 192 x 64  | 12 x 12 x 32 | 16 x 16 x  2 |  2359296  | 27743   |   512   |  1.82e+07   |  3.55e+04   |  1.61e+02   |  3.19e+00   
   25 |   192 x 192 x 64  | 12 x 12 x 16 | 16 x 16 x  4 |  2359296  | 25009   |  1024   |  1.64e+07   |  1.60e+04   |  1.45e+02   |  7.06e+00   


	-- the basic conclusions are the following: 

        ---- its best to use  N3 >= 32

	----  Z =  zc/sec/cpu  ~=  10^5 (Ncpu)^(-0.17)      

        ----  S =  SUs/M  ~=  1.9 + 2e-3 * Ncpu    ==   (Cost per unit of simulation time)     (for best geometries)  (down to 12x16x32 domains)

	----  note that you cannot recover Z from S (or vice versa) because there is a systematic 
		offset of runtime with Ncpu; that is, it takes a finite amount of time to 
		start a run with  "Ncpu = 0" .  






#############################
Mon May 25 16:59:33 EDT 2009
-------------------------------

-- added ./testing/hydro-torus that  I wrote to probe different disk thicknesses using 
    the "Keplerian disk" construction;   
	-- the disk thickness H/R is defined in Noble et al. 2007.    and corresponds 
		to this version of harm3d's "h_o_r" parameter. 

	-- we found the following set of initial data and their H/R (at r=rpmax) :

  rin       rpmax         q     l_ang_in    H/R     
-------------------------------------------------------
20.000000  35.000000  1.680000  5.285041  0.234843     
20.000000  35.000000  1.680000  5.285041  0.234843     
20.000000  35.000000  1.680000  5.285041  0.234843     
20.000000  35.000000  1.700000  5.342042  0.276415     
20.000000  35.000000  1.650000  5.200684  0.171866     
20.000000  35.000000  1.620000  5.117671  0.112366     
20.000000  35.000000  1.610000  5.090294  0.093970     
20.000000  35.000000  1.600000  5.063066  0.077122     
20.000000  35.000000  1.580000  5.009044  0.048165     
20.000000  35.000000  1.590000  5.035982  0.061883     
15.300000  25.300000  1.680000  4.692882  0.141188     
20.000000  35.000000  1.585000  5.022496  0.054881     
20.000000  35.000000  1.584000  5.019801  0.053515     
20.000000  35.000000  1.582000  5.014421  0.050831     
20.000000  35.000000  1.581000  5.011732  0.049502     
20.000000  35.000000  1.581400  5.012807  0.050022     
20.000000  35.000000  1.581400  5.012807  0.050022     
16.000000  26.000000  1.680000  4.769716  0.134668     
17.000000  27.000000  1.680000  4.877106  0.123430     
17.000000  27.000000  1.680000  4.877107  0.123430     
18.000000  28.000000  1.680000  4.981902  0.113999     
19.000000  29.000000  1.680000  5.084289  0.105480     
20.000000  30.000000  1.680000  5.184440  0.102511     
20.000000  31.000000  1.680000  5.205054  0.124468     
20.000000  32.000000  1.680000  5.225430  0.150083     



#############################
Wed Jun 10 14:36:36 EDT 2009
-------------------------------

-- fixed a few problems with the coordinates at the axis ;
	-- need to enforce xp2's periodicity for metric/coordinate 
	    functions to be periodic there

        -- 

-- added scale height calculation to init_prim() (in init.c only) ; 
	-- the scale height is printed to stdout at initialization time;


#############################
Tue Jul  7 16:46:06 EDT 2009
-------------------------------

-- strong scaling profile I made a couple of months ago on ranger: 
	-- I set the number of cells per cpu constant, and then varied the 
		number of cpus per dimension:

RUNID | Ntot1xNtot2xNtot3 |  N1xN2xN3    | Nc1xNc2xNc3  | # cells   |  Nt     | # cpus  |   zc/sec    |  zc/sec/cpu |  # M / hr   |  SUs / M  
-----------------------------------------------------------------------------------------------------------------------------------------------
  100 |    96 x 128 x 64  | 12 x 16 x 32 |  8 x  8 x  2 |   786432  | 26215   |   128   |  5.73e+06   |  4.47e+04   |  1.98e+02   |  6.47e-01   
  101 |    96 x 256 x 64  | 12 x 16 x 32 |  8 x 16 x  2 |  1572864  | 25405   |   256   |  1.11e+07   |  4.34e+04   |  9.82e+01   |  2.61e+00   
  102 |    96 x 384 x 64  | 12 x 16 x 32 |  8 x 24 x  2 |  2359296  | 25934   |   384   |  1.70e+07   |  4.43e+04   |  6.69e+01   |  5.74e+00   
  103 |    96 x 512 x 64  | 12 x 16 x 32 |  8 x 32 x  2 |  3145728  | 24330   |   512   |  2.13e+07   |  4.15e+04   |  4.70e+01   |  1.09e+01   
  104 |    96 x 640 x 64  | 12 x 16 x 32 |  8 x 40 x  2 |  3932160  | 14831   |   640   |  1.62e+07   |  2.53e+04   |  2.28e+01   |  2.81e+01   
  105 |    96 x 688 x 64  | 12 x 16 x 32 |  8 x 43 x  2 |  4227072  | 19833   |   688   |  2.33e+07   |  3.38e+04   |  2.84e+01   |  2.42e+01   
  106 |   192 x 128 x 64  | 12 x 16 x 32 | 16 x  8 x  2 |  1572864  | 24123   |   256   |  1.05e+07   |  4.12e+04   |  1.97e+02   |  1.30e+00   
  107 |   192 x 256 x 64  | 12 x 16 x 32 | 16 x 16 x  2 |  3145728  | 22541   |   512   |  1.97e+07   |  3.85e+04   |  9.91e+01   |  5.17e+00   
  108 |   192 x 384 x 64  | 12 x 16 x 32 | 16 x 24 x  2 |  4718592  | 24523   |   768   |  3.21e+07   |  4.19e+04   |  7.23e+01   |  1.06e+01   
  109 |   192 x 512 x 64  | 12 x 16 x 32 | 16 x 32 x  2 |  6291456  | 14547   |  1024   |  2.53e+07   |  2.47e+04   |  3.18e+01   |  3.22e+01   
  110 |   192 x 640 x 64  | 12 x 16 x 32 | 16 x 40 x  2 |  7864320  | 17859   |  1280   |  3.90e+07   |  3.05e+04   |  3.15e+01   |  4.07e+01   
  111 |   192 x 688 x 64  | 12 x 16 x 32 | 16 x 43 x  2 |  8454144  | 15580   |  1376   |  3.66e+07   |  2.66e+04   |  2.55e+01   |  5.40e+01   
  112 |   288 x 128 x 64  | 12 x 16 x 32 | 24 x  8 x  2 |  2359296  | 20850   |   384   |  1.36e+07   |  3.55e+04   |  1.69e+02   |  2.27e+00   
  113 |   288 x 256 x 64  | 12 x 16 x 32 | 24 x 16 x  2 |  4718592  | 22850   |   768   |  2.99e+07   |  3.90e+04   |  1.03e+02   |  7.45e+00   
  114 |   288 x 384 x 64  | 12 x 16 x 32 | 24 x 24 x  2 |  7077888  | 13726   |  1152   |  2.70e+07   |  2.34e+04   |  4.17e+01   |  2.76e+01   
  115 |   288 x 512 x 64  | 12 x 16 x 32 | 24 x 32 x  2 |  9437184  | 15722   |  1536   |  4.14e+07   |  2.69e+04   |  3.61e+01   |  4.25e+01   
  117 |   288 x 688 x 64  | 12 x 16 x 32 | 24 x 43 x  2 | 12681216  | 18125   |  2064   |  6.38e+07   |  3.09e+04   |  3.09e+01   |  6.67e+01   
  118 |   384 x 128 x 64  | 12 x 16 x 32 | 32 x  8 x  2 |  3145728  | 20260   |   512   |  1.77e+07   |  3.46e+04   |  1.57e+02   |  3.26e+00   
  119 |   384 x 256 x 64  | 12 x 16 x 32 | 32 x 16 x  2 |  6291456  | 13198   |  1024   |  2.31e+07   |  2.25e+04   |  5.94e+01   |  1.72e+01   
  120 |   384 x 384 x 64  | 12 x 16 x 32 | 32 x 24 x  2 |  9437184  | 14668   |  1536   |  3.84e+07   |  2.50e+04   |  4.51e+01   |  3.40e+01   
  121 |   384 x 512 x 64  | 12 x 16 x 32 | 32 x 32 x  2 | 12582912  | 18097   |  2048   |  6.33e+07   |  3.09e+04   |  4.22e+01   |  4.86e+01   
  122 |   384 x 640 x 64  | 12 x 16 x 32 | 32 x 40 x  2 | 15728640  | 18930   |  2560   |  8.27e+07   |  3.23e+04   |  3.54e+01   |  7.24e+01   
  123 |   384 x 688 x 64  | 12 x 16 x 32 | 32 x 43 x  2 | 16908288  | 17834   |  2752   |  8.39e+07   |  3.05e+04   |  3.10e+01   |  8.87e+01   
  124 |   480 x 128 x 64  | 12 x 16 x 32 | 40 x  8 x  2 |  3932160  | 19973   |   640   |  2.18e+07   |  3.41e+04   |  1.45e+02   |  4.41e+00   
  125 |   480 x 256 x 64  | 12 x 16 x 32 | 40 x 16 x  2 |  7864320  | 13722   |  1280   |  3.00e+07   |  2.34e+04   |  6.09e+01   |  2.10e+01   
  126 |   480 x 384 x 64  | 12 x 16 x 32 | 40 x 24 x  2 | 11796480  | 11222   |  1920   |  3.68e+07   |  1.92e+04   |  3.44e+01   |  5.58e+01   
  127 |   480 x 512 x 64  | 12 x 16 x 32 | 40 x 32 x  2 | 15728640  | 18597   |  2560   |  8.13e+07   |  3.17e+04   |  4.36e+01   |  5.88e+01   
  128 |   480 x 640 x 64  | 12 x 16 x 32 | 40 x 40 x  2 | 19660800  | 17513   |  3200   |  9.56e+07   |  2.99e+04   |  3.30e+01   |  9.71e+01   
  129 |   480 x 688 x 64  | 12 x 16 x 32 | 40 x 43 x  2 | 21135360  | 15307   |  3440   |  8.98e+07   |  2.61e+04   |  2.68e+01   |  1.28e+02   
  130 |   516 x 128 x 64  | 12 x 16 x 32 | 43 x  8 x  2 |  4227072  | 20125   |   688   |  2.36e+07   |  3.43e+04   |  1.43e+02   |  4.82e+00   
  131 |   516 x 256 x 64  | 12 x 16 x 32 | 43 x 16 x  2 |  8454144  | 10705   |  1376   |  2.50e+07   |  1.82e+04   |  4.68e+01   |  2.94e+01   
  132 |   516 x 384 x 64  | 12 x 16 x 32 | 43 x 24 x  2 | 12681216  | 18510   |  2064   |  6.52e+07   |  3.16e+04   |  5.70e+01   |  3.62e+01   
  133 |   516 x 512 x 64  | 12 x 16 x 32 | 43 x 32 x  2 | 16908288  | 17211   |  2752   |  8.08e+07   |  2.94e+04   |  4.03e+01   |  6.83e+01   
  134 |   516 x 640 x 64  | 12 x 16 x 32 | 43 x 40 x  2 | 21135360  | 17464   |  3440   |  1.03e+08   |  2.98e+04   |  3.29e+01   |  1.05e+02   
  300 |   768 x 256 x 64  | 12 x 16 x 32 | 64 x 16 x  2 | 12582912  | 11000   |  2048   |  3.85e+07   |  1.88e+04   |  4.48e+01   |  4.57e+01   
  301 |  1536 x 128 x 64  | 12 x 16 x 32 |128 x  8 x  2 | 12582912  |  7839   |  2048   |  2.73e+07   |  1.33e+04   |  2.79e+01   |  7.33e+01   
  302 |    96 x 2048 x 64  | 12 x 16 x 32 |  8 x128 x  2 | 12582912  | 22000   |  2048   |  7.69e+07   |  3.75e+04   |  1.05e+01   |  1.95e+02   
  303 |   192 x 1024 x 64  | 12 x 16 x 32 | 16 x 64 x  2 | 12582912  | 20240   |  2048   |  7.07e+07   |  3.45e+04   |  2.22e+01   |  9.21e+01   


-- these results seem to suggest that the number of cpus matters very little on the zone-cycles/sec/cpu
	we have  
		max(zc/s/cpu) = 44700 
		min(zc/s/cpu) = 13300 

       -- the only trend is that it seems best to avoid  Ntot1/Ntot2  >=~ 10


#############################
Thu Jul 30 23:43:54 EDT 2009
-------------------------------

-- changed COORD_DIAGONAL3   a little so that  x2  is  reflective at the the edges of xp2;
	-- in othe words,   x2[0]=x2[2*ng-1], x2[1]=x2[2*ng-2], x2[2]=xp2[2*ng-3], ... 
                                       5                 4                   3

	-- this then means that you should always use  BC_SPHERICAL_OUTFLOW3  for this setup (i think)

	-- still remember to use SMALL as the cutout to eliminate infinity blowup at the axis



#############################
Tue Sep 15 15:45:09 EDT 2009
-------------------------------

-- added makefile options for the newhorizons cluster at RIT (my new home)
-- added  BC_SPHERICAL_BONDI   for BC_TYPE_CHOICE 
	 in order to prevent boundary conditions to be set for Bondi initial data so that 
	the ghost cells are kept constant to the Bondi solution for all time
	-- also this eliminates use of the fix_flux() routine that ruins behavior at rmax



#############################
Fri Sep 18 12:09:47 EDT 2009   (really written : FRI Jun 12 14:36:36 EDT 2009) 
-------------------------------

-- working again at adding 3d geometry into harm3d, changing quite 
   a bit about how the geometry is handled.


-- working from code that I wrote using a version of harm3d circa July 30 2008
-- harm3d has not changed much, so I made the updates of harm3d to that "3d geometry"
   version and will commit this new version into harm3d's repository. 
   -- the changes should not hinder the performance of harm3d in any way 
      so the changes should be cosmetic; 
      -- in fact, the ./testing/geom_glob test suggests that our new "get_geometry()"
      	 method should be faster.



-- working on making harm3d able to use 3d metrics....
	-- adding macro  METRIC_DIM  which is set to the number of spatial dimensions
		on which the metric is dependent. 
		-- this will be used to determine within what loops we need to call get_geometry()

-- added N_GEOM and N_CONN  macro definitions to represent, respectively, the number of 
   unique locations where the metric is stored, and where the connection are stored. 

-- changed GDUMP_LOOP to take into account METRIC_DIM 

-- making gdump() and gdump_hdf() dependent on METRIC_DIM
   -- now going to dump x3 and xp3 coordinates;
   -- added support for 3d geometry dumps


-- changed source() routine in phys.c  to  use new definition conn[], taking into account 
   different metric choices.  


-- added ./testing/geom_glob  to compare timing between using global arrays of geometry functions 
  	or global array of geometry structures
	-- the latter seems to be about 1.5 to 2 times faster. 
	-- this is needed to speed up geometry retrieval per 3d cell;
	-- right now we don't see the slowdown because we do not retrieve per azimuthal cell;


-- added GEOM_LOOP that loops over all cells and positions where the geom structures are defined;

-- updated calc_all_geom() to include new changes to geometry data structures;

-- added README_user  as a file to contain notes to users on running harm3d.

-- adding routine advance_geometry()  that reads in the next temporal substep's geometry 
   information 

-- added marcro  DYNAMIC_SPACETIME  that is set if the metric changes in time
-- added macro  N0_GEOM which is set to the number of time levels the geometry is saved on;
    -- for a dynamic spacetime, we need at a minimum 2 time levels for a 2nd-order time scheme;


-- changed get_geometry() to macro called get_geometry()

-- in order to evaluate how many time slices we need the geometry data, let us make the following
	definitions: 

		tnew  = latest time that we currently know the conserved variables 
		tn   = starting time of the time integration cycle 
		tnp1 = final time of the time integration cycle,   tnp1 = tn + dt(tn)
		tnh  = tn + 0.5*dt(tn) 
		dt(tn) = timestep calculated at t=tn (it's always calculated at start of time integration cycle)
		
		
-- trying to figure out the dependencies each routine has on the metric's time:
	-- looking at phys.c .... : 	
		-- recover_primitives()  needs geom at  tnew and tn
				because of dependency on fixup_entropy_eq() which needs both
		-- added geom_old to both  recover_primitives() fixup_entropy_eq()
		-- step_ch(): 
			-- advance(): 
				-- at n_substep=0  needs geom(tn) and geom(tnh)
				-- at n_substep=1  needs geom(tnh) and geom(tnp1) and geom(tn)
				-- reconstruct_fast(): n_substep=0 -> geom(tn), n_substep=1 -> geom(tnh)
				-- numerical_flux():   n_substep=0 -> geom(tn), n_substep=1 -> geom(tnh)
				-- fix_flux():   (none)
				-- flux_ct_para_fast2(): (none)
		
			-- fixup_global() :   n_substep=0 -> geom(tnh), n_substep=1 -> geom(tnp1) 
			-- fixup_interp_v():  n_substep=0 -> geom(tnh), n_substep=1 -> geom(tnp1) 
			-- fixup_interp_prim(): n_substep=0 -> geom(tnh), n_substep=1 -> geom(tnp1) 
			-- bounds():  n_substep=0 -> geom(tnh), n_substep=1 -> geom(tnp1) 
			-- current_calc(): n_substep=0 only  -> geom(tn) and geom(tnh)
			-- 
	
			
			

-- added "ncurr" to indicate the current time position in the stepping process, or the 
	time position that a routine is to assume for instance when accessing time-dependent 
		geometry data

-- added : 
	extern int  n_beg, n_mid, n_end;  /* variables to store the beginning, middle (halfstep) and final time index for metric functions */    

	-- depending where we are in the update process, "ncurr" will be assigned one of these values;

	-- n_beg, n_mid and n_end are place holders for the geom_arr[] time indexes so that we can 
		read in the new geometry data without destroying the present data; 


-- finished advance_geometry();
-- added call to set_general_conn() in advance() in order to advance the conn to the mid-step;

-- added init_general_metric() routine to metric.c   in order to handle initialization of a general 
	metric or a dynamic metric  (i.e. either 
          METRIC_TYPE_CHOICE == METRIC_GENERAL_STATIC or 
          METRIC_TYPE_CHOICE == METRIC_GENERAL_DYNAMIC 



-- removing commented-out current_calc2() routine (I don't want to support routines we don't use anymore);


-- added routine macro_warnings() called by main() that reports possible problems with compile-time macro definitions;

DONE -- add time index to all calls of get_geometry()

DONE -- eliminate global arrays: 
		  DONE --  gcov[]
		  DONE --  gcon[]
		  DONE --  gdet[]
		  DONE     -- diag.c
		  DONE --  gdet_inv[]
		  DONE --  g_alpha[]
		  DONE --  g_beta[]
		  DONE --  g_ncon[]


      DONE -- make get_geometry() a macro;

      -- make sure get_geometry is called correctly each time;
      	 DONE -- main.c 
	 DONE -- bounds.c:
	 DONE -- coord.c:
	 DONE -- decs.h:
	 DONE -- diag.c:
	 DONE -- dump_hdf.c:
	 DONE -- dump_history.c:
	 DONE -- dump_stat.c:
	 DONE -- fixup.c:
	 DONE -- harm_mpi.c:
	 (NOTE THAT ALL INITIAL DATA IS GAUGE DEPENDENT)
	 DONE -- init.blob.c:
	 DONE -- init.c:
	 DONE -- init.kd.c:
	 DONE -- init.monopole.c:
	 DONE -- init.ring.c:
	 DONE -- init.torus.c:
	 DONE -- init.vertfieldkd.c:
	 DONE -- init.alf1.c 
	 DONE -- init.axistest1.c 
	 DONE -- init.axistest2.c 
	 DONE -- init.balsara1.c 
	 DONE -- init.balsara1y.c 
	 DONE -- init.balsara1z.c 
	 DONE -- init.balsara2.c 
	 DONE -- init.balsara3.c 
	 ODNE -- init.balsara4.c 
	 DONE -- init.balsara5.c 
	 DONE -- init.bondi2.c  (kinda need to transform to our new metric)
	 DONE -- init.bondi.c (kinda need to transform to our new metric)
	 DONE -- init.cylexp.c 
	 DONE -- init.cylexpxz.c 
	 DONE -- init.cylexpyz.c 
	 DONE -- init.funnel.c 
	 DONE -- init.gammie.c 
	 DONE -- init.gralf.c 
	 DONE -- init.kh.c 
	 DONE -- init.kom1.c 
	 DONE -- init.kom2.c 
	 DONE -- init.kom3.c 
	 DONE -- init.kom4.c 
	 DONE -- init.kom5.c 
	 DONE -- init.kom6.c 
	 DONE -- init.kom7.c 
	 DONE -- init.kom8.c 
	 DONE -- init.kom9.c 
	 DONE -- init.linearwave.c 
	 ODNE -- init.magbondi.c 
	 DONE -- init.rotor.c 
	 (NOTE THAT ALL INITIAL DATA IS GAUGE DEPENDENT)

	 DONE -- metric.c:
	 DONE -- phys.c:
	 DONE -- recon.c:
	 DONE -- step_ch.c:
	 DONE -- dump.c


	 DONE -- grep through all get_geometry() calls 
	 DONE -- grep through all of_geom declarations to make sure that all of_geom structures have "geom" in their name
	 DONE -- make sure that all assignments of icurr,jcurr,kcurr are necessary;



	DONE -- need to double check that get_geometry is always called within all loops

      DONE -- change all instances of  geom_arr[] uses to get_geometry() calls;

      DONE -- figure out when geometry needs to be updated in time;

      DONE -- change calc_all_geom() 

	DONE -- make global array of of_geom structures;
	DONE -- make conn[] 3d-able 



DONE   	-- generalize get_special_geometry() if possible;

DONE -- spherical bondi and spherical hydrostationary torus runs reproduce 
	old version perfectly,  no problems.  



#############################
Mon Jan 18 15:28:11 EST 2010
-------------------------------


-- This is the first note in the new branch called "harm3d_gr" 
   that is being hosted on the CCRG's CVS server.   This version is 
   meant to be more amenable for use in Numerical Relativity 
   calculations or one in which the metric/coordinate system is 
   time-dependent.  This was done to separate develope along 
   harm3d standard lines as more and more people are using the 
   standard harm3d code.   Plus, the users may not want to use 
   the extra baggage that comes with these changes. 


-- The initial version of the code has been vetted with evolutions of 
	 the Bondi solution and hydrostationary torus.  


-- Already, I have added support for 3d metrics that evolve in time.  See above
   notes. 


!! Adding new routines needed for Kerr-Schild Cartesian Coordinates...
   -- code generated and tested using Maple in  wormhole:~/maple/bh-metrics/kerr-schild-cartesian
   --  need to finish




#############################
Mon Feb 21 11:32:46 EST 2011
-------------------------------

-- added Nico Yunes' Near Zone PN metric to the code  metric_dyn_bbh_35pn_nz.c  metric_dyn_bbh_fullpn_nz.c

-- finishing up the final touches

-- working on new initial data routines to test dynamical spacetime:

    init.pn_bbh_wald.c
    init.pn_bbh_disk.c


-- got it compiled and working with the PN metric. 

   -- first test is     init.pn_bbh_wald.c  without a magnetic field and with the grid centered 
      around only one BH as it serves as a moving BH test.  
      -- during first time step, vchar (used to calculate the wave speeds used by the approximate
             Riemann problem) failed several times, the metric was not invertible several places, 
	     and the lorentz factor was not calculable (complex vsq).    This is what happens near 
             coordinate singularities.  

      -- the only solution is mask out the region near the horizon.  how do we find the horizon?

      -- I did verify that the metric being calculated within harm3d looks very similar to that 
          calculated in ./yunes/c-code 



#############################
Tue Feb 22 15:35:12 EST 2011
-------------------------------

-- ok, so we need a masking function to identify singular points (e.g., coordinate and 
   physical singularities) near which we alter the EOM integration in some way; 
   -- the pflag[] construct seems most natural to exploit, but will not work as it is reset to zero
      after each advance step;  plus, since the coordinates and metric are defined in the ghost zones 
      we do not need to send the mask values to MPI neighbors; 

   -- will will call the new mask array "evol_mask"

   -- the reconstruction procedure and the FluxCT method does not involve the metric, so these 
      can be safely done for all points---even in the excided region. 
      -- I believe we only need to use the mask within the main loop in advance();

#define MASK_EXCISED  0    /* Absolutely nothing is calculated here */
#define MASK_BUFFER   1    /* Reconstruction is performed here      */
#define MASK_NORMAL   2    /* Regular evolution, everything is calculatesd */



-- eliminating fixup_utoprim_2d() everywhere -- not used anymore 
-- eliminating fixup_utoprim3() everywhere -- not used anymore 



#############################
Wed Mar  9 11:39:20 EST 2011
-------------------------------

--  promoting HYDRO_ONLY macro to inclusion in decs.h  so it can be used everywhere (so we can avoid flux_ct, etc.)
    -- note that the user needs to make sure that the initial data is consistent with the value of this macro now 
       as some initial data routines set the B-field without using this macro (so, the B-field would be set to non-zero
       and then not evolved if  HYDRO_ONLY=1)


-- added width to the excision mask (BUFFER_WIDTH);  it should be the size of the stencil at most, may be 
  able to use a smaller width;  also have to consider the flux_ct algorithm since it requires the flux 
  to be calculated outside the evolved domain. 


-- search comments or commented out code  with   "//--bbh"     if you are not doing bbh run


-- added routines to convert to and from cartesian from and to spherical coordinates with Boyer-Lindquist metric;

-- 


#############################
Thu Mar 10 15:58:04 EST 2011
-------------------------------

-- discovered a bug with conn_func() and using METRIC_DYNAMIC_FULLPN_NZ_FAST;  the time-dependent-only 
   functions were not being updated at backward and forward times need for the metric's derivatives; 

   -- this has been fixed and has been tested against analytic KS connection 




#############################
Mon Mar 14 15:35:21 EDT 2011
-------------------------------

-- rearranging some things in metric.c to minimize clutter and make compatible with mixed topologies 
  (between metric definition and numerical coordinates); 

-- introducing macro "METRIC_TOP_TYPE" which holds the topology type of the physical metric being used; 
   -- TOP_TYPE_CHOICE is now the topology type of the physical coordinates of the simulation
   -- if the two are different then we need to perform an extra transformation of the metric to go 
      from one system to the other;  these steps are being set up in metric.c  now; 
   -- note that for now  TOP_TYPE_CHOICE!=METRIC_TOP_TYPE   only for dynamical or general spacetimes;
   -- also note that for now we will be assuming that when the two topologies are different that the 
      metric is defined in cartesian and the coordinates are spherical; 


-- promoted the "M" macro in metric.c to  a global variable since it's shared in many init.*.c files and 
     in metric.c ; 


#############################
Fri Mar 25 11:43:35 EDT 2011
-------------------------------

-- added many new instances of "trace_message()" in order to debug an MPI job at ranger. 

-- fixed routine that writes chunked hdf out via MPI-IO  in harmp_mpi.c  so that it is compatible with N2=1; 
   -- I am not sure this routine is used anymore since we've moved away from using MPI-IO

-- realized that the MPI problem on ranger was not a code problem, but a compiler problem; the pgcc's "-fast" option 
   was producing a bad executable that failed in set_general_geometry() for some reason.  "-fast" is a set of 
   common optimization flags, so I tested each one in turn and eliminated the ones that helped cause the problem. 
   These options are now given in "makefile". 


-- will now be working on implementing modern cooling function, MacFadyen and Milos data, and the new IZ+NZ metric.  


-- added cooling_func_isentropic_disk() that cools the disk to a constant target entropy;
-- promoted USE_COOLING_FUNCTION  to  parameter that specifies the cooling procedure if non-zero;


-- added restart_write()  before final diag() call to ensure that we dump a restart file at the end of 
   a run;  especially useful if we set the runtime via RUNTIME_SECONDS  as it will dump a restart file 
   right before it quits so that we can pick up exactly where we left off from;


-- added new Near Zone + Inner Zone metric  made by Bruno Mundim.  


#############################
Mon Apr 18 11:37:03 EDT 2011
-------------------------------

-- working on adding MacFadyen & Milosavljevic (2008) data; 
   -- look for "--HERE"  comments for places I need to verify in the future;


-- using Utoprim_1dfix1() and Utoprim_1dvsq2fix1() to find the primitive variables in order to	
   do isothermal evolution for MM08 test;
    -- making new routine "recover_primitives_simple()" that works like recover_primitives() but 
       uses these new inversion routines;

    -- modernizing Utoprim_1dfix1() and Utoprim_1dvsq2fix1()
    -- 

#############################
Thu May  5 15:58:01 EDT 2011
-------------------------------

-- still working on reproducing MM08; 
-- have the initial data finish and the utoprim issue finished, now working on adding a 
   viscous stress to the EOM; 

-- need to increase the number of locations at which the connection is stored; 
DONE    -- need to change dump_hdf.c 
DONE    -- metric.c
DONE    -- CONN_ID definition
DONE    -- CONN_LOOP 
DONE    -- GDUMP_LOOP (need to make sure that it's not used in any other context )

DONE -- need to add additional "advance()" call to adjust for  viscous source terms and recover primitives thereafter;

DONE -- will need to do tests after this change as I'm changing some fundamental macros;
   -- seems to work

-- will not implement  MAKE_STAT and MAKE_STAT2   into advance_viscous()  yet ; 


#############################
Tue May 31 16:31:58 EDT 2011
-------------------------------

-- did inviscid MM08 run (with one single black hole) 
   -- the floor within the disk does not seem to be stable for small N3 and N3=1 runs;
   -- the gap region fills in with more matter and then interacts with the boundary which ultimately results in a
      reflection that significantly disrupts the disk's bulk. 
     
-- did viscous MM08 test and the viscosity source terms seem strange;  a rough order of estimate guess seems to indicate 
       an error in the calculation, but maybe my estimate was wrong;
       -- still need to figure this out;



-- advancing to near-equilibrium tori around BBH's; 
   -- see notes in   ./init_data/hydro

   -- added  phi-averaged   called  METRIC_GENERAL_PHI_AVG   that takes a  METRIC_DYNAMIC_*  spacetime and averages it to get a 
      kind of time-average/phi-average spacetime.  

      -- made sure that using 

#define METRIC_TYPE_CHOICE (METRIC_GENERAL_PHI_AVG)
#define METRIC_DYNAMIC_TYPE_CHOICE (METRIC_DYNAMIC_KS_SPHERICAL)

          yields the same results as 

#define METRIC_TYPE_CHOICE (METRIC_KS_SPHERICAL)


-- using maple, we verified that the PN metric when phi-averaged yields a metric with the same zero elements in the metric as the 
Boyer-Lindquist metric, so that we can safely assume that the only non-zero non-diagonal metric element is   g_{t \phi} . 


-- now working on general torus equilibrium solution method  in  init.gen_axi_disk.c 
   -- problem:  cannot get  h > 1   disks...





#############################
Thu Jun  9 15:51:26 EDT 2011
-------------------------------

-- debugging still, 

-- verified that single black hole disk data works with init.gen_axi_disk.c 

-- working on PN BBH case

-- found that with  initial_bbh_separation=30.  and m_bh1=m_bh2=0.5, that  n_phi_avg=100 is equivalent to 
   n_phi_avg=1000  to roundoff error, while n_phi_avg=10 is not equivalent to them. 

-- 


-- remove //--testing  comments


DONE        -- verify PHI_AVG spacetime recovers KS again as I've added the sqrt(g_{phi phi}) weight ; 


#############################
Fri Jul  1 17:15:12 EDT 2011
-------------------------------

-- need to add  hdf5-type history-type dumps of data over r-phi-t  (i.e. integrated over theta) that 
would be dumped more frequently than full 3d dumps, but less frequently than (r-t) history dumps. 


#############################
Mon Jul 11 11:06:11 EDT 2011
-------------------------------

-- tracking down where all the calls to "dyn_fullpn_opt_nz_gcov_func" are coming from. 

-- nstep=0  step  called "dyn_fullpn_opt_nz_gcov_func"   213.42 times per cell 
-- nstep=1  step  called "dyn_fullpn_opt_nz_gcov_func"    90.98 times per cell 
-- nstep=2  step  called "dyn_fullpn_opt_nz_gcov_func"    90.98 times per cell  (exact same as before)
-- nstep=5  step  called "dyn_fullpn_opt_nz_gcov_func"    90.98 times per cell  (exact same as before)


        gcov_func
        |
        |    
        |    |   
        |---->|---->
                  |
                  |-----> 


dyn_fullpn_opt_nz_gcov_func   
  x1   <---- general_gcov_func                 
        x0       <---- general_gcon_func  
		       (never called)

        x1       <---- gcov_func
                 x8    <----- conn_func  (x8)
                       x1     <------ set_general_conn()   
                              x1      <----- init_general_metric() 
                                             <---- calc_all_geom() 
                                                   <----- init 
                                                          <------ main 

                              x2      <----- step_ch()   (once per substep)
                                             <---- main
 
                       x0     <------ test_geom()
		                      <------ init()  (typically, but not called from init.gen_axi_disk.c) 
                                             <---- main

                 x5    <----- set_general_geometry 
                       x2     <------ advance_geometry
                              x1      <----- step_ch() 
                                             <--- main





                       x3~    <------ init_general_metric()  (only at t=0)
                                             <---- calc_all_geom() 
                                                   <----- init 
                                                          <------ main 

               

                 <---- get_special_geometry (x1) 
                       <----- init()  
		              <--- main
                       <----- omega_circular_equatorial()  (only called once for first call to cooling functions) 
		              <--- ... main()



    -- this counting means that there are     
                       N_substep * NPOS * N1TOT * N2TOT * N3TOT    metric evaluations per update step (from tn to tnp1)  for just setting metric at cell locations
     and 
                       N_substep *  2 * NDIM * N1 * N2 * N3        metric evaluations per update step (from tn to tnp1)  for setting connection



#############################
Thu Jul 14 11:06:58 EDT 2011
-------------------------------

-- need to add  hdf5-type history-type dumps of data over r-phi-t  (i.e. integrated over theta) that 
would be dumped more frequently than full 3d dumps, but less frequently than (r-t) history dumps. 
      -- I will name it "dump_surface.c"  



-- adding surface_mpi() routine (like history_mpi() for history data)  in harm_mpi.c 
   -- added BASE_TAG_SURF    to harm_mpi.h 
   -- not compatible with MPI-IO yet (future project) 


-- will have to dump individual surface files per r,phi patch/processor as  the total Nr x Nphi x Nfunc
     *could* overwhelm a single node and we want to avoid MPI-IO


-- moved some code around in dump_hdf.c  so that the header data writing is in its own routine, so that 
   it can be used by other routines (e.g., dump_surface()); 



#############################
Mon Aug  8 11:23:26 EDT 2011
-------------------------------

-- adding new algorithm for determining the time step size:   find the global minimum light crossing time 
   of a MHD wave across a cell.   I forget the justification of the old method now. 

   -- added macro : TIMESTEP_METHOD   with options  TIMESTEP_ORIGINAL and TIMESTEP_MIN_CROSSING_TIME 




-- looking into transforming directly from cartesian metric to harm3d's numerical grid
   -- transform_rank2cov() and transform_rank2con() are used appropriately;
   -- transform_rank2cov()/transform_rank2con()  depend on dx_dxp_calc();
   -- dx_dxp  is most of the time used to translate into "r,th,ph" space so that we can rescale gridfunctions 
      or perform special boundary conditions;

   -- would only need this special dx_dxp() for transform_rank2cov() to calculate gcov;  gcon and gdet 
       are calculated from gcov when METRIC_TOP_TYPE != TOP_TYPE_CHOICE 


   -- realized that dx_dxp and dxp_dx  are calculated over the entire domain, but are recalculated 
      each time gcov_func is called even though it doesn't change over time.  
      -- 


gcov_func <---  conn_func 



-- making a big change, creating a new "of_coord" structure that acts like the of_geom structure 
   for the coordinate transformation to numerical coordinates and its necessity data; 

   -- changing conn_func()  so that time derivatives are only done if DYNAMIC_SPACETIME is set
!!  -- need to test conn_func when DYNAMIC_SPACETIME=0  



DONE -- need to get rid of 
DONE        -- x_gf ,xp_gf,dx_dxp_gf,dxp_dx_gf
DONE	  --  most calls to  x_of_xp() coord()
	      	   DONE -- bounds.c  done
		   DONE -- dump.c
		   DONE -- dump_hdf.c 
		   DONE -- recon.c 
DONE		   -- metric.c
DONE		   -- init.*.c
		   

DONE -- verify cooling function rate vs. r  


DONE -- will have to write "advance_coordinates()" routine eventually

-- wrote calc_all_coord() to set all elements in the of_coord structure array;

-- wrote void xcart_of_xspher_special(struct of_coord *coords )
   to set special transformation from cartesian metric to xp coordinates


#############################
Tue Jun  5 23:46:32 EDT 2012
-------------------------------

-- working on dyanmic coordinates now  (see COORD_DIAGONAL3_DYN_RAD)	
   -- one could piggy back on set_general_geometry() and set the coordinates from there,	
      but then that would mean we would have to use a dynamic metric whenever we would want a 
      a dynamic coordinate system;	
      -- is there a case where the metric is not dynamic when then coordinates are? 
      	 -- there are cases where the metric in the "physical" coordinates is stationary but the 
	    coordinates are dynamic, but this would make the metric in the numerical coordinates 
	    (which is what we use) dynamic; 
	 -- there's the theoretical possibility of the time-dependent aspect of a physical metric 
	    to be transformed away via the dynamic coordinate transformation; then this seems 
	    silly as we'd just be able to derive the metric as stationary straight away; 
	 -- this means that whenever the coordinates are dynamic, the metric need be, so we can 
	    piggy back; 
      -- or instead we could model the update scheme of geometry for the coordinates update procedure;

DONE  -- need to make sure that dynamics KS is really dynamic and allows for coordinate evolution;      

-- going down coord.c  for needed changes:
   -- finished with coord.c 
DOING   -- need to make sure we don't assume or use COORD_TYPE_CHOICE elsewhere
DONE   -- need to finish writing update procedure within the geometry update procedures;
!!   -- need to make sure that cooling function routines handle dynamic coordinates in phys.c


-- need to update or store the future and past coordinate time funcs in the conn_func() command when calculating the future and past coordinates; 
   -- should store them to speed up operation; 
   -- we are doing time differencing of metric like before, using local variables as doing the 4th order while using the already calculates quantities 
      does not save much time;
!! -- maybe in the future change this to the 4th-order scheme that uses those future/past levels of metric data; 
      -- likely inefficient and overly complicated...


#############################
Fri Aug  3 16:28:18 EDT 2012
------------------------------

-- dynamic coordinates implemented 
-- tests with old options verify bondi still works like before, so nothing is broken
-- verified that bondi works as well with COORD_DIAGONAL  as COORD_DIAGONAL3 to within roundoff error;
-- verified that METRIC_DYNAMIC_KS_SPHERICAL results in same results as METRIC_KS_SPHERICAL
-- verified that CONN_METHOD_4TH_ORDER method results in very similar results to that using exact analytic connection
-- added option "OUTPUT_COORDS"  that --- if set--- dumps the coordinate grid functions to the regular dump files


!! -- need to figure out how to handle history and surface data with dynamic coordinates
   -- one way would be to just perform integrations in numerical coordinates and let viz routines handle evolving coordinates;  
      -- this method is more inline with harm3d's covariant philosophy
   -- another method would be to interpolate onto a consistent radial grid
      -- history/surface data is gauge dependent anyway, so maybe we need not have to conform with the "covariant philosophy"


#############################
Thu Nov  1 15:51:18 EDT 2012
-------------------------------

-- added smoother transition between static and shrinking spherical coordinates of type choice 
      COORD_TYPE_CHOICE = (COORD_DIAGONAL3_DYN_RAD)

    -- doing an axi-symmetric hydrostationary disk, we see that the diffusion rate is actually lower than 
       the static grid;	      
       	   -- this is likely because the radial resolution becomes better over time.  
	   -- without the smoother transition, there's a large recoil by the disk;
	   -- even with the smoother transition, there is a slight wobble in the density maximum, though 
	      
    -- noticed that RESCALE_R (as it stands) should not be used when using dynamic coordinates;



#############################
Mon Dec 10 13:36:18 EST 2012
-------------------------------

-- Miguel Zilhao has joined the harm3d development team, along with older members Bruno Mundim and Hiroyuki Nakano; 

-- Miguel is adding the spherical warped coordinates for binary black hole spacetimes; 

-- I was looking at what needed x_of_xp(), as we want to optimize things as good as possible for the complicated warped 
   spherical system:

   -- x_of_xp  <----   xp_at_eq()      <----  coord_of_r()  <----  init()
                                                            <----  omega_circular_equatorial()  <---- cooling_func_hr_disk()
                                                                                                <---- cooling_func_isentropic_disk()
												<---- set_Katm()
                 
                                       
               <----   coord_of_xp()   <----  coord_of_r()
	                               <----  calc_coord() 
                                       <----  calc_all_coord()
				       <----  conn_func()

               <----   init() -- ok 
               <----   test_geom()   -- ok 
               <----   set_excision_mask()  -- ok
               <----   
               <----   



DONE --  need to make sure that time derivates (see conn.h)  of the metric for the connection calculation are correct if the 
   coordinates change in time


!! need to make sure that the cooling function is updated with dynamic coordinates or is not dependent on dynamic
    coordinates altogether;


DONE !! in looking at set_excision_mask(), I noticed that it would access out-of-bounds mmemory in some circumstances, because
DONE    it was not updated to conform to the new global grid array of "of_coord" data structures; 
DONE !!   -- need to fix distance_to_closest_point_in_cell();
DONE !!   -- need to fix set_excision_mask();



DONE -- noticed that init.c was an outdated version of init.kd.c, so I copied over init.kd.c to init.c

#############################
Tue Dec 11 16:09:10 EST 2012
-------------------------------

-- starting new branch to develop the dynamic domain decomposition

-- using : 
      unsigned int  n1_loc, n2_loc, n3_loc   as the local extents of the domain
      

-- parameters dependent on  N1,N2,N3:
	   -- N[1-3]TOT, 
	   -- NTOT
	   -- N[1-3]E
	   -- N_HIST_POINTS
	   -- N_SURF_POINTS
	   -- N_CONN
	   -- NM[1-3]
	   -- N_GEOM
	   -- MAX_NTOT
	   -- N_COORD
	   -- N[1-3]_R_E
	   -- NM[1-3]E
	   -- NM[1-3]_TOT
	   -- 

!! -- need to make sure that no arrays using the above parameters assume contiguous memory (e.g., the conn[0][0][i] loop);
DONE   -- hist_data loop in dump_history.c 
  !!  -- conn in dump.c, metric.c, coord.c,  (elsewhere)
DONE   -- surf_data  definition in dump_surface.c  
DONE   -- fixup_interp_prim() in fixup.c 
DONE   -- fixup_interp_v() in fixup.c 
DONE  !!  -- emf loop in flux_ct.c 
   DONE -- changed nfail reset loop in dump_stat.c  
   


-- I think we need to make the dependent parameters also variables that evaluate their expressions
   so that we do not evaluate things like  (N1+2*NG)  in loops;  with compile-time values set, the compiler (likely) knows to 
   compute this at time of compilation and not at runtime;  here, with a non-const variable in the expression the compiler 
   cannot evaluate the expression to a number so it will be forced to perform the computation per loop iteration; 
DONE  !! -- hence, we need to set local variables for all the dependent parameters/macros;


!! -- will likely also want to substitute all expressions like (N1*N2*N3)  that I assumed would be evaluated at compile time
      -- IND_LOOP
      -- CONN2_LOOP
      -- NV[1-3]_LOOP
      -- NM[1-3]_UP_LOOP
      -- check all loops....

!! -- along these lines, we should optimize loops that use arithmetic using non-constant variables (e.g., "N1E-1") 
       in the for statement (e.g., throughout flux_ct.c);


-- initial strategy is to implement a run-time determined uniform grid decomposition, and then add the dynamic part later after
   things are tested....


DONE ?? Are MACROs defined in harm_mpi.h  accessible in other files? 
     -- NO!
   

-- moving header stuff in recon.c to its own header file recon.h, necessary for use of its macros when setting globals....


!! -- need to make sure that there are no parts that depend on the fact that coordinate transformation matrix is diagonal for xp3/phi and  t/xp[1-3];  


!! -- check to see if coolflux[1-3][*] are ever used
      -- looked briefly through the code, and I think we can safely remove the other components....
      -- we can always recover the cooling flux from the velocity components and the metric if need be, but it's rarely 
      	 ever useful; 

!! -- need to write a routine that---detects a change in the local subdomain dimensions, calls the global array deallocation
      routine, then calls the allocation global array routine; 


-- cleaned up fixup.c  a little (removing unused variables);

DONE -- Need to make init.gen_axi_disk.c  compatible with  nondiagonal coordinate systems;	
     	-- added init.gen_disk.c


!! -- should write a routine that has each processor read its own portion of a merged/global restart file;
   !! -- or conversely, have the local grid structure set by the restart files, with the master making sure that 
      	 the number of restart files is equal to the number of processors requested; 
      -- right now the grid decomposition must match the checkpoint data;


!! -- need to make sure that routines that use "first_time"  or "local_first_time", etc., are compatible 
      with dynamic, 3d coordinates;


!! -- need to scan through all files to see if they are dynamic-safe:
      DONE -- flux_ct.c  -- needs optimization in loops for later
      DONE -- recon.c  -- needs optimization in loops for later
      DONE -- step_ch.c  
      DONE -- decs.h
      DONE -- main.c 
      DONE -- u2p_defs.h
      DONE -- u2p_util.c 
      DONE -- metric.h
      DONE -- harm_mpi.h
      DONE -- decs.h
      DONE -- conn.h
      DONE -- utoprim_*.c
      DONE -- restart.c
      DONE -- ranc.c
      DONE -- metric_*.c
      DONE -- lu.c
      DONE -- image.c  -- assuming that it still works.... I haven't used it in quite some time;
      DONE -- Ricci_v0.c  -- most likely not coordinate dependent...
      DONE -- bounds.c -- though could optimize loops in the "set_cutout_*" routines that use (N1TOT-1), etc
      DONE -- dump.c -- though we may want to eventually make sdf output work with dynamic domains
      DONE -- dump_stat2.c
      DONE -- dump_stat.c  -- though we need a way to handle dynamic regridding ;
      DONE -- diag.c
      DONE -- init*.c  -- i think....
      DONE -- init.gen_axi_disk.c --- though need to generalize it to mixed coordinates;
      DONE -- dump_radflux.c
      DONE -- dump_hdf.c      ??-- can we have USE_CHUNKS=1  and still merge all files with different chunk sizes???	
      DONE -- 
      DONE -- 
      DONE -- 
      DONE -- phys.c  :  need to make it dependent on th/phi too, need to optimize loops for USE_KINEMATIC_VISCOSITY;

     ~DONE  -- fixup.c :  need to make floor arrays 3d, dynamic, -- almost done, need to change Katm; 
	      	      	 -- need Katm[] for USE_SIMPLE_EOS
			 -- also used to calculate nu_visc[]  
			 -- Katm[] set in:
			    --  set_Katm() in fixup.c
			    -- init.gen_axi_disk.c
			    -- init.gen_disk.c
			    -- init.nz_pn_bbh_mm.c;
			 -- set_Katm() called only if USE_SIMPLE_EOS is set in restart.c 
			 -- nu_visc[] set in init.nz_pn_bbh_mm.c
			    -- nu_visc[] used only when USE_KINEMATIC_VISCOSITY  is set; 
			 -- will keep these for later!!


	   -- decomp.c : ?? what else do we need here

     ~DONE -- dump_history.c  -- needs to be fundamentally changed if totalsize changes
                              -- needs to be changed to handle nonuniform decomposition....


     ~DONE -- dump_surface.c  -- needs to be fundamentally changed if totalsize changes
                              -- needs to be changed to handle nonuniform decomposition....
      

!! -- fix Katm[] and nu_visc[] arrays for dynamic, non-diagonal coordinates  for when USE_SIMPLE_EOS or USE_KINEMATIC_VISCOSITY is used




-- MPI call structure:
   
   setup_mpi():
	-- init_MPI()
	   -- MPI_Init()
	   -- MPI_Comm_size()
	   -- MPI_Comm_rank()
	   -- MPI_Get_processor_name()
	   -- myargs()	
	      -- sets subdomain size and number of procs from command-line arguments;

	-- set_mpi_grid();
	   -- set_global_domain_variables()
	      -- sets other subdomain global vars;
	   -- sets some global vars, totalsize[], n_cells_glob, 
	   -- get_cpupos_pid();
	   -- get_global_ijk_pid()
	   -- get_global_index_pid()
	   -- setup_mpibc_info()
	      -- sets types of boundaries
	      -- sets maps of boundaries to processors;
	      -- sets ranges of indices for send data;
	      -- setup_mpibc_info_sym()
	      	 -- sets boundary conditions for special symmetry conditions;

	-- set_mpi_misc()
	   -- sets the chunk size for 3d comm size, and allocates that memory;
	   -- mpi_comm_chunk -- (data array)
	      	-- used by  get_global_gfunc(), write_chunk(), read_chunk(), myexit();
		-- only used by MAKE_SDF
		
	-- 	   
	--
  


#############################
Wed Jan  9 11:58:35 EST 2013
-------------------------------

-- code successfully compiles with dynamic allocation but with uniform decomposition;

-- comparing run results with that from the starting version of the Prac code;
   -- see > 1e-3 differences at some points in :  
      conn011, conn012, conn013, conn021, conn022, conn023, conn031, conn032, conn101, 
      conn102, conn103, conn110, conn120,  conn121, conn130, conn, conn201, conn202, 
      conn203, conn210, conn220, conn230, conn302, conn310, conn320

-- nans in B[1-3], bsq, divb, 
-- > 1e-3 differences in gamma, uu, rho, v[1-3]

-- nans seem to be produced in vchar_fast() when  eps=p=0 (0/0 calc), which seems to occur in the ghost zones;
   -- maybe we are failing to set the boundaries/ghost-zones before the first numerical_flux() call???


-- tracked down the bug to a typo in
    N1_R_E_glob = N1S+BUF; 

    should be 

    N1_R_E_glob = N1E+BUF; 


-- finding roundoff error level differences at t=0 that grow large (max 1e-4, typical 1e-10 to 1e-6) after ~50-90M of time; 
   	   -- even for magnetized disk
	   -- this is for 3d circumbinary magnetized disk evolution
	   -- I think this ok;

-- passes with 2d runs:  
   -- tested with N[1-3] = [32,32,1]
   -- tested with N[1-3] = [32,1,32]



#############################
Tue Jan 15 12:47:19 EST 2013
-------------------------------

-- fixing up using DYNAMIC_COORDINATES with DYNAMIC_GEOMETRY;	
   -- creating a new typedef called "of_coord_params" that is to be set to one of the structures that stores the 
      time-depeendent functions/quantities for the dynamic coordinate system;  
      -- similar to nz_params[], "coord_params" will be set to one of the three time levels of "coord_params_all[3]"
      -- thus, I had to change all the "params_dyn_rad" variables to "coord_params" instances;

      -- note that when DYNAMIC_COORDINATES  is set, then DYNAMIC_GEOMETRY must be set;
            -- we assume that nz_params needs to be set before coord_params as is often the case;
	       	  -- this makes sense because the physics should not be dependent on the numerical coordinate system	
		     that we are using but the numerical coordinates are almost always set to resolve the physics;


!!! -- may be able to optimize the connection calculation by limiting what "coord" functions are calculated in the
       call to "calc_coord()" for the time derivatives; 


-- simplifying when "set_excision_mask()" is called;  it's called whenver the geometry changes, as it's always ever
   used right now to excise the BH singularities and so would only change when the geometry changes; 
   -- I also made it have different time levels so that it had the same structure as the geometry grid functions;
      -- connecting the geometry and mask logically makes it easier to think about and prevent more/future bugs;


-- fixed the spherical warped coordinates as we were using the phi from nz_params for the phase of the binary orbit, 
   which put the black holes out of the "periodic" part of the warped coordinates since the coordinate system was 
   expecting phi1,phi2 to be less than 2pi; 

-- even after the coordinate fix, the excision mask is still not working;  not enough points are being flagged as	
   being within the horizon; 


???  -- do we want to set buffers cells along the diagonal from the excised surface?



!!!!!  -- the parabolic fluxct is technically dependent on the decomposition of the domain in that it always performs 
       	  linear reconstruction of the fluxes at the subdomain's boundaries instead of parabolic interpolation in the bulk; 
	  -- this will lead to differences in results between two equivalent runs that use different decompositions, though 
	     I'd expect these to be slightly below normal truncation error as we are averaging fluxes which are already 
	     calculated using 3rd-order reconstruction and the time step is 2nd-order accurate;
	  -- in order to fix this we would have to double the ghost zones to  NG=6  or communicate the EMFs or fluxes or 
	      something;


-- added a bit of documentation to flux_ct.c  that details the stencil of influence of the different CT schemes;


#############################
Tue Mar 26 16:37:53 EDT 2013
-------------------------------

-- have a working version of the WARPED_SPHERICAL coordinates;
   -- created a git tag version to identify working version

-- Miguel preformed the straightforward optimizations and they seem to work except there seems to be a problem 
   in how they are updated for the connection calculation;  
   -- 

-- added "EQUATORIAL_RUN" macro in decs.h that can be set to setup the code for a run performed in the 
   spherical equator of domain;  
   -- this is useful for debugging the warped spherical coordinate system in "2d";




#############################
Thu Jun 13 22:59:45 EDT 2013
-------------------------------

-- dynamic memory version (this version of harm3d) seems to perform as well as the static version (see profiling results on Blue Waters);

-- need to fuse the nonuniform decomposition code and this harm3d code;

!! -- in addition to clearing all the above todos (marked by "!!"), we need to do the following:
DONE   -- change history and surface reduction operations to support decomposition nonuniformity;
!!   -- figure out how to communicate decomposition to all procs
!!      -- master tells all, most likely;
!!   -- figure out how to checkpoint new runs;
!!   -- figure out how to redecompose (will we ever?)
!!      -- on the fly, i.e. dynamically? 
!!      -- or before submission, using rearranged checkpoint files/data??
!!   -- implement timers throughout the code
!!      -- how do we get cell-scale effort data?  just through interpolation of multiple decompositions? 
!!      -- maybe its enough to time the metric evaluation at the cell scale, or use preassigned weights on the NZ/IZ/FZ evaluations and knowing 
!!          when and where we use NZ, IZ, or FZ?


-- working on dump_history_gen.c , the new history dump routine for warped coordinates:
   -- the initial thought was to have each process bin the data using the full coordinate extent, then sum over 
      the binned data over all processes; 
   -- thinking also of the surface dumps, we will not likely be able to store the entire radial or radial x azimuthal 
       for hundreds of functions (~40 funcs x [jet,bound,unbound]) and communicate it to summing nodes very 
       efficiently; 
   -- alternate strategy would be to minimize the data summed and communicated by exploting the fact that even 
      though the coordinates are warped, they are still nearly spherical so a processes cells only cover a small volume;
      thus, we could only allocate and communicate those bins that need communicating:
      	  *surf_data[n_r_bins][n_phi_bins]
	  where surf_data[i][j] would be allocated to N_FUNCS if it is accessed.       
       -- this strategy will still be pretty efficient for Cartesian grids because the subdomains should be nearly cubical 
       	  and so local in r,phi coordinates.  

       -- using this strategy;


-- in order to spread out the responsibilties (e.g., collecting and summing history/surface data, writing	
   history/surface data, ...), we should assign different processes different tasks; 
   -- these roles will be assigned via variables such as  "pid_history_writer", or "pid_history_collector"; 
!! -- need to write a routine that assigns these duties;
   -- ideally, we would have different "writers" and "collectors" on different nodes for better load balance; 
      -- need to figure out how to do this in general;  are there MPI standards on "affinity"-like information;

#############################
Fri Jun 21 12:13:18 EDT 2013
-------------------------------

-- adding the general coordinate version of dump_surface.c called  dump_surface_gen.c  ;
-- after looking at dump_surface.c, I realized that I should update dump_history_gen.c to use the better way of naming and indexing history data functions; 
   -- I therefore updated dump_history_gen.c to use the scheme of dump_surface.c, which involved changing how N_HISTORY is set in decs.h 
   -- i verified that this new routine works after these changes;


#############################
Mon Jul  1 14:31:41 EDT 2013
-------------------------------

-- adding "myH5_write_array()" to dump_surface_gen.c;  it can be used to write a 1-d array of arbitrary type to a specified 
   hdf5 file;


-- working out some bugs in dump_surface_gen.c and am testing it out now. 
    -- confirmed that dump_surface_gen.c  works!!! 


#############################
Tue Mar  4 14:40:15 EST 2014
-------------------------------

-- making a stable single-patch version of this code, starting from  commit  1636d7915422d4be9ebb399f381f7488c08e6131
    of the "dyndom" branch

    -- will call it "monopatch"


-- updating this code to latest version of static version of harm3d:
   -- importing : 
!!		clean_monopoles.c  -- DONE -- need to test!! 
!!		corona.c           -- DONE -- need to test!!
		init.gen_shock.c   -- OK
		metric_dyn_bbh_fullpn_nz_drop.c  -- OK
		metric_dyn_bbh_fullpn_nz_iz_fz_drop.c -- OK

 !! -- will need to test init.gen_shock.c  
 !! -- will need to test clean_monopoles.c 
 !! -- will need to test corona.c  via USE_COOLING_FUNCTION=4


#############################
Thu Mar 13 16:33:22 EDT 2014
-------------------------------


DONE -- when free-ing  or deallocating  arrays that were allocated with ALLOC*ARRAY() macros, do I have to 
DONE      free() the fastest dimensions first, or just doing a free() on the pointer (e.g.,  free(p) ) suffices? 
DONE      -- i'd be surprised that free() recursively frees all the memory itself...
DONE      -- if it doesn't then we need to change all instances of FREE() and free()  to take into account the array's dimensions;
      -- yes, free() does NOT recursively free up the allocated arrays;  I think I always knew this but was being lazy when I didn't
      	 implement it;   
 
      -- now I correctly FREE all multi-dimensional arrays;  see the "DEALLOC_*" macros in decs.h 

      -- I also changed the "ALLOC_*"  macros so one does not need to specify the type of the array; 


      -- I have changed the default dx[2] and dt_conn parameters to be larger for EQUATORIAL runs as I was noticing round-off errors on 
      	 Blue Waters leading to significant errors in the connection which lead to very very significant effects on the disk; 
	 -- this run used the static version of harm3d with the PGI compilers on Blue Waters;
	 -- I am now trying this run on Blue Waters with the Cray compilers using this, dynamic version of harm3d;


      -- I got the MPMD test program working in ./mpmd-test  ; 
      	 -- the test worked as expected with no problems;  just type "make run" in the directory;
	 -- when I tried running it the first time, mpiexec reminded me to run "mpd";  once i did that an re-ran the program, it worked fine;



-- working on interpolation function: 
   -- the idea is to interpolate data from and to warped spherical coordinates and uniform Cartesian; 
   -- given src data at  x1, need to interpolate to coordinate at x2; 
      -- will use slope-limiting procedure to interpolate from uniform x1 system to an arbitrary point at x2; 
   --  src=warped-spherical    dest=Cartesian
       -- hard because  warped-spherical(Cartesian) is not known ;
       -- xp(x) requires Newton-Raphson solve, though we don't expect the solutions to change w.r.t. phi much 
       	  near the inner cutout;
	  -- we could eliminate the Newton-Raphson solve if we interpolate in r-th-ph space; 
       -- need  dxp/dxc  to transform vectors;  calculate this by numerically inverting dxc/dxp (which is known); 
       -- instead of writing  the xp[] coordinates for later use, calculate the Cartesian coordinates, and then 

   --  src=Cartesian           dest=warped-spherical
       -- easy because   Cartesian(warped-spherical) is known 
       -- need  dxc/dxp  to transform vectors in warped-spherical, which is analytically known, no problem;

       !! -- need Newton-raphson procedure to calculate  xp(x) for warped-spherical (this is needed anyway for bothros);
       !! -- need interpolation routine that given   src and dest coordinates, interpolates to the dest        
       


-- creating a new file called init.interp_data.c  that I will use to test out new interpolation schemes for 
   the dual-patch setup;  
   -- the idea behind init.interp_data.c  is to read in data generated in Cartesian coordinates, then 
      transform the vectors to the program's system, then interpolate the source data to the destination 
      system of the program. 



#############################
Tue Mar 18 11:19:46 EDT 2014
-------------------------------

-- on BlueWaters, for a 2-d hydro, all-zone 2PN, black holes on the grid, warped spherical coordinates run 
   using  25x50 cells per domain, 128 cores,  ala:
      ./harm3d  "16" "1" "8" "25" "1" "50"
   
    Cray compiler (w/ system's default options)  ran 1.784169365  times faster than PGI (w/ -fast option)

    -- using 1PN all-zone metric instead  
       Cray compiler (w/ system's default options)  ran 3.603919828  times faster than PGI (WITHOUT -fast option)
       	    	     -- I forgot to include the "-fast option with the PGI run, i'm rerunning it now...

    -- using 1PN all-zone metric instead  
       Cray compiler (w/ system's default options)  ran ???  times faster than PGI (WITH -fast option)
       -- the 1pn cray run had an IO problem that delayed the rdump write quite a bit; 	 
       	  -- will need to verify each run for this to get  more accurate numbers out of this experiement;
	     -- need to look at output_log.dat timing data



#############################
Thu Mar 20 15:04:29 EDT 2014
-------------------------------

-- tested init.interp_data.c and it works.  I did the following:

    1) generated Cartesian shock data using init.gen_shock.c, 
         CARTESIAN_MINK, COORD_IDENTITY, EQUATORIAL_RUN=0, and 
	 running  "harm3d 1 1 1 128 128 1 " 

    2) ln -s cart-data/KDHARM0.000000.h5 rdump_start_other.h5 

    3) ran with init.interp_data.c  (with SPHERICAL_MINK, COORD_IDENTITY, EQUATORIAL_RUN=1) 
        and running "harm3d 1 1 1 32 1 32 "

    4) compared result to spherical run (all same parameters as in (3) but now using init.gen_shock.c 
         and results are nearly identical.  

	 -- this test does NOT test the velocity component transformation part because the 
	    velocities are initially zero; 


#############################
Thu Apr  3 21:54:49 EDT 2014
-------------------------------

-- adding new data output format "TIMERS" which write timing data recorded along the way;  
   -- adding new file "dump_timers.c"  that will contain routines responsible for timing data IO; 
   -- see "MAKE_TIMERS"   in decs.h and throughout code;
   -- made a wrapper for MPI_Wait() to quickly add timing calculation around every MPI_Wait() call in harm_mpi.c 
   -- so far, we are just timing a few places in the code, the big parts I presume (see "TIMER_TYPE_*"  in decs.h) 


         -- writing a making two different version of myH5_write_gfunc():  myH5_write_gfunc_orig() and myH5_write_gfunc_gather();  the former is the 
	    original version where each processor dumps it's own block into its own file, while the latter performs and MPI_Gather to the 
	    head node before dumping.  need to sort the data also so that it makes sense on disk; 


 -- adding macro  GATHER_IO  that, when set, will gather data to the master process before dumping, pushing all 
     IO  (at least hdf5 3-d dumps) to the master process; 
     
     myH5_Fcreate()
     write_dump_header()
     set_hdf5_gfuncs()
     gdump_hdf5
     myH5_write_int_gfunc()
     myH5_write_gdump_func()
     myH5_write_scalar2()
     myH5_read_scalar2()     
     myH5_read_gfunc() 
     myH5_Fopen
     restart_check_hdf5
     restart_read_hdf5

     dump_stat
     dump_stat2
     dump_radflux

     -- i think dump_history.c is fine;
     -- i think dump_surface.c is fine;

  DONE -- this fully implemented now



-----------
URGENT TODOs:
   DONE     -- constraint "WARNING" messages only to myid==0 ; 

       -- still having problems with BW (BlueWaters);  it seems like most of the stalls happen with the rdumps; 
       -- possible solutions to attempt:
       	  DONE -- reduce frequency of rdumps;
	            -- tried this and the restart didn't even get to the restart dump; 
	  DONE -- do gathers and dump into "collected" single dump files for rdumps; 
	     --   or have each processor write to separate dataset in one rdump file; 
	  ?? -- better analyze the ATP core dumps;  i've visualized it but can't understand the representation; 
	     -- possibly ask for help on this matter;
	  -- I "disabled" (via early "return;" call) exit_status() and that didn't help, and the crash happens in bounds() now;
	       -- this reminds me othe early seg fault problem with the cray compiler;

!!	  -- I will try running the job with the dynamic version (this version);  the latest problems were with the code with the static version, 
	     which may have a seg fault bug still ; 
	  ??   -- seems to work so far
	  
	  
         DONE -- writing a making two different version of myH5_write_gfunc():  myH5_write_gfunc_orig() and myH5_write_gfunc_gather();  the former is the 
	    original version where each processor dumps it's own block into its own file, while the latter performs and MPI_Gather to the 
	    head node before dumping.  need to sort the data also so that it makes sense on disk; 


        !! need to figure out how to handle myid==0 cases with MPMD runs;  
	   -- i guess we need to designate different roles in the  code and store their pid's in variables; 
	  
---------



#############################
Mon Apr 14 10:26:03 EDT 2014
-------------------------------

-- finished implemented and tested the new GATHER_IO option.  
   -- it seems to only take 1-3 seconds to write the 3-d dumps;
   -- the surface dumps seem to take the longest now 
   -- so far runs have not stalled or crashed on BW, i'll need to do more experimenting. 



-- encountered a weird MPI bug that is probably a documented "feature" that I was unfamiliar with:

   -- If one executes an MPI call that requires one to specify the type of the data object being communicated 
      (i.e. MPI_SHORT in the example below),  and if you include a data object as an argument to be communicated 
      that does  NOT have that data type, then the compiler and linker will not complain and you MAY lead to a 
      segfault. I say "MAY" because I ran the following "BAD" lines on three clusters: newhorizons, bluesky and 
      bluewaters.  Only bluewaters lead to a seg fault:

	 BAD:
	    short int fail_max, sendbuf;
	    sendbuf = 1;
	    MPI_Allreduce(&sendbuf, &fail_max, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);

	 GOOD:
	    short int fail_max, sendbuf;
	    sendbuf = 1;
	    MPI_Allreduce(&sendbuf, &fail_max, 1, MPI_SHORT, MPI_MAX, MPI_COMM_WORLD);




#############################
Sat Apr 19 11:53:15 EDT 2014
--------------------------------- 

-- when doing MPMD runs, we can't rely on myid==0  to write data for all MPI ranks as it will lie
   on one patch while others may lie on other patches (and each patch must dump its own data); 
   -- hence we need to group ranks by patch and assign ranks to serve various roles; 
   -- here are the roles: 

   --      printer_pid;     /* id of rank that reports messages; */
   --           io_pid;     /* id of rank that gathers (if using GATHER_IO) and writes data to disk; */
   --       master_pid;     /* id of rank that controls the entire job (usually equals "0"); */
   --     special1_pid;     /* id of rank that performs special computations of type 1 */
   --     ener_out_pid;     /* id of rank that write *.ener.out file   */
   --     traj_out_pid;     /* id of rank that write *.ener.out file   */
   --       timers_pid;     /* id of rank that is responsible for gathering and writing timing data */
   --   out_pid[N_OUT_TYPES]; /* pid's to be responsible for that kind of IO type */


      !! need to check all MPI routines to make sure that the ranks are not assumed; 
      DONE !! need to update the list of "*_pid" variable assignments in harm_mpi.c  from decs.h

-- changing init.interp_data.c  into the form we will need it to be for using it to transfer
    data between two patches; 



#############################
Fri May  9 16:13:03 EDT 2014
-------------------------------

-- the above changes to using *_pid variables seems to work fine.  

-- I have renamed init.interp_data.c  to interp_data.c  so that it can be used to interpolate 
     local source data to any given set of destination points;
     
     -- using two different Newton-Raphson procedures to perform xp_of_x() inversion for non-trivial case:
     	-- "1d" for "diagonal-like" cases (including  "MIXED" which is really only nonlinear in xp[2] ); 
	-- "3d" for the WARPED_SPHERICAL case (will ignore WARPED_CARTESIAN for now); 

	-- confirmed that IDENTITY inversion works
	-- confirmed that "1d" method with DIAGONAL3   system works
	-- confirmed that "3d" method with DIAGONAL3 system works (easy test of 3d method);


############################
Wed May 21 22:31:37 EDT 2014
-------------------------------

     
-- adding working version of inter_data.c  
   	  -- getting it to work with the "diagonal" coordinates inversion was pretty straightfoward
	  -- getting it to work with inverting warped spherical coordinates was difficult because there was loss of precision in calculating r(y) ;
	     -- added routine "sinh_terms()" in coord.c to more accurately calculate the problematic terms;
	     -- after this improvement in accuracy, the 3d newton raphson routine converged very rapidly; 
	     -- also updated the "errx" check in coord.c 
	     -- verified that it works for uniform cartesian coordinates (inadvertently)


-- now dealing with magnetic monopoles being generated by black holes on the grid runs  with warped spherical coordinates; 
   -- I think it is because of "frozen" cells withn the excision region not updating the magnetic field and so when it comes out of the 
      excision region, monopoles arise;  e.g., consider how the solenoidal constraint is upheld:
            
    \sum_{over centers of cells intersecting point P} \partial_t  B^i  = 0  algebraically by cancelation of common EMFs at cell edges that converge on point P

     if one of the cell centers does not update, then the div.B at point P will increase.  

   We can do whatever we want to calculate the numerical flux, all that matters is that EMFs should be consistent and that ALL the cells update the induction equation. 
   -- one could imagine having a secondary buffer region that shields the physical region from the primary buffer's monopoles by evolving the induction equation only, 
      but then you have to deal with the possibility that excision region may uncover more than the secondary buffer's worth of cells in a time step  
      	  -- is this likely?  i would think that you would have other problems if that were the case. 


-- working on init.monopole_gen.c  to test out new excision method so that we can evolve a small cartesian patch that black holes traverse through. 

   -- I am also setting  gdet=1  in excision region;
   -- method WORKS!!!!


-- I also fixed bugs in how the STAT and STAT2 arrays were being indexed when using USE_MASK evol_mask;  
     -- now we index correctly no matter what;  the problem was that evol_mask[] would skip 
     	cells and not advance the index, making the array offset to what it should be; 

#############################
Thu Jun  5 14:15:59 EDT 2014
-------------------------------

!!   -- can optimize advance() and maybe numerical flux by  making doing things like  Uloc = U_gf[0][i][j][k]  and then using Uloc[l] from then on                                             
      -- evol_mask_loc = evol_mask[ncurr]    ,etc.                                                                                                                                           
                                                                                                                                                                                             
!! -- adding (from cut2-12) restriction to where dt_char_min was calculate when using TIMESTEP_MIN_CROSSING_TIME  in numerical_flux()                                                        
       to the right way                                                                                                                                                                      
       -- this might have made the time step size smaller than the previous run                                                                                                              
       !! -- can optimize this by calculativing   inv_dt_char_max = invdx[dim]*ctop   instead  then finding the maximum and then inverting eventually                                        
             to get  dt_char_min;                                                                                                                                                            


-- need to shrink the excision region to 0.3 * r_horizon1  so that we can put all the buffer zones within the horizon 
   for this particular setup of the coordinates;


-- setup init.monopole_gen.c  to test out excision with flux_ct; 
   -- fixed a MAJOR bug in it that was calling culr_of_A() from within the loop, which is should be called from outside; 
   -- changed the parameters in it so that the test is no longer is magnetically dominant;


-- adding a few mpi routines for the sake of interp_data.c 

-- added parallel support in interp_data.c   
   -- added various routines in  it for that sake; 
   
-- changed flux_ct.c  so that we do not introduce monopoles when excising: 
   -- the problem was that we need the induction equation update to yield no change for the excised cells even though
      the EOM are solved there; 
      -- that means that the fluxes of the induction equations must be zero for the excised cells;
      -- the only way I can see to do that while upholding the solenoidal constraint is to set all the EMF functions to zero
      	 around the excised cells that contribute the excised cells' fluxes;
         -- see flux_ct.c  for details;
	 -- this fix for USE_MASK is setup for each flux_ct*() routine; 


-- now dumping excised data in 3-d dumps instead of setting it to zero  so that we can see what is going there;
   -- be careful if your analysis routines depended on this aspect! 


!! need to make sure that all "sync_val()" or "sync_vect()" calls are broadcast from the right rank, since we've 
    the "*_pid" variables now allow for other ranks besides rank=0 to Bcast. 
    -- just fixed one in coord.c  that was wrong, so there are probably others ;



-- adding METRIC_DYNAMIC_SPHERICAL_MINK   option   for METRIC_DYNAMIC_TYPE_CHOICE   for testing purposes

-- adding macro  "USE_ISOTROPIC_COORDS" to simplify logic throughout the code when we need to add/subtract M from r. 


-- implementing a fix for problems where alpha evaluates to a nan;   	 
   -- now we make sure that gcon00 <= -eps  where eps=1e-3  in metric.c


-- adding "FAST_AND_FURIOUS" macro to turn off  "exit_status()" and other MPI-paranoid infrastructure; 
   -- NOTE: that if you run into MPI problems you should turn off FAST_AND_FURIOUS so that the MPI problems are		
      are more easily found; 



---------------------------------------------------------------------------------------------------


todo:	

!! Look into optimiziing surface dump IO


!! -- add scaleheight calculation to history and surface dumps


!!  -- need to test conn_func when DYNAMIC_SPACETIME=0  
DONE !! -- will have to write "advance_coordinates()" routine

	-- add MPI-IO support for  OUT_SURFACE (low priority)

	-- make the METRIC_GENERAL_*  spacetimes  compatible for use in cartesian coordinates 
	    (see places where I use  #if( METRIC_TOP_TYPE != TOP_TYPE_CHOICE)    -- those places are where 

	-- verify alpha viscosity source terms;
	
	-- stabilize  single-bh MM08 initial data;



DONE !!            -- find out if generating array of "of_coord" structures is faster than calculating 
     		       x_of_xp(), dx_dxp()  on the fly; 
		       -- it is faster;


DONE          -- we need to transform the initial disk data (in BL coordinates) to the Near-zone 
  	          spacetime somehow


            -- implement evol_mask usage with the various "RESCALE" options 
                    (e.g. those in reconstruct_fast()) ; 
		     -- this is needed if we use those RESCALE options as the metric is used 
                         and excised regions do not always have valid metric data; 

!!            -- need to change MAKE_STAT2  flux calculation compatible with excision;  
	         FL_stat2, FR_stat2, UL_stat2 and UR_stat2 are not set  in numerical_flux() 
  		 in excised points ;
		    -- is this a problem really?
                     

!!            -- maybe change pflag[] to unsigned short int   to safe on a little storage and speed 

!!            -- need a standalone routine that calculates the trajectory as a function of time; 
                        -- right now I'm relying on the calculating in the PN metric routines, but 
                           the trajectory needs only be calculated/assigned  once per timestep whereas 
                           now it's being recalculated at every cell; 
                        -- this could further improve optimization;


DONE !!            -- need to verify that gdump routines work with the new data;

DONE !!            -- will most likely need to optimize the general spacetime calculations (connection et al.)
	      

DONE !!            -- Finish implementing new dynamical 3d  spacetime data structures;

!!            -- Finish implementing Cartesian Kerr-Schild spacetime data;
                   -- make it a boosted KS hole
                   -- implement double boosted KS data 


	      
DONE !!	      -- do we need x_gf[],xp_gf[],dx_dxp_gf[],dxp_dx_gf[]????
DONE	      	 -- if we do, then we need to make them 3d 
DONE			-- x_gf used by dump.c
DONE			-- x_gf, xp_gf, dx_dxp_gf, dxp_dx_gf used by dump_history.c
DONE			-- x_gf used by set_floor_arrays() in fixup.c 
DONE			-- they are set in metric.c 

	      -- are IO speeds fast enough to read in the spectral coefficients		
		 and calculate the metric functions effiiciently per substep? 


              DONE -- will probably want to add "of_coord" structure and array of these structures in

  	      -- update test_geom() 


		DONE -- make conn[] routine that transforms a general connection in one 
		   set of coordinates (x) to our uniform numerical coordinates (xp)
		      -- this is like our ks_conn_func() w.r.t. mks coordinates but 
			      will be just a transformation routine taking the old connection
			      to the new one;

		    DONE (why?)  -- we will want to have this be a stand-alone program as we can't afford
			 to do this per time sub-step per cell; 

	        -- build boundary conditions;

		DONE -- to add to CCRG cvs repository:
		     -- add copyright information to all harm3d files. 



-----------------------------------------------------------------------------------------

New tests of the code:


linearwaves :
	-- using the initial data from the linear wave test of harm2d;
	-- runs/data are /data1/gauss/grmhd/harm3d/linearwaves/slow_mpi/dumps
	   and  /data1/euler/scn/harm3d/harm3d/linearwaves
	-- seems like mpi works fine, even in 3d with this initial data;
	-- ppm performs worse since the extremum are diffused away;
		-- we don't care about this in the disk simulations becuase
			the turbulence won't allow this to be a big effect;



bondi:
	-- eventually will try out Hawley's idea of accreting a blob
		of matter with angular momentum on a bondi background;

	-- starting with simple bondi data on simple coordinates

	-- runs/data are in :/data1/gauss/grmhd/harm3d/bondi
	-- seems like cutout helps the convergence of the bondi solution;
	-- harm paper eliminated the boundaries from the convergence test;
	-- we see that the largest errors in uu are from near the horizon;
	-- with the boundaries, bondi converges to 1st-order
	-- if we use the cutout boundary condition, we can get 1.6 order
		converge with MC, and 2nd-order with PPM;
		-- note that there are 2h oscillations in the PPM errors,
			which may be the origin of the oscillations in our
			simulations;
		-- i am trying to eliminate these oscillations in the directory:
			/data1/gauss/grmhd/harm3d/bondi/bondi-cutout-2d/ppm2


	-- bondi w/ cutout in 2d using all cells in the calculation
		is 1.6-order using MC and 2nd-order using PPM ;
		-- the lower convergence rate w/ MC may be due to boundary
			conditions, which I'm now testing out:




stability of 2d runs:
	-- since the instability is seen in 2d, it's more efficient to
		debug it in 2d than 3d;
	-- doing runs to try things out in:
		/data1/gauss/grmhd/harm3d/stability


TODO :
DONE	-- change MPI-IO=0  setting so that each node only dumps 1 value per scalar
		value to dumps insteads writing a vector with only the first element
		being non-zero
		-- fixed;

	-- figure out why flux_ct_para_fast() does not work when N3=1





